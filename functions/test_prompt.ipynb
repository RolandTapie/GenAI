{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-10T21:27:06.372870Z",
     "start_time": "2025-08-10T21:27:05.052981Z"
    }
   },
   "source": [
    "from llm import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"openai_key\")\n",
    "gemini_api_key = os.getenv(\"gemini_key\")\n",
    "model_path = os.getenv(\"model_path\")\n",
    "model = model()\n",
    "\n",
    "model.set_openai_key(openai_api_key)\n",
    "model.set_openai_model(\"gpt-4o\")\n",
    "model.set_openai_base_url(\"https://api.openai.com/v1\")\n",
    "\n",
    "model.set_gemini_key(gemini_api_key)\n",
    "model.set_gemini_model(\"gemini-2.5-flash\")\n",
    "model.set_gemini_base_url(\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "\n",
    "\n",
    "model.init_model(\"openai\")\n",
    "message = \"bonjour\"\n",
    "\n",
    "print(model.ask(message))\n",
    "\n",
    "texte=\"L’intelligence artificielle transforme profondément notre société. Dans le domaine médical, elle permet déjà d’analyser des images radiologiques avec une précision comparable, voire supérieure, à celle des experts humains. Les algorithmes peuvent détecter des anomalies invisibles à l’œil nu et ainsi accélérer le diagnostic. Dans l’industrie, l’IA optimise la production en anticipant les pannes grâce à la maintenance prédictive. Les entreprises économisent ainsi des millions d’euros tout en réduisant leur empreinte carbone. Dans l’éducation, les systèmes intelligents personnalisent les parcours d’apprentissage. Chaque élève reçoit des exercices adaptés à son niveau et à son rythme, ce qui améliore considérablement la réussite scolaire. Cependant, ces avancées posent des questions éthiques majeures. Comment garantir que les décisions automatisées restent justes et transparentes ? Comment éviter les biais dans les données ? Les gouvernements, les chercheurs et les citoyens doivent collaborer pour encadrer ces technologies de manière responsable.\"\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour ! Comment puis-je vous aider aujourd'hui ?\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:27:59.407609Z",
     "start_time": "2025-08-10T21:27:43.345322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rag import extract_text, extract_paragraphs, encode_chunks, reformulation, retrieve_top_k\n",
    "texte= extract_text(r\"C:\\Users\\tallar\\Documents\\PROJETS\\GenAI\\ChatBot\\files\\IntroML_Azencott.pdf\")\n"
   ],
   "id": "7dadb5fd7f726f62",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:28:03.118629Z",
     "start_time": "2025-08-10T21:28:03.079524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "texte = texte.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "texte = texte.lower()\n",
    "print(texte)"
   ],
   "id": "26aa4acb963f8fae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "introduction au machine learning chloé-agathe azencott 1 cet ouvrage s’adresse aux étudiantes et étudiants en ﬁn de licence et en master d’informatique ou de maths appliquées, ainsi qu’aux élèves d’école d’ingénieurs. l’apprentissage automatique, ou machine learning, est une discipline dont les outils puissants permettent aujourd’hui à de nombreux secteurs d’activité de réaliser des progrès spectaculaires grâce à l’exploitation de grands volumes de données. le but de cet ouvrage est de vous fournir des bases solides sur les concepts et les algorithmes de ce domaine en plein essor. il vous aidera à identiﬁer les problèmes qui peuvent être résolus par une approche machine learning, à les formaliser, à identiﬁer les algorithmes les mieux adaptés à chaque cas, à les mettre en oeuvre, et enﬁn à savoir évaluer les résultats obtenus. ce document est la version électronique d’un ouvrage publié aux éditions dunod dans la collection infosup 1, qui contient aussi 86 exercices corrigés. 1. https://www.dunod.com/sciences-techniques/introduction-au-machine-learning-0 \fpréambule le machine learning (apprentissage automatique) est au cœur de la science des données et de l’intelli- gence artiﬁcielle. que l’on parle de transformation numérique des entreprises, de big data ou de straté- gie nationale ou européenne, le machine learning est devenu incontournable. ses applications sont nom- breuses et variées, allant des moteurs de recherche et de la reconnaissance de caractères à la recherche en génomique, l’analyse des réseaux sociaux, la publicité ciblée, la vision par ordinateur, la traduction auto- matique ou encore le trading algorithmique. à l’intersection des statistiques et de l’informatique, le machine learning se préoccupe de la modélisa- tion des données. les grands principes de ce domaine ont émergé des statistiques fréquentistes ou bayé- siennes, de l’intelligence artiﬁcielle ou encore du traitement du signal. dans ce livre, nous considérons que le machine learning est la science de l’apprentissage automatique d’une fonction prédictive à partir d’un jeu d’observations de données étiquetées ou non. ce livre se veut une introduction aux concepts et algorithmes qui fondent le machine learning, et en propose une vision centrée sur la minimisation d’un risque empirique par rapport à une classe donnée de fonctions de prédictions. objectifs pédagogiques : le but de ce livre est de vous accompagner dans votre découverte du machine learning et de vous fournir les outils nécessaires à 1. identiﬁer les problèmes qui peuvent être résolus par des approches de machine learning ; 2. formaliser ces problèmes en termes de machine learning ; 3. identiﬁer les algorithmes classiques les plus appropriés pour ces problèmes et les mettre en œuvre ; 4. implémenter ces algorithmes par vous-même aﬁn d’en comprendre les tenants et aboutissants ; 5. évaluer et comparer de la manière la plus objective possible les performances de plusieurs algo- rithmes de machine learning pour une application particulière. public visé : ce livre s’adresse à des étudiants en informatique ou maths appliquées, niveau l3 ou m1 (ou deuxième année d’école d’ingénieur), qui cherchent à comprendre les fondements des principaux algo- rithmes utilisés en machine learning. il se base sur mes cours à centralesupélec 2 et sur openclassrooms 3 et suppose les prérequis suivants : — algèbre linéaire (inversion de matrice, théorème spectral, décomposition en valeurs propres et vec- teurs propres). — notions de probabilités (variable aléatoire, distributions, théorème de bayes). 2. http://tinyurl.com/ma2823-2017 3. https://openclassrooms.com/paths/data-scientist 2 \f3 plan du livre : ce livre commence par une vue d’ensemble du machine learning et des diﬀérents types de problèmes qu’il permet de résoudre. il présente comment ces problèmes peuvent être formulés mathé- matiquement comme des problèmes d’optimisation (chapitre 1) et pose en appendice les bases d’optimi- sation convexe nécessaires à la compréhension des algorithmes présentés par la suite. la majeure partie de ce livre concerne les problèmes d’apprentissage supervisé ; le chapitre 2 détaille plus particulièrement leur formulation et introduit les notions d’espace des hypothèses, de risque et perte, et de généralisation. avant d’étudier les algorithmes d’apprentissage supervisé les plus classiques et fréquemment utilisés, il est essentiel de comprendre comment évaluer un modèle sur un jeu de données, et de savoir sélectionner le meilleur modèle parmi plusieurs possibilités, ce qui est le sujet du chapitre 3. il est enﬁn pertinent à ce stade d’aborder l’entraînement de modèles prédictifs supervisés. le livre aborde tout d’abord les modèles paramétriques, dans lesquels la fonction modélisant la distribution des données ou permettant de faire des prédictions a une forme analytique explicite. les bases sont posées avec des éléments d’inférence bayésienne (chapitre 4), qui seront ensuite appliqués à des modèles d’ap- prentissage supervisé paramétriques (chapitre 5). le chapitre 6 présente les variantes régularisées de ces algorithmes. enﬁn, le chapitre 7 sur les réseaux de neurones propose de construire des modèles paramé- triques beaucoup plus complexes et d’aborder les bases du deep learning. le livre aborde ensuite les modèles non paramétriques, à commencer par une des plus intuitives de ces approches, la méthode des plus proches voisins (chapitre 8). suivront ensuite les approches à base d’arbres de décision, puis les méthodes à ensemble qui permettront d’introduire deux des algorithmes de machine learning supervisé les plus puissants à l’heure actuelle : les forêts aléatoires et le boosting de gradient (cha- pitre 9). le chapitre 10 sur les méthodes à noyaux, introduites grâce aux machines à vecteurs de support, permettra de voir comment construire des modèles non linéaires via des modèles linéaires dans un espace de redescription des données. enﬁn, le chapitre 11 présentera la réduction de dimension, supervisée ou non supervisée, et le cha- pitre 12 traitera d’un des problèmes les plus importants en apprentissage non supervisé : le clustering. chaque chapitre sera conclu par quelques exercices. comment lire ce livre : ce livre a été conçu pour être lu linéairement. cependant, après les trois pre- miers chapitres, il vous sera possible de lire les suivants dans l’ordre qui vous conviendra, à l’exception du chapitre 6, qui a été écrit dans la continuité du chapitre 5. de manière générale, des références vers les sections d’autres chapitres apparaîtront si nécessaire. remerciements : cet ouvrage n’aurait pas vu le jour sans jean-philippe vert, qui m’a fait découvrir le machine learning, avec qui j’ai enseigné et pratiqué cette discipline pendant plusieurs années, et qui m’a fait, enﬁn, l’honneur d’une relecture attentive. ce livre doit beaucoup à ceux qui m’ont enseigné le machine learning, et plus particulièrement pierre baldi, padhraic smyth, et max welling ; à ceux avec qui je l’ai pratiqué, notamment les membres du baldi lab à uc irvine, du mlcb et du département d’inférence empirique de l’institut max planck à tübingen, et du cbio à mines paristech, et bien d’autres encore qu’il serait diﬃcile de tous nommer ici ; à ceux avec qui je l’ai enseigné, karsten borgwardt, yannis chaouche, frédéric guyon, fabien moutarde, mais aussi judith abecassis, eugene belilovsky, joseph boyd, peter naylor, benoît playe, mihir sahasrabudhe, jiaqian yu, et luc bertrand ; et enﬁn à ceux auxquels je l’ai enseigné, en particulier les étudiants du cours data mining \f4 in der bioinformatik de l’université de tübingen qui ont subi ma toute première tentative d’enseignement des méthodes à noyaux en 2012, et les étudiants centraliens qui ont essuyé les plâtres de la première version de ce cours à l’automne 2015. mes cours sont le résultat de nombreuses sources d’inspirations accumulées au cours des années. je remercie tout particulièrement ethem alpaydin, david barber, christopher m. bishop, stephen boyd, hal daumé iii, jerome friedman, trevor hastie, tom mitchell, bernhard schölkopf, alex smola, robert tibshi- rani, lieven vandenberghe, et alice zhang pour leurs ouvrages. parce que tout serait diﬀérent sans scikit-learn, je remercie chaleureusement tous ses core-devs, et en particulier alexandre gramfort, olivier grisel, gaël varoquaux et nelle varoquaux. je remercie aussi matthew blaschko, qui m’a poussée à l’eau, et nikos paragios, qui l’y a encouragé. parce que je n’aurais pas pu écrire ce livre seule, merci à jean-luc blanc des éditions dunod, et à tous ceux qui ont relu tout ou partie de cet ouvrage, en particulier judith abecassis, luc bertrand, caroline petitjean, denis rousselle, erwan scornet. la relecture attentive de jean-marie monier, ainsi que les commentaires d’antoine brault, ont permis d’éliminer de nombreuses coquilles et approximations de la deuxième version de ce texte. merci à alix deleporte, enﬁn, pour ses relectures et son soutien. \ftable des matières 1 présentation du machine learning . . . . . 1.1.1 1.1 qu’est-ce que le machine learning ? . . pourquoi utiliser le machine learning ? . . . . . . . . . . 1.2 types de problèmes de machine learning . . . . . . . . . 1.2.1 apprentissage supervisé . . 1.2.2 apprentissage non supervisé . 1.2.3 apprentissage semi-supervisé . 1.2.4 apprentissage par renforcement . . . . . implémentations logicielles . . jeux de données . . . 1.3.1 1.3.2 1.4 notations . 1.3 ressources pratiques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 apprentissage supervisé . . . . . . . . . . . 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . classiﬁcation multi-classe . . formalisation d’un problème d’apprentissage supervisé . . . 2.1.1 décision . . 2.1.2 . 2.2 espace des hypothèses . . . 2.3 minimisation du risque empirique . . 2.4 . fonctions de coût pour la classiﬁcation binaire . . coûts pour la classiﬁcation multi-classe . . . coûts pour la régression . . . . . . . . . . . . 2.5 généralisation et sur-apprentissage . . . . . . . sur-apprentissage . . compromis biais-variance . . régularisation . 2.5.1 généralisation . 2.5.2 2.5.3 2.5.4 fonctions de coût . 2.4.1 2.4.2 2.4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 sélection de modèle et évaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . jeu de test . . jeu de validation . . 3.1.1 3.1.2 . 3.1.3 validation croisée . . bootstrap . 3.1.4 . 3.1 estimation empirique de l’erreur de généralisation . . . . . . . . . . . . . . . évaluation de méthodes de classiﬁcation binaire retournant un score . . . . . . 3.2.1 matrice de confusion et critères dérivés 3.2.2 . 3.2 critères de performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 . 10 . 11 . 13 . 13 . 15 . 17 . 17 . 17 . 17 . 18 . 18 20 . 20 . 21 . 22 . 23 . 24 . 25 . 26 . 28 . 29 . 30 . 30 . 31 . 32 . 34 36 . 36 . 37 . 37 . 37 . 39 . 40 . 40 . 42 5 \ftable des matières 6 3.2.3 3.2.4 erreurs de régression . . comparaison à des algorithmes naïfs . . . . . . . . . . . . . . . . . 4 inférence bayésienne . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.1 4.2.2 4.2 règles de décision . 4.3 estimation de densité inférence et prédiction . . loi de bayes . . 4.1.1 4.1.2 . 4.1.3 modélisation paramétrique . . . . . . . tests du rapport de vraisemblance . . théorie de la décision bayésienne . . . . . 4.1 modèles génératifs pour la classiﬁcation binaire . . . . . . . . . . . . . . . . . . estimation par maximum de vraisemblance . 4.3.1 . 4.3.2 . . estimateur de bayes . . 4.3.3 décomposition biais-variance . . . . . . . . . . principes . . . filtrage bayésien du spam . . 4.4.1 4.4.2 sélection de modèle bayésienne . 4.4 classiﬁcation naïve bayésienne . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 régressions paramétriques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.1 modèles paramétriques 5.1.2 5.1 apprentissage supervisé d’un modèle paramétrique . . . . . . estimation par maximum de vraisemblance et méthode des moindres carrés . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 régression linéaire . . . formulation . solution . . . . théorème de gauss-markov . . . . . . . . 5.4 régression polynomiale . 5.3 régression logistique . . . formulation . . solution . 5.2.1 5.2.2 5.2.3 5.3.1 5.3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 régularisation . . . . . . . . . . . . . . . . . . . . . 6.1 qu’est-ce que la régularisation ? . . 6.2 la régression ridge . 6.2.1 6.2.2 6.2.3 6.2.4 le lasso . 6.3.1 6.3.2 6.3.3 6.3.4 6.3.5 6.4 elastic net . . . . formulation de la régression ridge . . . solution . . . . . . chemin de régularisation . . . interprétation géométrique . . . . . . . . . . . parcimonie . . . . formulation du lasso . . solution . . . . . . interprétation géométrique . . . chemin de régularisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 . 46 49 . 49 . 50 . 50 . 51 . 51 . 52 . 53 . 57 . 57 . 59 . 60 . 60 . 60 . 61 . 62 64 . 64 . 64 . 66 . 66 . 66 . 66 . 67 . 68 . 69 . 70 . 70 72 . 72 . 73 . 73 . 74 . 74 . 75 . 76 . 76 . 76 . 77 . 77 . 77 . 78 \ftable des matières . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1 . . entraînement . 7 réseaux de neurones artiﬁciels . . . le perceptron . . . 7.1.1 modèle . . . 7.1.2 7.1.3 modélisation de fonctions booléennes . . . . . . . . . 7.2 perceptron multi-couche . . . . 7.2.1 architecture . 7.2.2 approximation universelle . 7.2.3 modéliser xor avec un perceptron multi-couche . . 7.2.4 . 7.2.5 entraînement par rétropropagation . . et le deep learning dans tout ça ? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 méthode des plus proches voisins . . . . . . . . . . . . . . . . . . . . . . . 8.1 méthode du plus proche voisin . . . . . . 8.1.1 méthode 8.1.2 diagramme de voronoï . . 8.2 méthode des plus proches voisins . . . . . . . . . . 8.2.1 méthode des k plus proches voisins . . . . 8.2.2 apprentissage paresseux . . . 8.2.3 nombre de plus proches voisins . . . . 8.2.4 variantes . . . . . . 8.3 distances et similarités . . . 8.3.1 distances . . . . . similarités entre vecteurs réels . 8.3.2 similarités entre ensembles . 8.3.3 . . . similarités entre données catégoriques . 8.3.4 . . filtrage collaboratif . . . . . . . . . . . . . . . . . . . . 8.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 arbres et forêts . . . . . . . . . . . . . . . . . . . . . . . . . . 9.1 arbres de décision . . 9.1.1 apprentissage hiérarchique . 9.1.2 . . partition de l’espace par un arbre de décision . . . . . . . . . . . . 9.3 méthodes ensemblistes : la sagesse des foules 9.3.1 méthodes parallèles : le bagging . . . 9.3.2 méthodes séquentielles : le boosting . 9.2 comment faire pousser un arbre . . . cart . . . . critères d’impureté . . élaguer un arbre . 9.2.1 9.2.2 9.2.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 machines à vecteurs de support et méthodes à noyaux 10.1 le cas linéairement séparable : svm à marge rigide . . . . . . . . . . 10.2 le cas linéairement non séparable : svm à marge souple . . 10.1.1 marge d’un hyperplan séparateur . . 10.1.2 formulation de la svm à marge rigide . . 10.1.3 formulation duale . . . 10.1.4 interprétation géométrique . 10.2.1 formulation de la svm à marge souple . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 81 . 81 . 82 . 83 . 85 . 86 . 86 . 87 . 88 . 88 . 91 93 . 93 . 93 . 94 . 95 . 95 . 96 . 96 . 97 . 98 . 98 . 99 . 100 . 101 . 102 104 . 104 . 104 . 105 . 106 . 106 . 108 . 108 . 109 . 110 . 111 116 . 116 . 117 . 118 . 119 . 121 . 121 . 121 \f8 . . . . 10.3 le cas non linéaire : svm à noyau . . 10.2.2 formulation duale . . 10.2.3 interprétation géométrique . . . . . . . 10.3.1 espace de redescription . 10.3.2 svm dans l’espace de redescription . . 10.3.3 astuce du noyau . . . 10.3.4 noyaux . . 10.4 régression ridge à noyau . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 réduction de dimension . 11.1 motivation . . . 11.2 sélection de variables . . . . . . . . . . . . . . . . . . . . . 11.1.1 visualiser les données . . 11.1.2 réduire les coûts algorithmiques . 11.1.3 améliorer la qualité des modèles . . . . . . . . . . . . . . . 11.3.1 analyse en composantes principales . 11.3.2 factorisation de la matrice des données . . 11.3.3 auto-encodeurs . . 11.3.4 autres approches non linéaires . . . 11.2.1 méthodes de ﬁltrage . . 11.2.2 méthodes de conteneur . . 11.2.3 méthodes embarquées . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.3 extraction de variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12.3 clustering hiérarchique . . 12.2.1 la forme des clusters . . 12.2.2 la stabilité des clusters . 12.2.3 les connaissances expert . . . 12.1 pourquoi partitionner ses données . 12.2 évaluer la qualité d’un algorithme de clustering . . . . . . . . . . . . . . . . . . . 12.3.1 dendrogramme . . 12.3.2 construction agglomérative ou divisive . . 12.3.3 fonctions de lien . . . 12.3.4 choix du nombre de clusters . 12.3.5 complexité algorithmique . . . . . . . . . . . . 12.4.1 algorithme de lloyd . . 12.4.2 forme des clusters . . . 12.4.3 variantes . . . . 12.5 clustering par densité . 12.4 méthode des k-moyennes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . a.1 convexité . a.1.1 a.1.2 a notions d’optimisation convexe . . . . . ensemble convexe . . . fonction convexe . a.2 problèmes d’optimisation convexe . formulation et vocabulaire . a.2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . table des matières . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122 . 124 . 124 . 125 . 125 . 125 . 126 . 128 132 . 132 . 132 . 133 . 133 . 134 . 134 . 135 . 137 . 137 . 137 . 140 . 142 . 145 148 . 148 . 149 . 149 . 152 . 152 . 153 . 153 . 154 . 154 . 155 . 155 . 156 . 156 . 156 . 157 . 158 162 . 162 . 162 . 163 . 164 . 164 \ftable des matières . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . a.2.2 extrema locaux et globaux . . a.3 optimisation convexe sans contrainte . . . . a.3.1 caractérisation diﬀérentielle de la convexité . . a.3.2 caractérisation du deuxième ordre de la convexité . . . a.3.3 algorithme du gradient . . . a.3.4 recherche linéaire par rebroussement . a.3.5 méthode de newton . . . . a.3.6 méthode de newton par gradient conjugué . . . a.3.7 méthodes de quasi-newton . . . . a.3.8 algorithme du gradient stochastique . . . . a.3.9 descente de coordonnées . . . . . . . . . . . . . . . . . . . . . a.4.1 . . a.4.2 dualité faible . . a.4.3 dualité forte . a.4.4 conditions de karush-kuhn-tucker . . a.4.5 programmes quadratiques . . a.4 optimisation convexe sous contraintes . . . . lagrangien . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 . 165 . 166 . 166 . 167 . 168 . 169 . 170 . 171 . 173 . 174 . 175 . 175 . 175 . 176 . 177 . 178 . 179 \fchapitre 1 présentation du machine learning le machine learning est un domaine captivant. issu de nombreuses disciplines comme les statistiques, l’optimisation, l’algorithmique ou le traitement du signal, c’est un champ d’études en mutation constante qui s’est maintenant imposé dans notre société. déjà utilisé depuis des décennies dans la reconnaissance automatique de caractères ou les ﬁltres anti-spam, il sert maintenant à protéger contre la fraude bancaire, recommander des livres, ﬁlms, ou autres produits adaptés à nos goûts, identiﬁer les visages dans le viseur de notre appareil photo, ou traduire automatiquement des textes d’une langue vers une autre. dans les années à venir, le machine learning nous permettra vraisemblablement d’améliorer la sécurité routière (y compris grâce aux véhicules autonomes), la réponse d’urgence aux catastrophes naturelles, le développement de nouveaux médicaments, ou l’eﬃcacité énergétique de nos bâtiments et industries. le but de ce chapitre est d’établir plus clairement ce qui relève ou non du machine learning, ainsi que des branches de ce domaine dont cet ouvrage traitera. objectifs — déﬁnir le machine learning — identiﬁer si un problème relève ou non du machine learning — donner des exemples de cas concrets relevant de grandes classes de problèmes de machine learning. 1.1 qu’est-ce que le machine learning ? qu’est-ce qu’apprendre, comment apprend-on, et que cela signiﬁe-t-il pour une machine ? la question de l’apprentissage fascine les spécialistes de l’informatique et des mathématiques tout autant que neuro- logues, pédagogues, philosophes ou artistes. une déﬁnition qui s’applique à un programme informatique comme à un robot, un animal de compagnie ou un être humain est celle proposée par fabien benureau (2015) : « l’apprentissage est une modiﬁcation d’un comportement sur la base d’une expérience ». dans le cas d’un programme informatique, qui est celui qui nous intéresse dans cet ouvrage, on parle d’apprentissage automatique, ou machine learning, quand ce programme a la capacité d’apprendre sans que cette modiﬁcation ne soit explicitement programmée. cette déﬁnition est celle donnée par arthur samuel 10 \f1.1. qu’est-ce que le machine learning ? 11 (1959). on peut ainsi opposer un programme classique, qui utilise une procédure et les données qu’il reçoit en entrée pour produire en sortie des réponses, à un programme d’apprentissage automatique, qui utilise les données et les réponses aﬁn de produire la procédure qui permet d’obtenir les secondes à partir des premières. exemple supposons qu’une entreprise veuille connaître le montant total dépensé par un client ou une cliente à partir de ses factures. il suﬃt d’appliquer un algorithme classique, à savoir une simple addition : un algorithme d’apprentissage n’est pas nécessaire. supposons maintenant que l’on veuille utiliser ces factures pour déterminer quels produits le client est le plus susceptible d’acheter dans un mois. bien que cela soit vraisemblablement lié, nous n’avons manifes- tement pas toutes les informations nécessaires pour ce faire. cependant, si nous disposons de l’historique d’achat d’un grand nombre d’individus, il devient possible d’utiliser un algorithme de machine learning pour qu’il en tire un modèle prédictif nous permettant d’apporter une réponse à notre question. 1.1.1 pourquoi utiliser le machine learning ? le machine learning peut servir à résoudre des problèmes — que l’on ne sait pas résoudre (comme dans l’exemple de la prédiction d’achats ci-dessus) ; — que l’on sait résoudre, mais dont on ne sait formaliser en termes algorithmiques comment nous les résolvons (c’est le cas par exemple de la reconnaissance d’images ou de la compréhension du langage naturel) ; — que l’on sait résoudre, mais avec des procédures beaucoup trop gourmandes en ressources informa- tiques (c’est le cas par exemple de la prédiction d’interactions entre molécules de grande taille, pour lesquelles les simulations sont très lourdes). le machine learning est donc utilisé quand les données sont abondantes (relativement), mais les connais- sances peu accessibles ou peu développées. ainsi, le machine learning peut aussi aider les humains à apprendre : les modèles créés par des al- gorithmes d’apprentissage peuvent révéler l’importance relative de certaines informations ou la façon dont elles interagissent entre elles pour résoudre un problème particulier. dans l’exemple de la prédic- tion d’achats, comprendre le modèle peut nous permettre d’analyser quelles caractéristiques des achats passés permettent de prédire ceux à venir. cet aspect du machine learning est très utilisé dans la recherche scientiﬁque : quels gènes sont impliqués dans le développement d’un certain type de tumeur, et comment ? quelles régions d’une image cérébrale permettent de prédire un comportement ? quelles caractéristiques d’une molécule en font un bon médicament pour une indication particulière ? quels aspects d’une image de télescope permettent d’y identiﬁer un objet astronomique particulier ? ingrédients du machine learning le machine learning repose sur deux piliers fondamentaux : — d’une part, les données, qui sont les exemples à partir duquel l’algorithme va apprendre ; — d’autre part, l’algorithme d’apprentissage, qui est la procédure que l’on fait tourner sur ces données pour produire un modèle. on appelle entraînement le fait de faire tourner un algorithme d’appren- tissage sur un jeu de données. ces deux piliers sont aussi importants l’un que l’autre. d’une part, aucun algorithme d’apprentissage ne pourra créer un bon modèle à partir de données qui ne sont pas pertinentes – c’est le concept garbage in, garbage out qui stipule qu’un algorithme d’apprentissage auquel on fournit des données de mauvaise qualité \f12 chapitre 1. présentation du machine learning ne pourra rien en faire d’autre que des prédictions de mauvaise qualité. d’autre part, un modèle appris avec un algorithme inadapté sur des données pertinentes ne pourra pas être de bonne qualité. cet ouvrage est consacré au deuxième de ces piliers – les algorithmes d’apprentissage. néanmoins, il ne faut pas négliger qu’une part importante du travail de machine learner ou de data scientist est un travail d’ingénierie consistant à préparer les données aﬁn d’éliminer les données aberrantes, gérer les données manquantes, choisir une représentation pertinente, etc. bien que l’usage soit souvent d’appeler les deux du même nom, il faut distinguer l’algorithme d’apprentis- sage automatique du modèle appris : le premier utilise les données pour produire le second, qui peut ensuite être appliqué comme un programme classique. attention un algorithme d’apprentissage permet donc de modéliser un phénomène à partir d’exemples. nous consi- dérons ici qu’il faut pour ce faire déﬁnir et optimiser un objectif. il peut par exemple s’agir de minimiser le nombre d’erreurs faites par le modèle sur les exemples d’apprentissage. cet ouvrage présente en eﬀet les algorithmes les plus classiques et les plus populaires sous cette forme. voici quelques exemples de reformulation de problèmes de machine learning sous la forme d’un pro- blème d’optimisation. la suite de cet ouvrage devrait vous éclairer sur la formalisation mathématique de ces problèmes, formulés ici très librement. exemple — un vendeur en ligne peut chercher à modéliser des types représentatifs de clientèle, à partir des tran- sactions passées, en maximisant la proximité entre clients et clientes aﬀectés à un même type ; — une compagnie automobile peut chercher à modéliser la trajectoire d’un véhicule dans son environ- nement, à partir d’enregistrements vidéo de voitures, en minimisant le nombre d’accidents ; — des chercheurs en génétique peuvent vouloir modéliser l’impact d’une mutation sur une maladie, à partir de données patient, en maximisant la cohérence de leur modèle avec les connaissances de l’état de l’art ; — une banque peut vouloir modéliser les comportements à risque, à partir de son historique, en maxi- misant le taux de détection de non solvabilité. ainsi, le machine learning repose d’une part sur les mathématiques, et en particulier les statistiques, pour ce qui est de la construction de modèles et de leur inférence à partir de données, et d’autre part sur l’informatique, pour ce qui est de la représentation des données et de l’implémentation eﬃcace d’algo- rithmes d’optimisation. de plus en plus, les quantités de données disponibles imposent de faire appel à des architectures de calcul et de base de données distribuées. c’est un point important mais que nous n’abor- dons pas dans cet ouvrage. et l’intelligence artiﬁcielle, dans tout ça ? le machine learning peut être vu comme une branche de l’intelligence artiﬁcielle. en eﬀet, un système incapable d’apprendre peut diﬃcilement être considéré comme intelligent. la capacité à apprendre et à tirer parti de ses expériences est en eﬀet essentielle à un système conçu pour s’adapter à un environnement changeant. l’intelligence artiﬁcielle, déﬁnie comme l’ensemble des techniques mises en œuvre aﬁn de construire des machines capables de faire preuve d’un comportement que l’on peut qualiﬁer d’intelligent, fait aussi appel aux sciences cognitives, à la neurobiologie, à la logique, à l’électronique, à l’ingénierie et bien plus encore. \f1.2. types de problèmes de machine learning 13 probablement parce que le terme « intelligence artiﬁcielle » stimule plus l’imagination, il est cependant de plus en plus souvent employé en lieu et place de celui d’apprentissage automatique. 1.2 types de problèmes de machine learning le machine learning est un champ assez vaste, et nous dressons dans cette section une liste des plus grandes classes de problèmes auxquels il s’intéresse. 1.2.1 apprentissage supervisé l’apprentissage supervisé est peut-être le type de problèmes de machine learning le plus facile à appré- hender : son but est d’apprendre à faire des prédictions, à partir d’une liste d’exemples étiquetés, c’est-à-dire accompagnés de la valeur à prédire (voir ﬁgure 1.1). les étiquettes servent de « professeur » et supervisent l’apprentissage de l’algorithme. déﬁnition 1.1 (apprentissage supervisé) on appelle apprentissage supervisé la branche du machine learning qui s’intéresse aux problèmes pouvant être formalisés de la façon suivante : étant données n obser- vations {(cid:126)x i}i=1,...,n décrites dans un espace x , et leurs étiquettes {yi}i=1,...,n décrites dans un espace y, on suppose que les étiquettes peuvent être obtenues à partir des observations grâce à une fonction φ : x → y ﬁxe et inconnue : yi = φ((cid:126)x i) + (cid:15)i, où (cid:15)i est un bruit aléatoire. il s’agit alors d’utiliser les données pour dé- (cid:4) terminer une fonction f : x → y telle que, pour tout couple ((cid:126)x, φ((cid:126)x)) ∈ x × y, f ((cid:126)x) ≈ φ((cid:126)x). l’espace sur lequel sont déﬁnies les données est le plus souvent x = rp. nous verrons cependant aussi comment traiter d’autres types de représentations, comme des variables binaires, discrètes, catégoriques, voire des chaînes de caractères ou des graphes. figure 1.1 – apprentissage supervisé. classiﬁcation binaire dans le cas où les étiquettes sont binaires, elles indiquent l’appartenance à une classe. on parle alors de classiﬁcation binaire. déﬁnition 1.2 (classiﬁcation binaire) un problème d’apprentissage supervisé dans lequel l’espace (cid:4) des étiquettes est binaire, autrement dit y = {0, 1} est appelé un problème de classiﬁcation binaire. observationsalgorithmede mlétiquettesmodèle prédictif\f14 chapitre 1. présentation du machine learning exemple voici quelques exemples de problèmes de classiﬁcation binaire : — identiﬁer si un email est un spam ou non ; — identiﬁer si un tableau a été peint par picasso ou non ; — identiﬁer si une image contient ou non une girafe ; — identiﬁer si une molécule peut ou non traiter la dépression ; — identiﬁer si une transaction ﬁnancière est frauduleuse ou non. classiﬁcation multi-classe dans le cas où les étiquettes sont discrètes, et correspondent donc à plusieurs 1 classes, on parle de clas- siﬁcation multi-classe. déﬁnition 1.3 (classiﬁcation multi-classe) un problème d’apprentissage supervisé dans lequel l’es- pace des étiquettes est discret et ﬁni, autrement dit y = {1, 2, . . . , c} est appelé un problème de classiﬁ- (cid:4) cation multi-classe. c est le nombre de classes. exemple voici quelques exemples de problèmes de classiﬁcation multi-classe : — identiﬁer en quelle langue un texte est écrit ; — identiﬁer lequel des 10 chiﬀres arabes est un chiﬀre manuscrit — identiﬁer l’expression d’un visage parmi une liste prédéﬁnie de possibilités (colère, tristesse, joie, etc.) ; — identiﬁer à quelle espèce appartient une plante ; — identiﬁer les objets présents sur une photographie. régression dans le cas où les étiquettes sont à valeurs réelles, on parle de régression. déﬁnition 1.4 (régression) un problème d’apprentissage supervisé dans lequel l’espace des étiquettes (cid:4) est y = r est appelé un problème de régression. exemple voici quelques exemples de problèmes de régression : — prédire le nombre de clics sur un lien ; — prédire le nombre d’utilisateurs et utilisatrices d’un service en ligne à un moment donné ; — prédire le prix d’une action en bourse ; — prédire l’aﬃnité de liaison entre deux molécules ; — prédire le rendement d’un plant de maïs. 1. nous utilisons ici la déﬁnition bourbakiste de « plusieurs », c’est-à-dire strictement supérieur à deux. \f1.2. types de problèmes de machine learning 15 régression structurée dans le cas où l’espace des étiquettes est un espace structuré plus complexe que ceux évoqués précé- demment, on parle de régression structurée – en anglais, structured regression, ou structured output prediction. il peut par exemple s’agir de prédire des vecteurs, des images, des graphes, ou des séquences. la régression structurée permet de formaliser de nombreux problèmes, comme ceux de la traduction automatique ou de la reconnaissance vocale (text-to-speech et speech-to-text, par exemple). ce cas dépasse cependant le cadre du présent ouvrage, et nous nous concentrerons sur les problèmes de classiﬁcation binaire et multi-classe, ainsi que de régression classique. l’apprentissage supervisé est le sujet principal de cet ouvrage, et sera traité du chapitre 2 au chapitre 9. 1.2.2 apprentissage non supervisé dans le cadre de l’apprentissage non supervisé, les données ne sont pas étiquetées. il s’agit alors de mo- déliser les observations pour mieux les comprendre (voir ﬁgure 1.2). déﬁnition 1.5 (apprentissage non supervisé) on appelle apprentissage non supervisé la branche du machine learning qui s’intéresse aux problèmes pouvant être formalisés de la façon suivante : étant don- nées n observations {(cid:126)x i}i=1,...,n décrites dans un espace x , il s’agit d’apprendre une fonction sur x qui (cid:4) vériﬁe certaines propriétés. figure 1.2 – apprentissage non supervisé. cette déﬁnition est très vague, et sera certainement plus claire sur des exemples. clustering tout d’abord, le clustering, ou partitionnement, consiste à identiﬁer des groupes dans les données (voir ﬁgure 1.3). cela permet de comprendre leurs caractéristiques générales, et éventuellement d’inférer les propriétés d’une observation en fonction du groupe auquel elle appartient. déﬁnition 1.6 (partitionnement) on appelle partitionnement ou clustering un problème d’apprentis- k=1 ck des n observations (cid:4) sage non supervisé pouvant être formalisé comme la recherche d’une partition (cid:83)k {(cid:126)x i}i=1,...,n. cette partition doit être pertinente au vu d’un ou plusieurs critères à préciser. exemple voici quelques exemples de problèmes de partitionnement — la segmentation de marché consiste à identiﬁer des groupes d’usagers ou de clients ayant un compor- tement similaire. cela permet de mieux comprendre leur proﬁl, et cibler une campagne de publicité, des contenus ou des actions spéciﬁquement vers certains groupes. — identiﬁer des groupes de documents ayant un sujet similaire, sans les avoir au préalable étiquetés par sujet. cela permet d’organiser de larges banques de textes. observationsalgorithmede mlobservations réduites\f16 chapitre 1. présentation du machine learning figure 1.3 – partitionnement des données, ou clustering. — la compression d’image peut être formulée comme un problème de partitionnement consistant à re- grouper des pixels similaires pour ensuite les représenter plus eﬃcacement. — la segmentation d’image consiste à identiﬁer les pixels d’une image appartenant à la même région. — identiﬁer des groupes parmi les patients présentant les mêmes symptômes permet d’identiﬁer des sous-types d’une maladie, qui pourront être traités diﬀéremment. ce sujet est traité en détail au chapitre 12. réduction de dimension la réduction de dimension est une autre famille importante de problèmes d’apprentissage non supervisé. il s’agit de trouver une représentation des données dans un espace de dimension plus faible que celle de l’espace dans lequel elles sont représentées à l’origine (voir ﬁgure 1.4). cela permet de réduire les temps de calcul et l’espace mémoire nécessaire au stockage les données, mais aussi souvent d’améliorer les perfor- mances d’un algorithme d’apprentissage supervisé entraîné par la suite sur ces données. déﬁnition 1.7 (réduction de dimension) on appelle réduction de dimension un problème d’appren- tissage non supervisé pouvant être formalisé comme la recherche d’un espace z de dimension plus faible que l’espace x dans lequel sont représentées n observations {(cid:126)x i}i=1,...,n. les projections {(cid:126)z i}i=1,...,n des (cid:4) données sur z doivent vériﬁer certaines propriétés à préciser. figure 1.4 – réduction de dimension. remarque certaines méthodes de réduction de dimension sont supervisées : il s’agit alors de trouver la représen- tation la plus pertinente pour prédire une étiquette donnée. nous traiterons de la réduction de dimension au chapitre 11. observationsalgorithmede mlobservationsalgorithmede mlobservationsréduites\f1.3. ressources pratiques estimation de densité 17 enﬁn, une grande famille de problèmes d’apprentissage non supervisé est en fait un problème tradi- tionnel en statistiques : il s’agit d’estimer une loi de probabilité en supposant que le jeu de données en est un échantillon aléatoire. le chapitre 4 aborde brièvement ce sujet. 1.2.3 apprentissage semi-supervisé comme on peut s’en douter, l’apprentissage semi-supervisé consiste à apprendre des étiquettes à par- tir d’un jeu de données partiellement étiqueté. le premier avantage de cette approche est qu’elle permet d’éviter d’avoir à étiqueter l’intégralité des exemples d’apprentissage, ce qui est pertinent quand il est fa- cile d’accumuler des données mais que leur étiquetage requiert une certaine quantité de travail humain. prenons par exemple la classiﬁcation d’images : il est facile d’obtenir une banque de données contenant des centaines de milliers d’images, mais avoir pour chacune d’entre elles l’étiquette qui nous intéresse peut requérir énormément de travail. de plus, les étiquettes données par des humains sont susceptibles de re- produire des biais humains, qu’un algorithme entièrement supervisé reproduira à son tour. l’apprentissage semi-supervisé permet parfois d’éviter cet écueil. il s’agit d’un sujet plus avancé, que nous ne considérerons pas dans cet ouvrage. 1.2.4 apprentissage par renforcement dans le cadre de l’apprentissage par renforcement, le système d’apprentissage peut interagir avec son en- vironnement et accomplir des actions. en retour de ces actions, il obtient une récompense, qui peut être positive si l’action était un bon choix, ou négative dans le cas contraire. la récompense peut parfois ve- nir après une longue suite d’actions ; c’est le cas par exemple pour un système apprenant à jouer au go ou aux échecs. ainsi, l’apprentissage consiste dans ce cas à déﬁnir une politique, c’est-à-dire une stratégie permettant d’obtenir systématiquement la meilleure récompense possible. les applications principales de l’apprentissage par renforcement se trouvent dans les jeux (échecs, go, etc) et la robotique. ce sujet dépasse largement le cadre de cet ouvrage. 1.3 ressources pratiques 1.3.1 implémentations logicielles de nombreux logiciels et librairies open source permettent de mettre en œuvre des algorithmes de machine learning. nous en citons ici quelques uns : — les exemples de ce livre ont été écrits en python, grâce à la très utilisée librairie scikit-learn (http: //scikit-learn.org) dont le développement, soutenu entre autres par inria et télécom paris- tech, a commencé en 2007. — de nombreux outils de machine learning sont implémentés en r, et recensés sur la page http: //cran.r-project.org/web/views/machinelearning.html — weka (waikato environment for knowledge analysis, https://www.cs.waikato.ac.nz/ml/weka/) est une suite d’outils de machine learning écrits en java et dont le développement a commencé en 1993. — shogun (http://www.shogun-toolbox.org/) interface de nombreux langages, et en particulier python, octave, r, et c#. shogun, ainsi nommée d’après ses fondateurs søren sonnenburg et gunnar rätsch, a vu le jour en 1999. — de nombreux outils spécialisés pour l’apprentissage profond et le calcul sur des architectures distri- buées ont vu le jour ces dernières années. parmi eux, tensorflow (https://www.tensorflow.org) implémente de nombreux autres algorithmes de machine learning. \f18 1.3.2 jeux de données chapitre 1. présentation du machine learning de nombreux jeux de données sont disponibles publiquement et permettent de se faire la main ou de tester de nouveaux algorithmes de machine learning. parmi les ressources incontournables, citons : — le répertoire de l’université de californie à irvine, uci repository (http://www.ics.uci.edu/ ~mlearn/mlrepository.html) — les ressources listées sur kdnuggets : http://www.kdnuggets.com/datasets/index.html — la plateforme de compétitions en sciences des données kaggle (https://www.kaggle.com/) 1.4 notations autant que faire se peut, nous utilisons dans cet ouvrage les notations suivantes : — les lettres minuscules (x) représentent un scalaire ; — les lettres minuscules surmontées d’une ﬂèche ((cid:126)x) représentent un vecteur ; — les lettres majuscules (x) représentent une matrice, un événement ou une variable aléatoire ; — les lettres calligraphiées (x ) représentent un ensemble ou un espace ; — les indices correspondent à une variable tandis que les exposants correspondent à une observation : est la j-ème variable de la i-ème observation, et correspond à l’entrée xij de la matrice x ; xi j — n est un nombre d’observations, p un nombre de variables, c un nombre de classes ; — [a]+ représente la partie positive de a ∈ r, autrement dit max(0, a). — p(a) représente la probabilité de l’événement a ; — δ est la fonction dirac, c’est-à-dire. — δz est la fonction indicatrice δ : x × x → {0, 1} (u, v) (cid:55)→ (cid:40) 1 0 si u = v sinon ; δz : {vrai, faux} → {0, 1} b (cid:55)→ (cid:40) 1 0 si best vrai sinon ; — (cid:104)., .(cid:105) représente le produit scalaire sur rp ; — (cid:104)., .(cid:105)h représente le produit scalaire sur h ; — m (cid:23) 0 signiﬁe que m est une matrice symétrique semi-déﬁnie positive. points clefs — un algorithme de machine learning est un algorithme qui apprend un modèle à partir d’exemples, par le biais d’un problème d’optimisation. — on utilise le machine learning lorsqu’il est diﬃcile ou impossible de déﬁnir les instructions explicites à donner à un ordinateur pour résoudre un problème, mais que l’on dispose de nombreux exemples illustratifs. — les algorithmes de machine learning peuvent être divisés selon la nature du problème qu’ils cherchent à résoudre, en apprentissage supervisé, non supervisé, semi-supervisé, et par renforcement. \fpour aller plus loin • pour plus de détails sur l’estimation de densité, on consultera le livre de scott (1992). • sur la régression structurée, on pourra se référer à bakir et al. (2007). • l’ouvrage de barto (1998) est un bon point de départ pour se plonger dans le sujet de l’apprentissage par renforcement. 19 bibliographie bakir, g., hofmann, t., schölkopf, b., smola, a. j., taskar, b., et vishwanathan, s. v. n. (2007). https://mitpress.mit.edu/books/ mit press, cambridge, ma. predicting structured data. predicting-structured-data. barto, r. s. et sutton a. g. (1998). reinforcement learning : an introduction. mit press, cambridge, ma. http: //incompleteideas.net/book/the-book-2nd.html. benureau, f. (2015). self-exploration of sensorimotor spaces in robots. thèse de doctorat, université de bor- deaux. samuel, a. l. (1959). some studies in machine learning using the game of checkers. ibm journal of research and development, 44(1.2) :206–226. scott, d. w. (1992). multivariate density estimation. wiley, new york. \fchapitre 2 apprentissage supervisé dans cet ouvrage, nous nous intéresserons principalement aux problèmes d’apprentissage supervisé : il s’agit de développer des algorithmes qui soient capables d’apprendre des modèles prédictifs. à partir d’exemples étiquetés, ces modèles seront capables de prédire l’étiquette de nouveaux objets. le but de ce chapitre est de développer les concepts généraux qui nous permettent de formaliser ce type de problèmes. objectifs — formaliser un problème comme un problème d’apprentissage supervisé ; — choisir une fonction de coût ; — lier la capacité d’un modèle à généraliser avec sa complexité. 2.1 formalisation d’un problème d’apprentissage supervisé un problème d’apprentissage supervisé peut être formalisé de la façon suivante : étant données n ob- servations {(cid:126)x 1, (cid:126)x 2, . . . , (cid:126)x n}, où chaque observation (cid:126)x i est un élément de l’espace des observations x , et leurs étiquettes {y1, y2, . . . , yn}, où chaque étiquette yi appartient à l’espace des étiquettes y, le but de l’apprentissage supervisé est de trouver une fonction f : x → y telle que f ((cid:126)x) ≈ y, pour toutes les paires ((cid:126)x, y) ∈ x × y ayant la même relation que les paires observées. l’ensemble de d = {((cid:126)x i, yi)}i=1,...,n forme le jeu d’apprentissage. nous allons considérer dans cet ouvrage trois cas particuliers pour y : — y = r : on parle d’un problème de régression ; — y = {0, 1} : on parle d’un problème de classiﬁcation binaire, et les observations dont l’étiquette vaut 0 sont appelées négatives tandis que celles dont l’étiquette vaut 1 sont appelées positives. dans certains cas, il sera mathématiquement plus simple d’utiliser y = {−1, 1} ; — y = {1, 2, . . . , c}, c > 2 : on parle d’un problème de classiﬁcation multi-classe. dans de nombreuses situations, on se ramènera au cas où x = rp. on dira alors que les observations soit la j-ème variable sont représentées par p variables. dans ce cas, la matrice x ∈ rn×p telle que xij = xi j de la i-ème observation est appelée matrice de données ou matrice de design. 20 \f2.1. formalisation d’un problème d’apprentissage supervisé 21 le machine learning étant issu de plusieurs disciplines et champs d’applications, on trouvera plusieurs noms pour les mêmes objets. ainsi les variables sont aussi appelées descripteurs, attributs, prédicteurs, ou caractéristiques (en anglais, variables, descriptors, attributes, predictors ou encore features). les observations sont aussi appelées exemples, échantillons ou points du jeu de donnée (en anglais, samples ou data points). enﬁn, les étiquettes sont aussi appelées variables cibles (en anglais, labels, targets ou outcomes). ces concepts sont illustrés sur la ﬁgure 2.1. figure 2.1 – les données d’un problème d’apprentissage supervisé sont organisées en une matrice de de- sign et un vecteur d’étiquettes. les observations sont représentées par leurs variables explicatives. 2.1.1 décision dans le cas d’un problème de classiﬁcation, le modèle prédictif peut prendre directement la forme d’une fonction f à valeurs dans {0, 1}, ou utiliser une fonction intermédiaire g à valeurs réelles, qui associe à une observation un score d’autant plus élevé qu’elle est susceptible d’être positive. ce score peut par exemple être la probabilité que cette observation appartienne à la classe positive. on obtient alors f en seuillant g ; g est appelée fonction de décision. déﬁnition 2.1 (fonction de décision) dans le cadre d’un problème de classiﬁcation binaire, on appelle fonction de décision, ou fonction discriminante, une fonction g : x (cid:55)→ r telle que f ((cid:126)x) = 0 si et seulement si g((cid:126)x) ≤ 0 et f ((cid:126)x) = 1 si et seulement si g((cid:126)x) > 0. cette déﬁnition se généralise dans le cas de la classiﬁcation multi-classe : on a alors c fonctions de (cid:4) décision gc : x (cid:55)→ r telles que f ((cid:126)x) = arg maxc=1,...,c gc((cid:126)x). le concept de fonction de décision permet de partitionner l’espace en régions de décision : déﬁnition 2.2 (région de décision) dans le cas d’un problème de classiﬁcation binaire, la fonction discriminante partitionne l’espace des observations x en deux régions de décision, r0 et r1, telles que r0 = {(cid:126)x ∈ x |g((cid:126)x) ≤ 0} et r1 = {(cid:126)x ∈ x |g((cid:126)x) > 0}. \f22 chapitre 2. apprentissage supervisé dans le cas multi-classe, on a alors c régions de décision rc = {(cid:126)x ∈ x |gc((cid:126)x) = max k gk((cid:126)x)}. les régions de décision sont séparées par des frontières de décision : déﬁnition 2.3 (frontière de décision) dans le cadre d’un problème de classiﬁcation, on appelle fron- tière de décision, ou discriminant, l’ensemble des points de x où une fonction de décision s’annule. dans le cas d’un problème binaire, il y a une seule frontière de décision ; dans le cas d’un problème multi-classe à (cid:4) c classes, il y en a c. (cid:4) 2.1.2 classiﬁcation multi-classe parmi les méthodes d’apprentissage supervisé que présente cet ouvrage, certaines permettent de ré- soudre directement des problèmes de classiﬁcation multi-classe. cependant, tout algorithme de classiﬁ- cation binaire peut être utilisé pour résoudre un problème de classiﬁcation à c classes, par une approche une-contre-toutes ou une approche une-contre-une. déﬁnition 2.4 (une-contre-toutes) étant donné un problème de classiﬁcation multi-classe à c classes, on appelle une-contre-toutes, ou one-versus-all, l’approche qui consiste à entraîner c classiﬁeurs binaires. le c-ème de ces classiﬁeurs utilise tous les exemples de la classe c comme exemples positifs, et toutes les autres observations comme exemples négatifs, pour apprendre une fonction de décision gc. ainsi, chaque classi- ﬁeur apprend à distinguer une classe de toutes les autres. l’étiquette de (cid:126)x est donnée par celle des fonctions de décision qui retourne le score le plus élevé : f ((cid:126)x) = arg max c=1,...,c gc((cid:126)x). (cid:4) déﬁnition 2.5 (une-contre-une) étant donné un problème de classiﬁcation multi-classe à c classes, on appelle une-contre-une, ou one-versus-one, l’approche qui consiste à créer c(c − 1) classiﬁeurs binaires séparant chacun une classe d’une autre, en ignorant tous les autres exemples. soit gck la fonction de déci- sion du classiﬁeur binaire qui sépare la classe c de la classe k. l’étiquette de (cid:126)x est déterminée selon : f ((cid:126)x) = arg max c=1,...,c  gck((cid:126)x)  .  (cid:88)  k(cid:54)=c il est diﬃcile de dire laquelle de ces deux approches est la plus performante. en pratique, le choix sera souvent guidé par des considérations de complexité algorithmique : est-il plus eﬃcace d’entraîner c modèles sur n observations, ou c(c − 1) modèles sur n observations (en supposant les classes équilibrées, c autrement dit que d contient sensiblement autant d’exemples de chaque classe) ? par ailleurs, on prendra aussi en compte le nombre d’exemples disponibles pour chaque classe : s’il est plus eﬃcace d’entraîner un modèle sur peu de données, si ce nombre est trop faible, il sera diﬃcile d’obtenir un modèle de bonne qualité. (cid:4) \f2.2. espace des hypothèses 2.2 espace des hypothèses 23 pour poser un problème d’apprentissage supervisé, il nous faut décider du type de fonctions de modé- lisation que nous allons considérer. déﬁnition 2.6 (espace des hypothèses) on appelle espace des hypothèses l’espace de fonctions f ⊆ y x décrivant les fonctions de modélisation que nous allons considérer. cet espace est choisi en fonction de nos (cid:4) convictions par rapport au problème. dans l’exemple de la ﬁgure 2.2, on pourra décider de se restreindre à des discriminants qui soient des ellipses à axes parallèles aux axes de coordonnées. ainsi, l’espace des hypothèse sera f = {(cid:126)x (cid:55)→ α(x1 − a)2 + β(x2 − b)2 − 1}. figure 2.2 – les exemples positifs (+) et négatifs (x) semblent être séparables par une ellipse. étant donnés un jeu de n observations étiquetées d = {((cid:126)x i, yi)}i=1,...,n et un espace d’hypothèses f. la tâche d’apprentissage supervisé consiste à supposer que les étiquettes yi ont été calculées grâce à une fonction φ : x → y, et à trouver une hypothèse f ∈ f qui approche au mieux la fonction cible φ. pour réaliser une telle tâche, nous allons avoir alors besoin de deux outils supplémentaires : 1. une façon de quantiﬁer la qualité d’une hypothèse, aﬁn de pouvoir déterminer si une hypothèse satis- faisante (voire optimale) a été trouvée. pour cela, nous allons déﬁnir dans la section 2.4 la notion de fonction de coût. 2. une façon de chercher une hypothèse optimale dans f. dans cet ouvrage, nous allons nous concentrer sur les méthodes d’apprentissage par optimisation : les algorithmes d’apprentissage supervisé que nous allons étudier auront pour but de trouver dans f l’hypothèse optimale au sens de la fonction de coût (cf. section 2.3). diﬀérents algorithmes déﬁniront diﬀérents f, et selon les cas cette recherche sera exacte ou approchée. le choix de l’espace des hypothèses est fondamental. en eﬀet, si cet espace ne contient pas la « bonne » fonc- tion, par exemple si l’on choisit comme espace des hypothèses pour les données de la ﬁgure 2.2 l’ensemble des droites, il sera impossible de trouver une bonne fonction de décision. cependant, si l’espace est trop \f24 chapitre 2. apprentissage supervisé générique, il sera plus diﬃcile et intensif en temps de calcul d’y trouver une bonne fonction de modélisa- tion. 2.3 minimisation du risque empirique résoudre un problème d’apprentissage supervisé revient à trouver une fonction f ∈ f dont les pré- dictions soient les plus proches possibles des véritables étiquettes, sur tout l’espace x . on utilise pour formaliser cela la notion de fonction de coût : déﬁnition 2.7 (fonction de coût) une fonction de coût l : y ×y → r, aussi appelée fonction de perte ou fonction d’erreur (en anglais : cost function ou loss function) est une fonction utilisée pour quantiﬁer la qualité d’une prédiction : l(y, f ((cid:126)x)) est d’autant plus grande que l’étiquette f ((cid:126)x) est éloignée de la vraie valeur (cid:4) y. étant donnée une fonction de coût l, nous cherchons donc f qui minimise ce coût sur l’ensemble des valeurs possibles de (cid:126)x ∈ x , ce qui est formalisé par la notion de risque. déﬁnition 2.8 (risque) dans le cadre d’un problème d’apprentissage supervisé, on appelle risque l’es- pérance d’une fonction de coût (voir chapitre 4 pour la modélisation probabiliste d’un problème d’appren- tissage supervisé) : r(h) = ex [l(h((cid:126)x), y)]. (cid:4) la fonction f que nous cherchons vériﬁe donc f = arg minh∈f e[l(h((cid:126)x), y)]. ce problème est géné- ralement insoluble sans plus d’hypothèses : si nous connaissions les étiquettes de tous les points de x , nous n’aurions pas besoin d’apprentissage automatique. étant données n observations étiquetées {((cid:126)x i, yi)}i=1,...,n, on approchera donc le risque par son estimation sur ces données observées déﬁnition 2.9 (risque empirique) dans le cadre d’un problème d’apprentissage supervisé, étant don- nées n observations étiquetées {((cid:126)x i, yi)}i=1,...,n, on appelle risque empirique l’estimateur rn(h) = 1 n n (cid:88) i=1 l(h((cid:126)x i), yi). le prédicteur par minimisation du risque empirique est donc f = arg min h∈f 1 n n (cid:88) i=1 l(h((cid:126)x i), yi). (cid:4) (2.1) selon le choix de f, l’équation 2.1 peut avoir une solution analytique explicite. cela ne sera pas souvent le cas ; cependant on choisira souvent une fonction de coût convexe aﬁn de résoudre plus facilement ce problème d’optimisation (voir l’annexe a sur l’optimisation convexe). la minimisation du risque empirique est généralement un problème mal posé au sens de hadamard, c’est-à-dire qu’il n’admet pas une solution unique dépendant de façon continue des conditions initiales. il se peut par exemple qu’un nombre inﬁni de solutions minimise le risque empirique à zéro (voir ﬁgure 2.3). \f2.4. fonctions de coût 25 figure 2.3 – une inﬁnité de droites séparent parfaitement les points positifs (+) des points négatifs (x). chacune d’entre elles a un risque empirique nul. de plus, le prédicteur par minimisation du risque empirique n’est pas statistiquement consistant. rap- pelons qu’un estimateur θn (dépendant de n observations) d’un paramètre θ est consistant s’il converge en probabilité vers θ quand n croit vers l’inﬁni : ∀(cid:15) > 0, lim n→∞ p(|θn − θ| ≥ (cid:15)) = 0. la loi des grands nombres nous garantit que le risque empirique converge vers le risque : ∀h ∈ f, rn(h) −−−→ n→∞ r(h). (2.2) (2.3) cela ne suﬃt cependant pas à garantir que le minimum du risque empirique minh∈f rn(h) converge vers le minimum du risque. en eﬀet, si f est l’espace des fonctions mesurables, minh∈f rn(h) vaut générale- ment 0, ce qui n’est pas le cas de r(h). il n’y a donc aucune garantie que la fonction fn qui minimise rn(h) soit un bon estimateur du minimiseur f de r(h). la consistance de la minimisation du risque empirique dépend de l’espace des versions f. l’étude de cette consistance est un des principaux éléments de la théorie de l’apprentissage de vapnik-chervonenkis, qui dépasse l’ambition de cet ouvrage. 2.4 fonctions de coût il existe de nombreuses fonctions de coût. le choix d’une fonction de coût dépend d’une part du pro- blème en lui-même, autrement dit de ce que l’on trouve pertinent pour le cas pratique considéré, et d’autre part de considérations pratiques : peut-on ensuite résoudre le problème d’optimisation qui résulte de ce choix de façon suﬃsamment exacte et rapide ? cette section présente les fonctions de coûts les plus cou- ramment utilisées et on pourra s’y référer tout au long de la lecture de cet ouvrage. \f26 chapitre 2. apprentissage supervisé 2.4.1 fonctions de coût pour la classiﬁcation binaire pour déﬁnir des fonctions de coût pour la classiﬁcation binaire, on considèrera souvent y = {−1, 1}. en eﬀet, dans le cas d’une classiﬁcation parfaite, le produit yf ((cid:126)x) est alors égal à 1. coût 0/1 pour la classiﬁcation binaire déﬁnition 2.10 (coût 0/1 – classiﬁcation binaire) dans le cas d’une fonction f à valeurs binaires, on appelle fonction de coût 0/1, ou 0/1 loss, la fonction suivante : l0/1 : y × y → r (cid:40) 1 0 y, f ((cid:126)x) (cid:55)→ si f ((cid:126)x) (cid:54)= y sinon. (cid:4) en utilisant y = {−1, 1}, on peut la réécrire de la manière suivante : l0/1(y, f ((cid:126)x)) = 1 − yf ((cid:126)x) 2 . quand on utilise cette fonction de coût, le risque empirique est le nombre moyen d’erreurs de prédiction. si l’on considère pour f une fonction de décision (à valeurs réelles) plutôt qu’une fonction de prédiction à valeurs binaires, on peut déﬁnir la fonction de coût 0/1 comme suit : coût 0/1 pour la régression déﬁnition 2.11 (coût 0/1 – régression) quand on considère une fonction de décision à valeurs réelles, on appelle fonction de coût 0/1, ou 0/1 loss, la fonction suivante : l0/1 : y × r → r (cid:40) 1 0 y, f ((cid:126)x) (cid:55)→ si yf ((cid:126)x) ≤ 0 sinon. l’inconvénient de cette fonction de coût est qu’elle n’est pas dérivable, ce qui compliquera les pro- blèmes d’optimisation l’utilisant. de plus, elle n’est pas très ﬁne : l’erreur est la même que f ((cid:126)x) soit très proche ou très loin du seuil de décision. rappelons que pour une classiﬁcation parfaite, quand y = {−1, 1}, yf ((cid:126)x) = 1. on peut ainsi déﬁnir une fonction de coût qui soit d’autant plus grande que yf ((cid:126)x) s’éloigne de 1 à gauche ; on considère qu’il n’y a pas d’erreur si yf ((cid:126)x) > 1. cela conduit à la déﬁnition d’erreur hinge, ainsi appelée car elle forme un coude, ou une charnière (cf. ﬁgure 2.4). (cid:4) erreur hinge \f2.4. fonctions de coût 27 déﬁnition 2.12 (erreur hinge) on appelle fonction d’erreur hinge, ou hinge loss, la fonction suivante : lhinge : {−1, 1} × r → r (cid:40) 0 1 − yf ((cid:126)x) y, f ((cid:126)x) (cid:55)→ si yf ((cid:126)x) ≥ 1 sinon. de manière plus compacte, l’erreur hinge peut aussi s’écrire lhinge(f ((cid:126)x), y) = max (0, 1 − yf ((cid:126)x)) = [1 − yf ((cid:126)x)]+ . on peut aussi considérer que f ((cid:126)x) doit être la plus proche possible de 1 pour les observations positives (et −1 pour les observations négatives). ainsi, on pénalisera aussi les cas où yf ((cid:126)x) s’éloigne de 1 par la droite, ce que l’on peut faire avec le coût quadratique. coût quadratique pour la classiﬁcation binaire déﬁnition 2.13 (coût quadratique) dans le cadre d’un problème de classiﬁcation binaire, on appelle coût quadratique, ou square loss, la fonction suivante : (cid:4) lsquare : {−1, 1} × r → r y, f ((cid:126)x) (cid:55)→ (1 − yf ((cid:126)x))2 . (cid:4) enﬁn, on peut chercher à déﬁnir une fonction de décision dont la valeur absolue quantiﬁe notre conﬁance en sa prédiction. on cherche alors à ce que yf ((cid:126)x) soit la plus grande possible, et on utilise le coût logistique. coût logistique déﬁnition 2.14 (coût logistique) on appelle fonction de coût logistique, ou logistic loss, la fonction sui- vante : llog : {−1, 1} × r → r y, f ((cid:126)x) (cid:55)→ log (1 + exp(−yf ((cid:126)x))) . (cid:4) si l’on préfère utiliser y = {0, 1}, le coût logistique est équivalent à l’entropie croisée. entropie croisée déﬁnition 2.15 (entropie croisée) dans le cas binaire, on appelle entropie croisée, ou cross-entropy, la fonction suivante : lh : {0, 1}×]0, 1[ → r y, f ((cid:126)x) (cid:55)→ −y log f ((cid:126)x) − (1 − y) log(1 − f ((cid:126)x)). (cid:4) \f28 chapitre 2. apprentissage supervisé l’entropie croisée est issue de la théorie de l’information, d’où son nom. en considérant que la véritable classe de (cid:126)x est modélisée par une distribution q, et sa classe prédite par une distribution p , nous allons chercher à modéliser p de sorte qu’elle soit la plus proche possible de q. on utilise pour cela la divergence de kullback-leibler : remarque kl(q||p ) = (cid:88) c=0,1 q(y = c|(cid:126)x) log q(y = c|(cid:126)x p (y = c|(cid:126)x) = − (cid:88) c=0,1 q(y = c|(cid:126)x) log p (y = c|(cid:126)x) + (cid:88) c=0,1 q(y = c|(cid:126)x) log q(y = c|(cid:126)x) comme q(y = c|(cid:126)x) vaut soit 0 (c n’est pas la classe de (cid:126)x) soit 1 (dans le cas contraire), le deuxième terme de cette expression est nul et on retrouve ainsi la déﬁnition ci-dessus de l’entropie croisée. les fonctions de perte pour la classiﬁcation binaire sont illustrées sur la ﬁgure 2.4. figure 2.4 – fonctions de perte pour la classiﬁcation binaire. 2.4.2 coûts pour la classiﬁcation multi-classe entropie croisée la déﬁnition de l’entropie croisée dans le cas binaire se généralise naturellement au cas multi-classe en considérant c fonctions de décision fc : x → y. \f2.4. fonctions de coût 29 déﬁnition 2.16 (entropie croisée) dans le cas multi-classe, on appelle entropie croisée, ou cross-entropy, la fonction suivante : lh : {1, 2, . . . , c} × r → r y, f ((cid:126)x) (cid:55)→ − c (cid:88) c=1 δ(y, c) log fc((cid:126)x) = − log fy((cid:126)x). (cid:4) extension de la fonction d’erreur hinge plusieurs propositions permettent de généraliser la fonction d’erreur hinge au cas multi-classe. dans tous les cas, il s’agit d’assurer que la fonction de décision pour la véritable classe de (cid:126)x, fy((cid:126)x), prend une valeur supérieure à toutes les autres fonctions de décision fc((cid:126)x), c (cid:54)= y. weston and watkins (1999) proposent la déﬁnition suivante : lhinge(y, f ((cid:126)x)) = (cid:88) c(cid:54)=y [1 + fc((cid:126)x) − fy((cid:126)x)]+ . crammer and singer (2001), eux, utilisent un maximum plutôt qu’une somme : déﬁnition suivante : lhinge(y, f ((cid:126)x)) = (cid:20) 1 + max c(cid:54)=y (cid:21) fc((cid:126)x) − fy((cid:126)x) . + ces fonctions de coût sont rarement utilisées en pratique, et on a tendance à leur préférer l’utilisation de la perte hinge binaire dans une approche une-contre-une ou une-contre-toutes. 2.4.3 coûts pour la régression dans le cas d’un problème de régression, nous considérons maintenant y = r. le but de notre fonction de coût est de pénaliser les fonctions de prédiction f dont la valeur est éloignée de la valeur cible (cid:126)x. coût quadratique déﬁnition 2.17 (coût quadratique) on appelle fonction de coût quadratique, ou quadratic loss, ou encore squared error, la fonction suivante : lse : r × r → r 1 2 y, f ((cid:126)x) (cid:55)→ (y − f ((cid:126)x))2 . permet d’éviter d’avoir des coeﬃcients multiplicateurs quand on dérive le risque em- (cid:4) le coeﬃcient 1 2 pirique pour le minimiser. \f30 coût (cid:15)-insensible chapitre 2. apprentissage supervisé le coût quadratique a tendance à être dominé par les valeurs aberrantes : dès que quelques observations dans le jeu de données ont une prédiction très éloignée de leur étiquette réelle, la qualité de la prédiction sur les autres observations importe peu. on peut ainsi lui préférer le coût absolu : déﬁnition 2.18 (coût absolu) on appelle fonction de coût absolu, ou absolute error, la fonction suivante : lae : r × r → r y, f ((cid:126)x) (cid:55)→ |y − f ((cid:126)x)|. (cid:4) avec cette fonction de coût, même les prédictions très proches de la véritable étiquette sont pénalisées (même si elles le sont faiblement). cependant, il est numériquement quasiment impossible d’avoir une prédiction exacte. le coût (cid:15)-insensible permet de remédier à cette limitation. déﬁnition 2.19 (coût (cid:15)-insensible) étant donné (cid:15) > 0, on appelle fonction de coût (cid:15)-insensible, ou (cid:15)- insensitive loss, la fonction suivante : l(cid:15) : r × r → r y, f ((cid:126)x) (cid:55)→ max (0, |y − f ((cid:126)x)| − (cid:15)) . (cid:4) coût de huber le coût (cid:15)-insensible n’est dérivable ni en −(cid:15) ni en +(cid:15), ce qui complique l’optimisation du risque empi- rique. la fonction de coût de huber permet d’établir un bon compromis entre le coût quadratique (dérivable en 0) et le coût absolu (qui n’explose pas dans les valeurs extrêmes). déﬁnition 2.20 (coût de huber) on appelle fonction de coût de huber, ou huber loss, la fonction suivante : lhuber : r × r → r (cid:40) 1 y, f ((cid:126)x) (cid:55)→ 2 (y − f ((cid:126)x))2 (cid:15)|y − f ((cid:126)x)| − 1 2 (cid:15)2 si |y − f ((cid:126)x)| < (cid:15) sinon. (cid:4) le terme − 1 2 (cid:15)2 permet d’assurer la continuité de la fonction. les fonctions de coût pour la régression sont illustrées sur la ﬁgure 2.5. 2.5 généralisation et sur-apprentissage 2.5.1 généralisation imaginons un algorithme qui, pour prédire l’étiquette d’une observation (cid:126)x, retourne son étiquette si (cid:126)x appartient aux données dont l’étiquette est connue, et une valeur aléatoire sinon. cet algorithme aura une erreur empirique minimale quelle que soit la fonction de coût choisie, mais fera de très mauvaises prédictions pour toute nouvelle observation. ce n’est pas vraiment ce que l’on a en tête quand on parle d’apprentissage. \f2.5. généralisation et sur-apprentissage 31 figure 2.5 – fonctions de coût pour un problème de régression. ainsi, évaluer un algorithme de machine learning sur les données sur lesquelles il a appris ne nous permet absolument pas de savoir comment il se comportera sur de nouvelles données, en d’autres mots, sa capacité de généralisation. c’est un point essentiel ! attention déﬁnition 2.21 (généralisation) on appelle généralisation la capacité d’un modèle à faire des prédic- (cid:4) tions correctes sur de nouvelles données, qui n’ont pas été utilisées pour le construire. 2.5.2 sur-apprentissage l’exemple, certes extrême, que nous avons pris plus haut, illustre que l’on peut facilement mettre au point une procédure d’apprentissage qui produise un modèle qui fait de bonnes prédictions sur les données utilisées pour le construire, mais généralise mal. au lieu de modéliser la vraie nature des objets qui nous intéressent, un tel modèle capture aussi (voire surtout) un bruit qui n’est pas pertinent pour l’application considérée. en eﬀet, dans tout problème d’apprentissage automatique, nos données sont inévitablement bruitées — par des erreurs de mesure dues à la faillibilité des capteurs utilisés pour mesurer les variables par lesquelles on représente nos données, ou à la faillibilité des opérateurs humains qui ont entré ces mesures dans une base de données ; \f32 chapitre 2. apprentissage supervisé — par des erreurs d’étiquetage (souvent appelés teacher’s noise en anglais) dues à la faillibilité des opéra- teurs humains qui ont étiqueté les données ; — enﬁn, parce que les variables mesurées ne suﬃsent pas à modéliser le phénomène qui nous intéresse, soit qu’on ne les connaisse pas, soit qu’elles soient coûteuses à mesurer. exemple supposons que nous voulions classiﬁer des photographies selon qu’elles représentent des pandas ou non. chaque image est représentée par les valeurs rgb des pixels qui la composent. nous aimerions faire en sorte que le modèle que nous construisons capture la véritable nature d’un panda. nous pouvons cependant être exposés à des erreurs de mesure (erreurs techniques des capteurs de l’appareil photo) ainsi que des erreurs d’étiquetage (erreurs de la personne qui a dû décider, pour chaque photo, s’il s’agissait ou non d’un panda, et a pu cliquer sur le mauvais choix, ou confondre un panda avec un ours). de plus, nous sommes limités par notre choix de variables : nos pixels ne capturent pas directement le fait qu’un panda est un animal rondouillard, avec un masque autour des yeux, généralement entouré de bambous. remarque on voit ici que le choix des variables utilisées pour représenter les données est une étape très impor- tante du processus de modélisation. nous verrons d’ailleurs au chapitre 7 que la puissance des réseaux de neurones profonds qui sont si populaires de nos jours vient de leur capacité à extraire des données une re- présentation pertinente. les techniques présentées dans le chapitre 11 pourront être utilisées pour guider le choix des variables prédictives à utiliser. déﬁnition 2.22 (sur-apprentissage) on dit d’un modèle qui, plutôt que de capturer la nature des objets à étiqueter, modélise aussi le bruit et ne sera pas en mesure de généraliser qu’il sur-apprend. en (cid:4) anglais, on parle d’overﬁtting. un modèle qui sur-apprend est généralement un modèle trop complexe, qui « colle » trop aux données et capture donc aussi leur bruit. à l’inverse, il est aussi possible de construire un modèle trop simple, dont les performances ne soient bonnes ni sur les données utilisées pour le construire, ni en généralisation. déﬁnition 2.23 (sous-apprentissage) on dit d’un modèle qui est trop simple pour avoir de bonnes performances même sur les données utilisées pour le construire qu’il sous-apprend. en anglais, on parle (cid:4) d’underﬁtting. ces concepts sont illustrés sur la ﬁgure 2.6a pour un problème de classiﬁcation binaire et la ﬁgure 2.6b pour un problème de régression. 2.5.3 compromis biais-variance pour mieux comprendre le risque d’un modèle f : x → y, nous pouvons le comparer à l’erreur minimale r∗ qui peut être atteinte par n’importe quelle fonction mesurable de x dans y : c’est ce qu’on appelle l’excès d’erreur, et que l’on peut décomposer de la façon suivante : r(f ) − r∗ = (cid:20) (cid:21) (cid:20) r(f ) − min h∈f r(h) + min h∈f (cid:21) r(h) − r∗ (2.4) \f2.5. généralisation et sur-apprentissage 33 (a) pour séparer les observations négatives (x) des observations positives (+), la droite pointillée sous- apprend. la frontière de séparation en trait plein ne fait aucune erreur sur les données mais est susceptible de sur-apprendre. la frontière de séparation en trait dis- continu est un bon compromis. (b) les étiquettes y des observations (représentées par des points) ont été générées à partir d’un polynôme de degré d = 3. le modèle de degré d = 2 approxime très mal les données et sous-apprend, tandis que celui de de- gré d = 13, dont le risque empirique est plus faible, sur- apprend. figure 2.6 – sous-apprentissage et sur-apprentissage le premier terme, r(f ) − minh∈f r(h), quantiﬁe la distance entre le modèle f et le modèle optimal sur f. c’est ce que l’on appelle l’erreur d’estimation. le second terme, minh∈f r(h) − r∗, quantiﬁe la qualité du modèle optimal sur f, autrement dit, la qualité du choix de l’espace des hypothèses. c’est ce que l’on appelle l’erreur d’approximation. si f est l’ensemble des fonctions mesurables, alors l’erreur d’approximation est nulle. ainsi, l’écriture 2.4 permet de décomposer l’erreur entre un terme qui découle de la qualité de l’espace des hypothèses et un autre qui découle de la qualité de la procédure d’optimisation utilisée. en pratique, sauf dans des cas très particuliers où cela est rendu possible par construction, il n’est pas possible de calcu- ler ces termes d’erreur. cependant, cette écriture nous permet de comprendre le problème suivant : choisir un espace des hypothèses plus large permet généralement de réduire l’erreur d’approximation, car un mo- dèle plus proche de la réalité a plus de chances de se trouver dans cet espace. cependant, puisque cet espace est plus vaste, la solution optimale y est aussi généralement plus diﬃcile à trouver : l’erreur d’estimation, elle, augmente. c’est dans ce cas qu’il y a sur-apprentissage. un espace des hypothèses plus large permet généralement de construire des modèles plus complexes : par exemple, l’ensemble des droites vs. l’ensemble des polynômes de degré 9 (cf. ﬁgure 2.6b). c’est une variante du principe du rasoir d’ockham, selon lequel les hypothèses les plus simples sont les plus vraisem- blables. il y a donc un compromis entre erreur d’approximation et erreur d’estimation : il est diﬃcile de ré- duire l’une sans augmenter l’autre. ce compromis est généralement appelé compromis biais-variance : l’er- reur d’approximation correspond au biais de la procédure d’apprentissage, tandis que l’erreur d’estimation correspond à sa variance. on retrouvera ce compromis dans l’estimation bayésienne de paramètres à la \f34 section 4.3.3. chapitre 2. apprentissage supervisé exemple considérons pour un problème de régression un espace des hypothèses naïf qui ne contient que des fonctions constantes. supposons que les étiquettes soient générées par une distribution normale centrée en a. quelles que soient les données observées, la procédure d’apprentissage va construire un modèle qui retourne a quelle que soit l’observation concernée : la variance de la procédure par rapport au jeu de don- nées est très faible. à l’inverse, comme la fonction de prédiction apprise est très peu sensible au jeu de données, il y a un biais très important qui conduit à construire des prédicteurs qui retournent a pour toutes les observations. 2.5.4 régularisation plus un modèle est simple, et moins il a de chances de sur-apprendre. pour limiter le risque de sur- apprentissage, il est donc souhaitable de limiter la complexité d’un modèle. c’est ce que permet de faire la régularisation, une technique que nous verrons plus en détail au cours de cet ouvrage (à commencer par le chapitre 6), et qui consiste à ajouter au terme d’erreur que l’on cherche à minimiser un terme qui mesure la complexité du problème (par exemple, dans le cas précédent, le degré du polynôme ou le nombre de coeﬃcients du modèle). ainsi, un modèle complexe qui a une erreur empirique faible peut être défavorisé face à une modèle plus simple, même si celui-ci présente une erreur empirique plus élevée. points clefs — les trois ingrédients d’un algorithme d’apprentissage supervisé sont : — l’espace des hypothèses, — la fonction de coût, — l’algorithme d’optimisation qui permet de trouver l’hypothèse optimale au sens de la fonction de coût sur les données (minimisation du risque empirique). — le compromis biais-variance traduit le compromis entre l’erreur d’approximation, correspondant au biais de l’algorithme d’apprentissage, et l’erreur d’estimation, correspondant à sa variance. — la généralisation et le sur-apprentissage sont des préoccupations majeures en machine learning : comment s’assurer que des modèles entraînés pour minimiser leur erreur de prédiction sur les don- nées observées seront généralisables aux données pour lesquelles il nous intéresse de faire des pré- dictions ? pour aller plus loin • la notion de complexité d’un modèle a été formalisée par vladimir vapnik et alexey chervonenkis dans les années 1970, et est détaillée par exemple dans l’ouvrage de vapnik (1995). • pour en savoir plus sur la théorie de l’apprentissage, on pourra se référer au livre de kearns et va- zirani (1994). • on trouvera une discussion détaillée du compromis biais-variance dans friedman (1997). \f35 bibliographie crammer, k. and singer, y. (2001). on the algorithmic implementation of multiclass kernel-based vector machines. journal of machine learning research, 2 :265–292. friedman, j. h. (1997). on bias, variance, 0/1-loss and the curse of dimensionality. data mining and knowledge discovery, 1 :55–77. kearns, m. j. et vazirani, u. v. (1994). an introduction to computational learning theory. mit press, cambridge, ma. vapnik, v. n. (1995). the nature of statistical learning theory. springer, new york. weston, j. and watkins, c. (1999). support vector machines for multi-class pattern recognition. in european symposium on artiﬁcial neural networks. \fchapitre 3 sélection de modèle et évaluation nous avons formalisé au chapitre 2 l’apprentissage supervisé comme la recherche d’un modèle dont l’erreur empirique est minimale sur un ensemble donné d’observations. cependant, minimiser cette erreur empirique ne garantit pas de minimiser l’erreur du modèle sur la totalité de l’espace des données. en eﬀet, dans une situation de sur-apprentissage, l’erreur du modèle sera sous-estimée. c’est cependant cette erreur, ou, en d’autres mots, notre capacité à faire des prédictions sur des choses qui ne sont pas connues, qui nous intéresse. ce chapitre présente comment mettre en place un cadre expérimental qui permette d’évaluer un modèle en évitant le biais du sur-apprentissage. dans cette optique, nous veillerons à distinguer l’évaluation d’un modèle, qui consiste à déterminer sa performance sur l’espace des données dans sa totalité, de sa sélection, qui consiste à choisir le meilleur modèle parmi plusieurs. objectifs — concevoir un cadre expérimental dans lequel sélectionner un modèle d’apprentissage supervisé ; — choisir un ou des critères d’évaluation d’un modèle d’apprentissage supervisé ; — estimer la performance en généralisation d’un modèle d’apprentissage supervisé. le théorème du no free lunch de wolpert et macready (1997) indique qu’aucun algorithme de machine learning ne peut bien fonctionner pour tous les problèmes d’apprentissage : un algorithme qui fonctionne bien sur un type particulier de problèmes le compensera en fonctionnant moins bien sur d’autres types de problèmes. en d’autres termes, il n’y a pas de « baguette magique » qui puisse résoudre tous nos problèmes de machine learning, et il est donc essentiel, pour un problème donné, de tester plusieurs possibilités aﬁn de sélectionner le modèle optimal. notons au passage que plusieurs critères peuvent intervenir dans ce choix : non seulement celui de la qualité des prédictions, qui nous intéresse dans ce chapitre, mais aussi celui des ressources de calcul nécessaires, qui peuvent être un facteur limitant en pratique. 3.1 estimation empirique de l’erreur de généralisation l’erreur empirique mesurée sur les observations qui ont permis de construire le modèle est un mauvais estimateur de l’erreur du modèle sur l’ensemble des données possibles, ou erreur de généralisation : si le modèle sur-apprend, cette erreur empirique peut être proche de zéro voire nulle, tandis que l’erreur de généralisation peut être arbitrairement grande. 36 \f3.1. estimation empirique de l’erreur de généralisation 37 3.1.1 jeu de test il est donc indispensable d’utiliser pour évaluer un modèle des données étiquetées qui n’ont pas servi à le construire. la manière la plus simple d’y parvenir est de mettre de côté une partie des observations, réservées à l’évaluation du modèle, et d’utiliser uniquement le reste des données pour le construire. déﬁnition 3.1 (jeu de test, jeu d’entraînement) étant donné un jeu de données d = {((cid:126)x i, yi)}i=1,...,n, partitionné en deux jeux dtr et dte, on appelle jeu d’entraînement (training set en anglais) l’ensemble dtr uti- lisé pour entraîner un modèle prédictif, et jeu de test (test set en anglais) l’ensemble dte utilisé pour son (cid:4) évaluation. comme nous n’avons pas utilisé le jeu de test pour entraîner notre modèle, il peut être considéré comme un jeu de données « nouvelles ». la perte calculée sur ce jeu de test est un estimateur de l’erreur de géné- ralisation. 3.1.2 jeu de validation considérons maintenant la situation dans laquelle nous voulons choisir entre k modèles. nous pouvons alors entraîner chacun des modèles sur le jeu de données d’entraînement, obtenant ainsi k fonctions de décision f1, f2, . . . , fk, puis calculer l’erreur de chacun de ces modèles sur le jeu de test. nous pouvons ensuite choisir comme modèle celui qui a la plus petite erreur sur le jeu de test : ˆf = arg min k=1,...,k 1 |dte| (cid:88) (cid:126)x,y∈dte l(y, fk((cid:126)x)) (3.1) mais quelle est son erreur de généralisation ? comme nous avons utilisé dte pour sélectionner le modèle, il ne représente plus un jeu indépendant composé de données nouvelles, inutilisées pour déterminer le modèle. la solution est alors de découper notre jeu de données en trois parties : — un jeu d’entraînement dtr sur lequel nous pourrons entraîner nos k algorithmes d’apprentissage ; — un jeu de validation (validation set en anglais) dval sur lequel nous évaluerons les k modèles ainsi obtenus, aﬁn de sélectionner un modèle déﬁnitif ; — un jeu de test dte sur lequel nous évaluerons enﬁn l’erreur de généralisation du modèle choisi. on voit ici qu’il est important de distinguer la sélection d’un modèle de son évaluation : les faire sur les mêmes données peut nous conduire à sous-estimer l’erreur de généralisation et le sur-apprentissage du modèle choisi. une fois un modèle sélectionné, on peut le ré-entraîner sur l’union du jeu d’entraînement et du jeu de validation aﬁn de construire un modèle ﬁnal. remarque 3.1.3 validation croisée la séparation d’un jeu de données en un jeu d’entraînement et un jeu de test est nécessairement ar- bitraire. nous risquons ainsi d’avoir, par hasard, créé des jeux de données qui ne sont pas représentatifs. pour éviter cet écueil, il est souhaitable de reproduire plusieurs fois la procédure, puis de moyenner les résultats obtenus aﬁn de moyenner ces eﬀets aléatoires. le cadre le plus classique pour ce faire est celui de la validation croisée, illustré sur la ﬁgure 3.1 \f38 chapitre 3. sélection de modèle et évaluation déﬁnition 3.2 (validation croisée) étant donné un jeu d de n observations, et un nombre k, on appelle validation croisée la procédure qui consiste à 1. partitionner d en k parties de tailles sensiblement similaires, d1, d2, . . . , dk 2. pour chaque valeur de k = 1, . . . , k, l(cid:54)=k dl — entraîner un modèle sur (cid:83) — évaluer ce modèle sur dk. chaque partition de d en deux ensembles dk et (cid:83) l(cid:54)=k dl est appelée un fold de la validation croisée. (cid:4) chaque observation étiquetée du jeu d appartient à un unique jeu de test, et à (k − 1) jeux d’entraîne- ment. ainsi, cette procédure génère une prédiction par observation de d. pour conclure sur la performance du modèle, on peut : — soit évaluer la qualité des prédictions sur d ; — soit évaluer la qualité de chacun des k prédicteurs sur le jeu de test dk correspondant, et moyenner leurs performances. cette deuxième approche permet aussi de rapporter l’écart-type de ces perfor- mances, ce qui permet de se faire une meilleure idée de la variabilité de la qualité des prédictions en fonction des données d’entraînement. figure 3.1 – une validation croisée en 5 folds : chaque observation appartient à un des 5 jeux de validation (en blanc) et aux 4 autres jeux d’entraînement (en noir). stratiﬁcation déﬁnition 3.3 (validation croisée stratiﬁée) une validation croisée est dite stratiﬁée si la moyenne des étiquettes des observations est sensiblement la même dans chacun des k sous-ensembles dk : 1 |d1| (cid:88) yi ≈ i∈d1 1 |d2| (cid:88) i∈d2 yi ≈ · · · ≈ 1 |dk| (cid:88) yi ≈ i∈dk 1 |d| (cid:88) yi. i∈d (cid:4) dans le cas d’un problème de classiﬁcation, cela signiﬁe que la proportion d’exemples de chaque classe est la même dans chacun des dk. cette proportion est donc aussi la même que dans le jeu de données d complet. l’intérêt de cette procédure est de faire en sorte que la distribution des observations au sein de chaque dk soit la même qu’au sein du jeu de données d. imaginons que par malchance un des folds ne contienne que des exemples positifs dans son jeu d’entraînement et que des exemples négatifs dans son jeu de test : il est vraisemblable que, sur ce fold, tout modèle apprenne à prédire que tout est positif et ait une très mauvaise performance. \f3.1. estimation empirique de l’erreur de généralisation 39 leave-one-out un algorithme d’apprentissage apprendra d’autant mieux qu’il y a d’avantage de données disponibles pour l’entraînement : plus on connaît d’étiquettes pour des observations de l’espace x , plus on peut contraindre le modèle à les respecter. or pour un jeu de données de taille n, un jeu de test d’une validation croisée à k folds contient (k−1)n points : les modèles entraînés apprendront d’autant mieux sur chacun des folds qu’ils sont grands, ce qui nous pousse à considérer le cas où k = n. k déﬁnition 3.4 (validation croisée leave-one-out) une validation croisée dont le nombre de folds est égal au nombre d’observations dans le jeu d’entraînement, et dont chaque fold est donc composé d’un jeu d’entraînement de taille n − 1 et d’un jeu de test de taille 1, est appelée leave one out : on met de côté, pour (cid:4) chaque fold, un unique exemple. l’évaluation par leave-one-out présente deux inconvénients. tout d’abord, elle requiert un grand temps de calcul : on entraîne n modèles, chacun sur n−1 observations, au lieu de (dans le cas k = 10) 10 modèles, chacun sur 90% des observations. de plus, les jeux d’entraînement ainsi formés sont très similaires entre eux. les modèles entraînés seront eux aussi très similaires, et généralement peu diﬀérents d’un modèle entraîné sur l’intégralité du jeu de données. par contre, les jeux de test seront disjoints, et les performances pourront ainsi avoir une grande variabilité, ce qui compliquera leur interprétation. 3.1.4 bootstrap une autre façon de rééchantillonner les données aﬁn d’estimer l’erreur de généralisation est connue sous le nom de bootstrap. déﬁnition 3.5 (bootstrap) étant donné un jeu d de n observations, et un nombre b, on appelle boots- trap la procédure qui consiste à créer b échantillons d1, d2, . . . , db de d, obtenus chacun en tirant n exemples de d avec remplacement. ainsi, chaque exemple peut apparaître plusieurs fois, ou pas du tout, (cid:4) dans db. le bootstrap est une procédure couramment utilisée en statistiques pour estimer un paramètre en fonc- tion de son estimation sur les b échantillons. en la suivant, on pourrait entraîner le modèle à évaluer sur chaque échantillon db, puis évaluer sa performance sur l’intégralité de d. cependant, cette estimation se- rait biaisée par la présence d’une partie des exemples de d dans db. il faut donc se limiter aux exemples de d \\ db. en pratique, cette procédure est jugée trop complexe pour être souvent appliquée. la probabilité que ((cid:126)x i, yi) apparaisse dans db peut être calculée comme le complémentaire à 1 de la probabilité que ((cid:126)x i, yi) ne soit tiré aucune des n fois. la probabilité de ((cid:126)x i, yi) soit tiré une fois vaut 1 . n ainsi remarque p[((cid:126)x i, yi) ∈ db] = 1 − (cid:18) 1 − (cid:19)n . 1 n quand n est grand, cette probabilité vaut donc environ 1 − e−1 ≈ 0.632, car la limite en +∞ de (cid:0)1 + x n vaut ex. (cid:1)n ainsi, db contient environ deux tiers des observations de d. \f40 chapitre 3. sélection de modèle et évaluation 3.2 critères de performance il existe de nombreuses façons d’évaluer la performance prédictive d’un modèle d’apprentissage super- visé. cette section présente les principaux critères utilisés. 3.2.1 matrice de confusion et critères dérivés comme nous l’avons vu, le nombre d’erreurs de classiﬁcation permet d’évaluer la qualité d’un mo- dèle prédictif. notons que l’on préférera généralement décrire le nombre d’erreurs comme une fraction du nombre d’exemples : un taux d’erreur de 1% est plus parlant qu’un nombre absolu d’erreurs. mais toutes les erreurs ne se valent pas nécessairement. prenons l’exemple d’un modèle qui prédise si oui ou non une radiographie présente une tumeur inquiétante : une fausse alerte, qui sera ensuite inﬁrmée par des examens complémentaires, est moins problématique que de ne pas déceler la tumeur et de ne pas traiter la personne concernée. les performances d’un modèle de classiﬁcation, binaire comme multi-classe, peuvent être résumée dans une matrice de confusion. déﬁnition 3.6 (matrice de confusion) étant donné un problème de classiﬁcation, on appelle matrice de confusion une matrice m contenant autant de lignes que de colonnes que de classes, et dont l’entrée mck est le nombre d’exemples de la classe c pour laquelle l’étiquette k a été prédite. dans le cas de la classiﬁcation binaire, la matrice de confusion prend la forme suivante : classe réelle classe prédite 0 1 0 vrais négatifs (tn) faux positifs (fp) 1 faux négatifs (fn) vrais positifs (tp) on appelle vrais positifs (en anglais true positives) les exemples positifs correctement classiﬁés ; faux posi- tifs (en anglais false positives) les exemples négatifs étiquetés positifs par le modèle ; et réciproquement pour les vrais négatifs (true negatives) et les faux négatifs (false negatives). on note généralement par tp le nombre de vrais positifs, fp le nombre de faux positifs, tn le nombre de vrais négatifs et fn le nombre de faux négatifs. les faux positifs sont aussi appelés fausses alarmes ou erreurs de type i, par opposition aux erreurs de type (cid:4) ii qui sont les faux négatifs. il est possible de dériver de nombreux critères d’évaluation à partir de la matrice de confusion. en voici quelques exemples : déﬁnition 3.7 (rappel) on appelle rappel (recall en anglais), ou sensibilité (sensitivity en anglais), le taux de vrais positifs, c’est-à-dire la proportion d’exemples positifs correctement identiﬁés comme tels : rappel = tp tp + fn . (cid:4) il est cependant très facile d’avoir un bon rappel en prédisant que tous les exemples sont positifs. ainsi, ce critère ne peut pas être utilisé seul. on lui adjoint ainsi souvent la précision : déﬁnition 3.8 (précision) on appelle précision, ou valeur positive prédictive (positive predictive value, ppv) la proportion de prédictions correctes parmi les prédictions positives : précision = tp tp + fp . (cid:4) \f3.2. critères de performance 41 de même que l’on peut facilement avoir un très bon rappel au détriment de la précision, il est aisé d’obtenir une bonne précision (au détriment du rappel) en faisant très peu de prédictions positives (ce qui réduit le risque qu’elles soient erronées) l’anglais distingue precision (la précision ci-dessus) et accuracy, qui est la proportion d’exemples cor- rectement étiquetés, soit le complémentaire à 1 du taux d’erreur, aussi traduit par précision en français. on utilisera donc ces termes avec précaution. attention pour résumer rappel et précision en un seul nombre, on calculera la f-mesure : déﬁnition 3.9 (f-mesure) on appelle f-mesure (f-score ou f1-score en anglais) la moyenne harmonique de la précision et du rappel : f = 2 précision . rappel précision + rappel = 2tp 2tp + fp + fn . (cid:4) déﬁnition 3.10 (spéciﬁcité) on appelle spéciﬁcité le taux de vrais négatifs, autrement dit la proportion d’exemples négatifs correctement identiﬁés comme tels. spéciﬁcité = tn fp + tn . exemple (cid:4) prenons l’exemple d’un test clinique pour illustrer ces diﬀérents critères. il ne s’agit pas ici d’un modèle d’apprentissage automatique, mais d’un frottis de dépistage du cancer du col de l’utérus : il s’agit d’un examen beaucoup plus simple et moins invasif qu’un examen histologique, qui doit être interprété par un expert, et servira de vérité terrain. les résultats d’une expérience menée sur 4 000 femmes âgées de 40 ans et plus sont présentés sur le tableau 3.1. frottis + frottis - total cancer 190 10 200 pas de cancer total 400 210 3600 3590 4000 3800 table 3.1 – matrice de confusion pour une expérience sur le dépistage du cancer du col de l’utérus par frottis. le rappel est de 95%, la spéciﬁcité de 94.5%, mais la précision ne vaut que 47.5%. en eﬀet, ce test est un bon outil de dépistage : la probabilité de n’avoir eﬀectivement pas de cancer quand le frottis est négatif est élevée (3590/3600 ≈ 99.7%). cependant, c’est un mauvais outil diagnostique, au sens où la probabilité de fausse alarme est très élevée. \f42 chapitre 3. sélection de modèle et évaluation 3.2.2 évaluation de méthodes de classiﬁcation binaire retournant un score de nombreux algorithmes de classiﬁcation ne retournent pas directement une étiquette de classe, mais utilisent une fonction de décision qui doit ensuite être seuillée pour devenir une étiquette. cette fonction de décision peut être un score arbitraire (par exemple, la proportion d’exemples positifs parmi les k plus proches voisins du point à étiqueter – nous verrons l’algorithme des k plus proches voisins en détails au chapitre 8) ou la probabilité d’appartenir à la classe positive (comme c’est par exemple le cas pour la ré- gression logistique présentée à la section 5.3). plusieurs critères permettent d’évaluer la qualité de la fonction de décision avant seuillage. courbe roc déﬁnition 3.11 (courbe roc) on appelle courbe roc, de l’anglais receiver-operator characteristic la courbe décrivant l’évolution de la sensibilité en fonction du complémentaire à 1 de la spéciﬁcité, parfois appelé antispéciﬁcité, lorsque le seuil de décision change. le terme vient des télécommunications, où ces courbes servent à étudier si un système arrive à séparer le signal du bruit de fond. on peut synthétiser une courbe roc par l’aire sous cette courbe, souvent abrégée auroc pour area (cid:4) under the roc. un exemple de courbe roc est présenté sur la ﬁgure 3.2. le point (0, 0) apparaît quand on utilise comme seuil un nombre supérieur à la plus grande valeur retournée par la fonction de décision : ainsi, tous les exemples sont étiquetés négatifs. à l’inverse, le point (1, 1) apparaît quand on utilise pour seuil une valeur inférieure au plus petit score retourné par la fonction de décision : tous les exemples sont alors étiquetés positifs. figure 3.2 – les courbes roc de deux modèles. pour construire la courbe roc, on prend pour seuil les valeurs successives de la fonction de décision sur notre jeu de données. ainsi, à chaque nouvelle valeur de seuil, une observation que l’on prédisait précédem- ment négative change d’étiquette. si cette observation est eﬀectivement positive, la sensibilité augmente \f3.2. critères de performance 43 de 1/np (où np est le nombre d’exemples positifs) ; sinon, c’est l’antispéciﬁcité qui augmente de 1/nn, où nn est le nombre d’exemples négatifs. la courbe roc est donc une courbe en escaliers. un classiﬁeur idéal, qui ne commet aucune erreur, associe systématique des scores plus faibles aux exemples négatifs qu’aux exemples positifs. sa courbe roc suit donc le coin supérieur gauche du carré [0, 1]2 ; il a une aire sous la courbe de 1. la courbe roc d’un classiﬁeur aléatoire, qui fera sensiblement la même proportion d’erreurs que de classiﬁcations correctes quel que soit le seuil utilisé, suit la diagonale de ce carré. l’aire sous la courbe roc d’un classiﬁeur aléatoire vaut donc 0.5. pour illustrer la construction d’une courbe roc, prenons l’exemple décrit sur le tableau 3.2. exemple étiquette score + 0.9 - 0.8 + 0.6 + 0.4 - 0.3 - 0.1 table 3.2 – exemple de résultats d’une expérience de classiﬁcation binaire, évaluée sur 6 échantillons. pour un seuil supérieur à 0.9, les 6 exemples sont étiquetés négatifs. on commence donc par le point (0, 0). pour un seuil entre 0.95 et 0.9, seule la première observation est étiquetée positive. la sensibilité est donc de 1 tandis que l’antispéciﬁcité reste nulle. on peut continuer ainsi jusqu’à utiliser un seuil inférieur 3 à 0.1 : seuil tp/p fp/p > 0.9 0 0 0.8–0.9 1/3 0 0.6–0.8 1/3 1/3 0.4–0.6 2/3 1/3 0.3–0.4 1 1/3 0.1–0.3 1 2/3 < 0.1 1 1 la courbe roc correspondante est visible sur la ﬁgure 3.3. figure 3.3 – courbe roc correspondant à l’expérience du tableau 3.2. on peut enﬁn utiliser la courbe roc pour choisir un seuil de décision, à partir de la sensibilité (ou de la spéciﬁcité) que l’on souhaite garantir. \f44 courbe précision-rappel chapitre 3. sélection de modèle et évaluation la courbe précision-rappel vient souvent complémenter la courbe roc. déﬁnition 3.12 (courbe précisioj-rappel) on appelle courbe précision-rappel, ou precision-recall curve en anglais, la courbe décrivant l’évolution de la précision en fonction du rappel, lorsque le seuil de décision change. pour synthétiser cette courbe, on peut utiliser l’aire sous celle-ci, souvent abrégée aupr pour area under (cid:4) the precision-recall curve. un exemple de courbe précision-rappel, pour les mêmes données que la ﬁgure 3.2, est présenté sur la ﬁgure 3.4. figure 3.4 – les courbes précision-rappel de deux modèles. pour le seuil le plus élevé, aucun exemple n’est étiqueté positif, et la précision n’est donc pas déﬁnie. par convention, on utilise généralement une précision de 1 si la première observation à considérer est positive, et une précision de 0 sinon. remarque reprenons l’exemple précédent pour construire une courbe précision-rappel. les valeurs de la préci- sion et du rappel sont les suivantes : exemple > 0.9 seuil rappel 0 précision – 0.8–0.9 1/3 1 0.6–0.8 1/3 1/2 0.4–0.6 2/3 2/3 0.3–0.4 1 3/4 0.1–0.3 1 3/5 < 0.1 1 3/6 on obtient donc la courbe précision-rappel visible sur la ﬁgure 3.5. \f3.2. critères de performance 45 figure 3.5 – courbe précision-rappel correspondant à l’expérience du tableau 3.2. 3.2.3 erreurs de régression dans le cas d’un problème de régression, le nombre d’erreurs n’est pas un critère approprié pour évaluer la performance. d’une part, à cause des imprécisions numériques, il est délicat de dire d’une prédiction à valeur réelle si elle est correcte ou non. d’autre part, un modèle dont 50% des prédictions sont correctes à 0.1% près et les 50 autres pourcent sont très éloignées des vraies valeurs vaut-il mieux qu’un modèle qui n’est correct qu’à 1% près, mais pour 100% des exemples ? ainsi, on préférera quantiﬁer la performance d’un modèle de régression en fonction de l’écart entre les prédictions et les valeurs réelles. un premier critère est donc l’erreur quadratique moyenne : déﬁnition 3.13 (erreur quadratique moyenne (mse)) étant données n étiquettes réelles y1, y2, . . . , yn et n prédictions f ((cid:126)x 1), f ((cid:126)x 2), . . . , f ((cid:126)x n), on appelle erreur quadratique moyenne, ou mse de l’anglais mean squared error la valeur mse = 1 n n (cid:88) i=1 (cid:0)f ((cid:126)x i) − yi(cid:1)2 . (cid:4) pour mesurer l’erreur dans la même unité que la cible, on lui préfère souvent sa racine : déﬁnition 3.14 (rmse) étant données n étiquettes réelles y1, y2, . . . , yn et n prédictions f ((cid:126)x 1), f ((cid:126)x 2), . . . , f ((cid:126)x n), on appelle racine de l’erreur quadratique moyenne, ou rmse de l’anglais root mean squared error la valeur rmse = (cid:118) (cid:117) (cid:117) (cid:116) 1 n n (cid:88) i=1 (f ((cid:126)x i) − yi)2. (cid:4) \f46 chapitre 3. sélection de modèle et évaluation dans le cas où les valeurs cibles couvrent plusieurs ordres de grandeur, on préfère parfois passer au log avant de comparer f ((cid:126)x i) à yi, aﬁn de ne pas donner plus d’importance aux erreurs faites pour des valeurs plus élevées. déﬁnition 3.15 (rmsle) étant données n étiquettes réelles y1, y2, . . . , yn et n prédictions f ((cid:126)x 1), f ((cid:126)x 2), . . . , f ((cid:126)x n), on appelle racine du log de l’erreur quadratique moyenne, ou rmsle de l’anglais root mean squared log error la va- leur rmsle = (cid:118) (cid:117) (cid:117) (cid:116) 1 n n (cid:88) i=1 (log (f ((cid:126)x i) + 1) − log (yi + 1))2. (cid:4) l’interprétation de ces erreurs requiert néanmoins de connaître la distribution des valeurs cibles : une rmse de 1 cm n’aura pas la même signiﬁcation selon qu’on essaie de prédire la taille d’humains ou celle de drosophiles. pour répondre à cela, il est possible de normaliser la somme des carrés des résidus non pas en en faisant la moyenne, mais en la comparant à la somme des distances des valeurs cibles à leur moyenne. déﬁnition 3.16 (coefﬁcient de détermination) étant données n étiquettes réelles y1, y2, . . . , yn et n prédictions f ((cid:126)x 1), f ((cid:126)x 2), . . . , f ((cid:126)x n), on appelle erreur carrée relative, ou rse de l’anglais relative squared error la valeur rse = (cid:80)n (cid:0)f ((cid:126)x i) − yi(cid:1)2 (cid:80)n i=1 (cid:0)yi − 1 n l=1 yl(cid:1)2 . (cid:80)n i=1 le complémentaire à 1 de la rse est le coeﬃcient de détermination, noté r2. (cid:4) on note le coeﬃcient de détermination r2 car il s’agit du carré du coeﬃcient de corrélation entre (cid:126)y et (f ((cid:126)x 1), f ((cid:126)x 2), . . . , f ((cid:126)x n)) donné par r = (cid:113) (cid:80)n i=1 (cid:80)n (cid:0)yi − 1 n (cid:80)n i=1 (cid:0)yi − 1 n (cid:80)n l=1 yl(cid:1) (cid:0)f ((cid:126)x i) − 1 (cid:80)n n (cid:0)f ((cid:126)x i) − 1 n l=1 yl(cid:1)2(cid:113) (cid:80)n i=1 l=1 f ((cid:126)x l)(cid:1) (cid:80)n l=1 f ((cid:126)x l)(cid:1)2. (3.2) ce coeﬃcient indique à quel point les valeurs prédites sont corrélées aux valeurs réelles ; attention, il sera élevé aussi si elles leur sont anti-corrélées. 3.2.4 comparaison à des algorithmes naïfs pour construire un modèle de machine learning, nous nous appuyons d’une part sur les données, et d’autre part sur des hypothèses quant à la forme de ce modèle ; ces hypothèses déterminent l’espace des hypothèses. la validité de ces hypothèses dépend du problème étudié. ce problème peut être plus ou moins facile, et la performance d’un modèle que nous avons entraîné ne peut être interprétée qu’à la lueur de cette diﬃculté. pour la déterminer, il peut être très utile d’utiliser des approches d’apprentissage naïves, autrement dit très simples, qui utilisent certaines propriétés du jeu d’entraînement mais pas de l’observation à étiqueter. nous n’attendons pas de bonnes performances de ces méthodes mais elles servent d’étalon pour mieux comprendre les performances mesurées par ailleurs, en nous indiquant le « strict minimum » que l’on peut attendre de nos modèles. \f3.2. critères de performance méthodes naïves pour la classiﬁcation 47 pour un problème de classiﬁcation, on peut par exemple considérer une des approches suivantes : — prédire systématiquement l’étiquette majoritaire dans le jeu d’entraînement. — prédire une étiquette aléatoire, selon la distribution des étiquettes dans le jeu d’entraînement. — dans le cas d’une classiﬁcation binaire, prédire des scores de manière uniforme avant de les seuiller. cette méthode est plus recommandée si l’on cherche à tracer des courbes roc ou pr. si le jeu d’entraînement est déséquilibré, à savoir qu’une classe y est largement plus présente que les autres, le premier algorithme naïf que nous décrivons peut avoir un taux d’erreur très faible. il faudra aussi prendre en compte la spéciﬁcité de l’algorithme. remarque méthodes naïves pour la régression pour un problème de régression, on peut considérer les approches naïves suivantes : — prédire une valeur aléatoire, uniformément entre la plus petite et la plus grande des valeurs des étiquettes du jeu d’entraînement ; — prédire systématiquement la moyenne ou la médiane des étiquettes du jeu d’entraînement. points clefs — pour éviter le sur-apprentissage, il est essentiel lors de l’étape de sélection du modèle de valider les diﬀérents modèles testés sur un jeu de données diﬀérent de celui utilisé pour l’entraînement. — pour estimer la performance en généralisation d’un modèle, il est essentiel de l’évaluer sur des don- nées qui n’ont été utilisées ni pour l’entraînement, ni pour la sélection de ce modèle. — de nombreux critères permettent d’évaluer la performance prédictive d’un modèle. on les choisira en fonction de l’application. — pour interpréter la performance d’un modèle, il peut être utile de le comparer à une approche naïve. pour aller plus loin • en construisant la courbe précision-rappel de la ﬁgure 3.5, nous avons relié les points par des seg- ments. cette interpolation linéaire n’est en fait pas appropriée. on pourra se référer aux articles de davis et goadrich (2006) et fawcett (2006). • pour certains modèles, en particulier linéaires (voir chapitre 5), il est possible d’estimer l’optimisme, c’est-à-dire la diﬀérence entre l’erreur d’entraînement et l’erreur de test, de manière théorique, plutôt que d’utiliser une validation croisée. dans le cas des modèles linéaires, on pourra notamment utiliser le coeﬃcient cp de mallow, le critère d’information d’akaike ou le critère d’information bayésien. pour plus de détails, on se reportera au livre de dodge et rousson (2004). • la longueur de description minimale (ou minimum description length, mdl) est un concept issu de la théo- rie de l’information qui peut être utilisé pour la sélection de modèle (rissanen, 1978). dans ce cadre, les étiquettes d’un jeu de données d peuvent être représentées par, d’une part une représentation d’un modèle, et d’autre part une représentation de la diﬀérence entre les prédictions du modèle et \f48 chapitre 3. sélection de modèle et évaluation leurs vraies étiquettes dans d. le meilleur modèle est celui pour lequel la somme des tailles de ces représentations est minimale : un bon modèle permet de compresser eﬃcacement les données. • pour une discussion détaillée des procédures de validation croisée, on pourra consulter arlot et celisse (2010). bibliographie arlot, s. et celisse, a. (2010). a survey of cross-validation procedures for model selection. statistics surveys, 4 :40–79. davis, j. et goadrich, m. (2006). the relationship between precision-recall and roc curves. in proceedings of the 23rd international conference on machine learning, icml ’06, pages 233–240, new york, ny, usa. acm. dodge, y. et rousson, v. (2004). analyse de régression appliquée. dunod. fawcett, t. (2006). an introduction to roc analysis. pattern recognition letters, 27 :861–874. rissanen, j. (1978). modeling by shortest data description. automatica, 14(5) :465–471. wolpert, d. h. et macready, w. g. (1997). no free lunch theorems for optimization. evolutionary computation, 1(1) :67–82. ieee transactions on \fchapitre 4 inférence bayésienne un des problèmes fondamentaux auxquels nous faisons face dans le cadre du machine learning est celui de l’incertitude : notre compréhension du monde est limitée par nos observations nécessairement partielles. les modèles probabilistes nous permettent de prendre en compte cette incertitude de manière explicite. dans ce chapitre, nous allons voir comment formuler l’apprentissage d’un modèle comme un problème d’inférence sur une distribution jointe des observations et des paramètres de ce modèle. nous verrons aussi comment utiliser le cadre bayésien pour prendre des décisions, ce qui requiert de modéliser additionnel- lement à quel point il est utile de prendre la bonne décision. à titre illustratif, nous présenterons la classi- ﬁcation bayésienne naïve. objectifs — formaliser le concept de classe grâce à des modèles probabilistes ; — déﬁnir des règles de décision, sur la base de tests de rapport de vraisemblance ; — estimer une densité par maximum de vraisemblance ou par estimateur de bayes ; — mettre en œuvre un algorithme de classiﬁcation naïve bayésienne. 4.1 modèles génératifs pour la classiﬁcation binaire l’approche statistique de la classiﬁcation formalise le concept de classe grâce à des modèles probabi- listes. dans ce cadre, nous allons supposer que les n observations (cid:126)x 1, (cid:126)x 2, . . . , (cid:126)x n sont la réalisation d’une variable aléatoire x ∈ x . de plus, nous supposons que leurs étiquettes y1, y2, . . . , yn sont la réalisation d’une variable aléatoire y ∈ {0, 1}. dans le cas de la classiﬁcaton multi-classe, nous utiliserons une valeur aléatoire y ∈ {1, 2, . . . , c} (où c est le nombre total de classes). considérons maintenant une loi de probabilité jointe p(x, y ) pour l’ensemble des variables entrant dans le modèle, autrement dit x et y . cette approche est qualiﬁée de modélisation générative : elle répond à la question « comment les données que l’on observe auraient-elles pu être générées ? ». bien sûr, en pra- tique, nos données ne sont pas toujours le fruit d’un processus aléatoire ; cependant, considérer qu’elles 49 \f50 chapitre 4. inférence bayésienne sont la réalisation d’une variable aléatoire peut être une façon eﬃcace de représenter l’information com- plexe qu’elles contiennent. ainsi, si l’une de nos variables est le résultat d’un test de dépistage médical, il est plus simple de la représenter comme la réalisation d’une loi de bernoulli que de modéliser toutes les informations biochimiques qui ont pu aboutir à ce résultat. c’est dans ce cadre que nous allons maintenant nous placer. 4.1.1 inférence et prédiction la probabilité qu’une observation (cid:126)x appartienne à la classe c est ainsi déterminée par p(y = c|x = (cid:126)x). deux problèmes se posent alors : — un problème d’inférence, qui consiste à déterminer les lois de probabilité p(y = c|x = (cid:126)x) à partir de nos observations et hypothèses ; — un problème de prédiction, ou de décision, qui consiste à utiliser ces lois pour déterminer la classe d’une observation (cid:126)x. pour prédire la classe d’une observation (cid:126)x, il semble raisonnable de déterminer la classe la plus probable étant donnée cette observation. il s’agit alors de comparer p(y = 1|x = (cid:126)x) et p(y = 0|x = (cid:126)x). formellement, nous considérons alors la règle de décision suivante : ˆy = (cid:40) 1 0 si p(y = 1|x = (cid:126)x) > p(y = 0|x = (cid:126)x) sinon. (4.1) pour s’étendre au cas multi-classe, cette règle peut se réécrire comme ˆy = arg max c=1,...,c p(y = c|x = (cid:126)x). nous discuterons plus en détails de cette règle de décision et d’autres dans la section 4.2. à partir de maintenant, nous écrirons parfois p((cid:126)x) au lieu de p(x = (cid:126)x) quand il n’y a pas d’ambiguité. 4.1.2 loi de bayes le raisonnement probabiliste s’appuie fortement sur la loi de bayes, qui nous permet d’exprimer la distribution conditionnelle p(y = c|(cid:126)x) comme suit : théorème 4.1 (loi de bayes) p(y = c|(cid:126)x) = p(y = c)p((cid:126)x|y = c) p((cid:126)x) . (4.2) (cid:4) chacun des éléments de cette équation joue un rôle bien précis et porte ainsi un nom : — p(y = c) est la distribution a priori des étiquettes, avant d’avoir observé les données ; — p((cid:126)x|y = c) est la vraisemblance. elle quantiﬁe à quel point il est vraisemblable que l’on observe la réalisation (cid:126)x de x sachant que la classe est c ; — p(y = c|(cid:126)x) est la distribution a posteriori des étiquettes, après avoir observé les données. le dénominateur p((cid:126)x) est la probabilité marginale que (cid:126)x soit observée, indépendamment de sa classe. il peut être réécrit sous la forme p((cid:126)x) = p((cid:126)x|y = 0)p (y = 0) + p((cid:126)x|y = 1)p (y = 1). dans le cas multi-classe, on écrira p((cid:126)x) = (cid:80)c p((cid:126)x|y = c)p (y = c). c=1 \f4.2. règles de décision 51 exemple prenons le cas du dépistage du cancer du col de l’utérus par frottis sanguin. ce test, bien que simple à réaliser, est assez imprécis : il a une sensibilité (la proportion de personnes atteintes pour lequel il est positif) d’environ 70% 1, et une spéciﬁcité (la proportion de personnes non atteintes pour lequel il est négatif) d’environ 98%. (pour plus de détails sur la sensibilité et la spéciﬁcité, voir la section 3.2.1.) de plus, l’incidence de la maladie est d’environ 1 femme sur 10 000 2. quelle est la probabilité qu’une personne testée soit atteinte d’un cancer si le test est positif ? c’est un problème d’inférence bayésienne. soient x une variable aléatoire à valeurs dans {0, 1} modélisant le résultat du test (1 pour positif, 0 pour négatif), et y une variable aléatoire à valeurs dans {0, 1} modélisant le statut de la personne testée (1 pour malade, 0 pour non atteinte). inférence : nous cherchons à calculer p(y = 1|x = 1). appliquons la loi de bayes : p(y = 1|x = 1) = p(x = 1|y = 1)p(y = 1) p(x = 1) . p(x = 1|y = 1) n’est autre que la sensibilité du test, autrement dit, p(x = 1|y = 1) = 0, 70. p(y = 1) est l’incidence de la maladie, soit 10−4. enﬁn, p(x = 1) = p(x = 1|y = 1)p(y = 1) + p(x = 1|y = 0)p(y = 0). nous connaissons déjà les deux premiers termes de cette équation. de plus, p(x = 1|y = 0) = 1 − 0, 90 = 0, 02, et p(y = 0) = 1 − p(y = 1) = 0, 9999. ainsi, p(y = 1|x = 1) = 0, 70 × 10−4 0, 70 × 10−4 + 0, 02 × 0, 9999 = 0, 0035. prédiction : ainsi, la probabilité qu’une personne dont le test est positif soit atteinte est seulement de 0, 35%. sans autre information, p(y = 0|x = 1) > p(y = 0|x = 1) et la règle de décision 4.1 retournera une prédiction négative pour tous les tests positifs. 4.1.3 modélisation paramétrique dans l’exemple précédent, p(x|y ) était donnée. cependant, il se peut que nous devions aussi modéliser cette distribution. dans ce cas, nous allons la contraindre à appartenir à une famille bien précise de lois de probabilité, paramétrisée par un vecteur (cid:126)θ à valeurs dans un espace θ de dimension ﬁnie : c’est ce que l’on appelle la modélisation paramétrique. en plus de l’inférence et de la prédiction, nous avons maintenant une tâche supplémentaire : celle de l’apprentissage, qui consiste à estimer la valeur du vecteur de paramètres (cid:126)θ. nous verrons comment faire dans la section 4.3. 4.2 règles de décision dans cette section, nous allons voir comment prendre une décision, ou faire une prédiction, à partir des lois de probabilité nécessaires. 1. c’est pourquoi il est recommandé de le faire régulièrement. 2. 6,7 sur 100 000 en 2012 \f52 chapitre 4. inférence bayésienne 4.2.1 tests du rapport de vraisemblance la règle de décision (équation 4.1) qui consiste à prédire la classe la plus probable étant donnée l’ob- servation consiste à sélectionner la classe ˆy qui maximise la valeur a posteriori p(y = ˆy|(cid:126)x). on parle donc de décision par maximum a posteriori. en appliquant la loi de bayes, elle peut être reformulée comme suit : ˆy = (cid:40) 1 0 si p((cid:126)x|y =1)p(y =1) p((cid:126)x) sinon. > p((cid:126)x|y =0)p(y =0) p((cid:126)x) p((cid:126)x) n’aﬀecte pas cette règle de décision et peut donc être éliminée. remarquons néanmoins que p((cid:126)x) sera nécessaire si nous voulons estimer la valeur a posteriori p(y = ˆy|(cid:126)x) pour quantiﬁer la qualité de notre décision. déﬁnition 4.1 (décision par maximum a posteriori) dans le cas binaire, la règle de décision ˆy = (cid:40) 1 0 si p((cid:126)x|y = 1)p(y = 1) > p((cid:126)x|y = 0)p(y = 0) sinon. est appelée règle de décision par maximum a posteriori. dans le cas multi-classe, cette règle s’écrit ˆy = arg max c=1,...,c p((cid:126)x|y = c)p(y = c). on peut réécrire cette règle en utilisant le rapport des vraisemblances. déﬁnition 4.2 (rapport de vraisemblance) on représente par λ((cid:126)x) le rapport de vraisemblance λ((cid:126)x) = p((cid:126)x|y = 1) p((cid:126)x|y = 0) , (4.3) (cid:4) (cid:4) la règle de décision par maximum a posteriori peut être reformulée comme ce que l’on appelle un test du rapport de vraisemblance : ˆy = (cid:40) 1 0 si λ((cid:126)x) > sinon. p(y =0) p(y =1) (4.4) dans le cas où l’on fait l’hypothèse d’égalité des distributions a priori, p(y = 0) = p(y = 1) et l’on comparera le rapport de vraisemblance à 1. déﬁnition 4.3 (décision par maximum de vraisemblance) la règle de décision ou, de manière équivalente, ˆy = (cid:40) 1 0 si p((cid:126)x|y = 1) > p((cid:126)x|y = 0) sinon. ˆy = (cid:40) 1 0 si λ((cid:126)x) > 1 sinon. est appelée règle de décision par maximum de vraisemblance. (cid:4) \f4.2. règles de décision 53 dans de nombreux cas, on préfèrera passer au log et évaluer le signe de log λ((cid:126)x). remarque exemple supposons que nous disposions d’un échantillon d’une population de poissons d’une même espèce, parmi lesquels se trouvent des mâles et des femelles. nous cherchons à déterminer leur sexe, modélisé comme une variable aléatoire binaire y (0 pour les mâles, 1 pour les femelles), uniquement à partir de leur longueur, modélisée comme une variable aléatoire continue x. nous allons supposer la distribution des longueurs des mâles normalement distribuée, centrée en 4 cm et d’écart-type 1 cm, et celle des longueurs des femelles normalement distribuée, centrée en 6 cm, et d’écart-type 1 cm. p(x|y = 0) ∼ n (4, 1) et p(x|y = 1) ∼ n (6, 1). le rapport de vraisemblance s’écrit p(x|y = 1) p(x|y = 0) = e−(x−6)2/2 e(x−4)2/2 et son logarithme vaut donc ln λ(x) = −(x − 6)2 + (x − 4)2 = 4(x − 5). ainsi, si l’on suppose les proportions de mâles et de femelles identiques dans notre échantillon, la règle de décision par maximum a posteriori est équivalente à la règle de décision par maximum de vraisemblance et consiste à prédire qu’un poisson est une femelle si sa longueur est supérieure à 5 cm et un mâle sinon. cette règle de décision est illustrée sur la ﬁgure 4.1a. supposons maintenant que l’on sache avoir cinq fois plus de femelles que de mâles dans notre échan- tillon. le rapport des distributions a priori vaut donc p(y =0) 5 . nous comparons donc le logarithme du rapport de vraisemblance à ln 1 . nous allons prédire qu’un poisson est une femelle si sa longueur est 5 supérieure à 5 − ln(5)/4 ≈ 4.58 : il est plus probable que précédemment qu’un poisson d’une taille légè- rement inférieure à 5 cm soit femelle. on observe un déplacement de la valeur seuil en accord avec notre connaissance a priori du problème. cette règle de décision est illustrée sur la ﬁgure 4.1b. p(y =1) = 1 4.2.2 théorie de la décision bayésienne les règles de décision que nous venons de voir s’inscrivent dans le cadre plus général de la théorie de la décision. dans ce cadre, la variable aléatoire y déﬁnie sur y représente non pas une étiquette, mais une « vérité » cachée, ou un état de la nature. x est une variable aléatoire déﬁnie sur x qui représente les données observées. soit de plus une variable a, déﬁnie sur un espace, a qui représente l’ensemble des décisions qui peuvent être prises. nous nous donnons maintenant une fonction de coût (loss function en an- glais), l : y × a → r. cette fonction de coût est à rapprocher de celles que nous avons déﬁnies dans la section 2.4. étant donné une action a et un état caché véritable y, cette fonction quantiﬁe le prix à payer pour avoir choisi l’action a alors que l’état caché véritable était y. \f54 chapitre 4. inférence bayésienne (a) si les deux classes sont également probables, les pois- sons dont la taille est inférieure à 5 cm sont étiquetés mâles et les autres femelles. (b) si un poisson a cinq fois plus de chances a priori d’être femelle, les poissons dont la taille est inférieure à 4,58 cm sont étiquetés mâles et les autres femelles. figure 4.1 – règle de décision pour le sexe d’un guppy en fonction de sa taille. exemple dois-je prendre ou non mon parapluie ce matin ? nous pouvons modéliser ce problème de la façon sui- vante : a contient deux actions (« prendre mon parapluie » et « ne pas prendre mon parapluie »). y contient les vérités « il ne pleut pas », « il pleut un peu », « il pleut fort », « il y a beaucoup de vent ». enﬁn, x est un espace décrivant les informations sur lesquelles je peux m’appuyer, comme les prévisions météorologiques et la couleur du ciel quand je pars de chez moi. je peux choisir la fonction de coût suivante : parapluie pas de parapluie pas de pluie 1 0 pluie faible 0 2 pluie forte 0 4 vent 2 0 dans ce cadre, il est raisonnable de choisir l’action a qui minimise la probabilité d’erreur, autrement dit l’espérance de la fonction de coût : déﬁnition 4.4 (décision de bayes) la règle de décision qui consiste à choisir l’action a∗ qui minimise l’espérance de la fonction de coût est appellée règle de décision de bayes : a∗((cid:126)x) = arg min e[l(y, a)] = arg min a∈a a∈a (cid:88) y∈y p(y = y|(cid:126)x)l(y, a). (4.5) on parlera aussi du principe de minimisation de la perte espérée (minimum expected loss en anglais.) (cid:4) en économie, on préfère au concept de fonction de coût celui d’utilité. l’utilité peut être simplement dé- ﬁnie comme l’opposé d’une fonction de coût. le principe ci-dessus s’appelle alors la maximisation de l’utilité espérée (maximum expected utility en anglais.) remarque \f4.2. règles de décision 55 cette approche est à contraster avec la minimisation du risque empirique 2.3, dans laquelle on rem- place la distribution p(x, y ) par sa distribution empirique obtenue en partageant également la masse de probabilité entre les n observations p(x = (cid:126)x, y = y|d) = 1 n n (cid:88) i=1 δ(y, yi)δ((cid:126)x, (cid:126)x i). (4.6) par contraste, dans le cadre bayésien, on paramétrise la distribution p(x, y ) par un paramètre (cid:126)θ optimisé sur d. dans le cadre empirique, les hypothèses sur la distribution des données sont potentiellement sim- plistes ; mais dans le cadre bayésien, cette distribution est apprise sans considérer le processus de décision dans lequel elle sera utilisée. alors que la décision de bayes consiste à choisir, pour une observation donnée, l’espérance de la fonc- tion de coût, on déﬁnit le risque de bayes comme l’espérance globale de la fonction de coût : déﬁnition 4.5 (risque de bayes) le risque de bayes est l’espérance du coût sous la règle de décision de bayes. (cid:90) r = (cid:88) l(y, a∗((cid:126)x))p((cid:126)x, y) d(cid:126)x. (cid:126)x∈x y∈y (4.7) (cid:4) déﬁnir une stratégie qui minimise le risque de bayes est équivalent à appliquer la règle de décision de bayes. classiﬁcation binaire par la règle de décision de bayes revenons au cadre de la classiﬁcation binaire. y ∈ y représente de nouveau la véritable classe d’une observation, tandis que a ∈ a représente sa classe prédite : a = y = {0, 1}. la fonction de coût l : y × y → r c, k (cid:55)→ λck quantiﬁe maintenant le coût de prédire la classe k quand la classe véritable est c (voir section 2.4). la règle de décision de bayes (équation 4.5) est équivalente à la règle de décision suivante : ˆy = (cid:40) 1 si λ11p(y = 1|(cid:126)x) + λ10p(y = 0|(cid:126)x) ≤ λ01p(y = 1|(cid:126)x) + λ00p(y = 0|(cid:126)x) 0 sinon. elle peut s’écrire sous la forme d’un test du rapport de vraisemblance : ˆy = (cid:40) 1 0 p((cid:126)x|y =0) > (λ01−λ00)p(y =0) (λ10−λ11)p(y =1) si p((cid:126)x|y =1) sinon. dans le cas multi-classe, cette règle devient : ˆy = arg min c=1,...,c k (cid:88) k=1 λkcp(y = k|(cid:126)x). (4.8) (4.9) \f56 coût 0/1 chapitre 4. inférence bayésienne on retrouve le coût 0/1 (section 2.4) en utilisant λck = 1 − δ(k, c). l’équation 4.8 devient ˆy = (cid:40) 1 0 si p(y = 0|(cid:126)x) ≤ p(y = 1|(cid:126)x) sinon (4.10) et ainsi la règle de décision de bayes est équivalente à la règle décision par maximum a posteriori. ceci est vrai aussi dans le cas multi-classe. le coût 0/1 n’est pas la seule fonction de coût possible, même pour un problème de classiﬁcation bi- naire. en particulier, toutes les erreurs de classiﬁcation ne sont pas nécessairement également coûteuses. par exemple, prédire qu’une patiente atteinte d’un cancer est en bonne santé peut être largement plus problématique que l’inverse. régions de décision les règles de décisions peuvent aussi s’exprimer en termes de régions de décision (cf. section 2.1) : la règle de décision consiste simplement à étiqueter l’observation (cid:126)x en accord avec la région de décision à laquelle elle appartient : (cid:40) 1 0 si (cid:126)x ∈ r1 sinon. (4.11) dans le cas multi-classe, cette règle revient à ˆy = ˆy = c (cid:88) c=1 δ(cid:126)x∈rc. cette règle de décision est équivalente à la règle de décision de bayes si l’on déﬁnit comme fonction discriminante la fonction : ou, dans le cas multi-classe f ((cid:126)x) = (λ01p(y = 1|(cid:126)x) + λ00p(y = 0|(cid:126)x)) − (λ11p(y = 1|(cid:126)x) + λ10p(y = 0|(cid:126)x)) , (4.12) fc((cid:126)x) = − c (cid:88) k=1 λckp(y = k|(cid:126)x). dans le cas du coût 0/1, la fonction discriminante vaut f ((cid:126)x) = p(y = 1|(cid:126)x) − p(y = 0|(cid:126)x) et la règle de décision de bayes est bien équivalente à la décision par maximum a posteriori (équation 4.4). le risque de bayes(4.7) peut s’exprimer en fonction des régions de décision : (cid:90) r = (cid:126)x∈x (cid:90) = λ0,a∗((cid:126)x)p((cid:126)x|y = 0)p(y = 0) + λ1,a∗((cid:126)x)p((cid:126)x|y = 1)p(y = 1) d(cid:126)x λ00p((cid:126)x|y = 0)p(y = 0) + λ10p((cid:126)x|y = 1)p(y = 1) d(cid:126)x (4.13) (cid:126)x∈r0 (cid:90) + (cid:126)x∈r1 λ01p((cid:126)x|y = 0)p(y = 0) + λ11p((cid:126)x|y = 1)p(y = 1) d(cid:126)x. déﬁnir les régions de décision de sorte à minimiser le risque de bayes est équivalent à utiliser la fonction discriminante déﬁnie par l’équation 4.12. cela vaut aussi pour le cas multi-classe, où c (cid:88) (cid:90) r = c (cid:88) k=1 (cid:126)x∈rk c=1 λckp((cid:126)x|y = c)p(y = c) d(cid:126)x. \f4.3. estimation de densité rejet 57 pour certaines applications, il peut être intéressant que l’algorithme refuse de prendre une décision quand sa conﬁance en sa prédiction est faible, autrement dit, quand le rapport de vraisemblance est très proche de 1. dans ce cas, on pourra rajouter une classe artiﬁcielle c + 1 de rejet. on peut adapter le coût 0/1 à ce cas en proposant λck =  si k = c 0  λ si k = c + 1  sinon, 1 avec 0 < λ < 1. la règle de décision par minimisation de la perte espérée devient alors (cid:40) c rejet ˆy = si p(y = c|(cid:126)x) ≥ p(y = k|(cid:126)x) ∀k (cid:54)= c sinon. (4.14) (4.15) dans ce cas, les régions de décision r1, . . . rc ne couvrent pas x et la réunion de leur complémentaire est la zone de rejet. 4.3 estimation de densité supposons disposer d’un échantillon d = x1, x2, . . . , xn constitué n observations d’une variable aléa- toire x, à valeurs sur x . nous allons de plus supposer que la distribution de x a une forme connue et est paramétrisée par le paramètre θ. comment estimer θ ? 4.3.1 estimation par maximum de vraisemblance déﬁnition 4.6 (estimateur par maximum de vraisemblance) l’estimateur par maximum de vraisem- blance (maximum likelihood estimator ou mle en anglais) de θ est le vecteur ˆθmle qui maximise la vraisem- blance, autrement dit la probabilité d’observer d étant donné θ : ˆθmle = arg max p(d|θ). θ (4.16) (cid:4) pour trouver ˆθmle, nous allons supposer que les n observations sont indépendantes et identiquement dis- tribuées (ou iid). cela nous permet de décomposer la vraisemblance comme p(d|θ) = n (cid:89) i=1 p(x = (cid:126)x i|θ). (4.17) enﬁn, remarquons que pour simpliﬁer les calculs, on choisira souvent de maximiser non pas directe- ment la vraisemblance mais son logarithme : ˆθmle = arg max θ n (cid:88) i=1 log p(x = (cid:126)x i|θ). (4.18) exemple prenons l’exemple d’un jeu de pile ou face. nous modélisons l’observation « pile » ou « face » comme la réalisation d’une variable aléatoire x, déﬁnie sur l’univers x = {0, 1} (0 correspondant à « pile » et 1 \f58 chapitre 4. inférence bayésienne à « face »), et suivant une loi de probabilité p. un choix classique pour cette loi de probabilité est d’utiliser une loi de bernoulli : p(x = x) = (cid:40) p (1 − p) si x = 1 si x = 0, (4.19) ou, de manière équivalente, p(x = x) = px(1 − p)1−x. supposons d = {x1, x2, . . . , xn} constitué de n observations iid. d’après l’équation 4.18, l’estimateur par maximum de vraisemblance de p est ˆpmle = arg max p∈[0,1] = arg max p∈[0,1] n (cid:88) i=1 n (cid:88) i=1 (cid:16) pxi (1 − p)1−xi(cid:17) log n (cid:88) i=1 log p(x = xi|p) = arg max p∈[0,1] (cid:33) (cid:32) xi log p + n − xi log(1 − p). n (cid:88) i=1 i=1 xi log p+(cid:0)n − (cid:80)n i=1 xi(cid:1) log(1−p) est concave, nous pouvons donc la maximiser la fonction l : p (cid:55)→ (cid:80)n en annulant sa dérivée : ce qui nous donne et donc ∂l ∂p = n (cid:88) i=1 xi 1 p (cid:32) − n − (cid:33) xi n (cid:88) i=1 1 1 − p , (1 − ˆpmle) n (cid:88) i=1 (cid:32) xi − ˆpmle n − (cid:33) xi = 0 n (cid:88) i=1 ˆpmle = 1 n n (cid:88) i=1 xi. (4.20) l’estimateur par maximum de vraisemblance de p est tout simplement la moyenne de l’échantillon. exemple reprenons l’exemple du test de dépistage du cancer du col de l’utérus. alors que dans l’exemple précé- dent, p(x|y = 0) et p(x|y = 1) étaient données, nous allons maintenant les estimer à partir de données observées. supposons que nous observions un jeu d0 de n0 personnes non atteintes, parmi lesquelles t0 ont un test négatif, et un jeu d1 de n1 personnes atteintes, parmi lesquelles t1 ont un test positif. la pré- valence de la maladie est p(y = 1) = π. nous pouvons modéliser la probabilité p(x|y = 1) par une loi de bernoulli de paramètre p1, et p(x|y = 0) par une loi de bernoulli de paramètre p0. la probabilité qu’une personne dont le test est positif soit atteinte est p(y = 1|x = 1) = p(x = 1|y = 1)p(y = 1) p(x = 1) . nous savons que p(x = x|y = 0) = px 0(1 − p0)1−x et p(x = x|y = 1) = px 1(1 − p1)1−x. ainsi, p(y = 1|x = 1) = p1π p1π + p0(1 − π) . nous pouvons remplacer dans cette équation p0 et p1 par leurs estimateurs par maximum de vraisemblance (équation 4.20) : ˆp0 = 1 − t0 n0 et ˆp1 = t1 n1 . (4.21) il s’agit respectivement de la spéciﬁcité et de la sensibilité estimées du test. en utilisant t0 n0 0, 70 et π = 10−5, on retrouve les résultats précédents. = 0, 98, t1 n1 = \f(4.22) (cid:4) (4.23) 4.3. estimation de densité 4.3.2 estimateur de bayes 59 supposons que plutôt que de ne pas connaître du tout la valeur du paramètre θ, nous ayons, en tant qu’expert du domaine d’application, une bonne idée des valeurs qu’il peut prendre. cette information peut être très utile, surtout quand le nombre d’observations est faible. pour l’utiliser, nous allons modéliser θ à son tour comme une variable aléatoire, et déﬁnir sa distribution a priori p(θ). déﬁnition 4.7 (estimateur de bayes) étant donnée une fonction de coût l, l’estimateur de bayes ˆθbayes de θ est déﬁni par ˆθbayes = arg min e[l(θ, ˆθ)]. ˆθ si l’on utilise pour l l’erreur quadratique moyenne, alors ˆθbayes = arg min e[(θ − ˆθ)2] ˆθ en considérant ˆθ déterministe, nous avons ˆθbayes = arg min ˆθ2 − 2ˆθe[θ] + e[θ2] ˆθ = arg min ˆθ = e[θ]. (cid:16)ˆθ − e[θ] (cid:17)2 − e[θ]2 + e[θ2] cette dernière égalité s’obtient en remarquant que ni e[θ] ni e[θ2] ne dépendent de ˆθ. cette espérance est prise sur la distribution de θ et de x, qui nous sert à estimer θ ; ainsi ˆθbayes = e[θ|x] = (cid:90) θp(θ|x) dθ. (4.24) quand la distribution a priori du paramètre est uniforme, l’estimateur de bayes est équivalent à l’esti- mateur par maximum de vraisemblance. reprenons notre exemple de dépistage du cancer du col de l’utérus, et supposons maintenant que p0 et p1 suivent chacun une loi bêta de paramètres (α0, β0) et (α1, β1) respectivement. la densité de probabilité de la loi bêta de paramètres α, β > 0, déﬁnie sur 0 ≤ u ≤ 1, est donnée par : exemple fα,β(u) = uα−1(1 − u)β−1 b(α, β) (4.25) où b(α, β) = γ(α)γ(β) γ(α+β) . et γ est la fonction gamma. l’espérance de cette loi est α α+β pour calculer l’estimateur de bayes de p0, il nous faut connaître la loi p(p0|d0). la loi de bayes nous permet d’écrire p(p0|d0) = = = p(d0|p0)p(p0) p(d0) 1 p(d0)b(α0, β0) 1 p(d0)b(α0, β0) n0(cid:89) 0 (1 − p0)1−xi pxi pα0−1 0 (1 − p0)β0−1 (hypothèse iid) i=1 pn0−t0+α0−1 0 (1 − p0)t0+β0−1. \f60 chapitre 4. inférence bayésienne ainsi p0|d0 suit une distribution beta de paramètres (n0 − t0 + α0) et (t0 + β0). l’estimateur de bayes de p0 est ainsi (cid:101)p0 = e[p0|d0] = (n0 − t0 + α0) (n0 − t0 + α0) + (t0 + β0) = n0 − t0 + α0 n0 + α0 + β0 . (4.26) en utilisant l’équation 4.21, on peut réécrire l’équation 4.26 sous la forme (cid:101)p0 = n0 n0 + α0 + β0 (cid:98)p0 + α0 + β0 n0 + α0 + β0 α0 α0 + β0 . (cid:101)p0 est proche de l’estimateur par maxi- quand le nombre d’observations n0 est grand, l’estimateur de bayes mum de vraisemblance qui est l’espérance de la distribution a priori sur p0. ainsi, plus on a de données, plus on leur fait conﬁance et plus l’estimateur s’éloigne de l’espérance a priori du paramètre, dont on sera plus proche avec peu d’observa- tions. (cid:98)p0. à l’inverse, si n0 est faible, l’estimateur de bayes est proche de α0 α0+β0 le même raisonnement s’applique à p1, dont l’estimateur de bayes est (cid:101)p1 = t1 + α1 n1 + α1 + β1 = n1 n1 + α1 + β1 (cid:98)p1 + α1 + β1 n1 + α1 + β1 α1 α1 + β1 . (4.27) 4.3.3 décomposition biais-variance pour un paramètre θ, l’erreur quadratique moyenne d’un estimateur ˆθ est mse(ˆθ) = e[(ˆθ − θ)2] = e[(ˆθ − e[ˆθ] + e[ˆθ] − θ)2] = e[(ˆθ − e[ˆθ])2] + e[(e[ˆθ] − θ)2] + e[2(ˆθ − e[ˆθ])(e[ˆθ] − θ)] = var(ˆθ) + (e[ˆθ] − θ)2. cette dernière égalité est obtenue en remarquant que e[ˆθ] − θ est déterministe et que e[(e[ˆθ] − θ)] = e[e[ˆθ]] − e[θ] = 0. ainsi l’erreur quadratique moyenne d’un estimateur est la somme de sa variance et du carré de son biais. c’est pourquoi un estimateur biaisé peut, si sa variance est plus faible, avoir une erreur quadratique moyenne plus faible qu’un estimateur non biaisé. on retrouve ici la notion de compromis biais- variance de la section 2.5.3. 4.4 classiﬁcation naïve bayésienne nous allons maintenant mettre en œuvre les principes que nous venons de voir pour élaborer notre premier algorithme d’apprentissage automatique. 4.4.1 principes dans les exemples que nous avons vu jusqu’à présent, les variables aléatoires que nous avons utilisées étaient unidimensionnelles. cependant, dans la plupart des applications, les données sont multi-dimensionnelles. cela peut grandement compliquer les calculs, sauf dans le cas où nous faisons l’hypothèse naïve que les va- riables sont conditionnellement indépendantes. \f4.4. classiﬁcation naïve bayésienne 61 supposons n observations en p dimensions : {(cid:126)x 1, (cid:126)x 2, . . . , (cid:126)x n} telles que (cid:126)x i ∈ rp, et leurs étiquettes y1, y2, . . . , yn. nous allons modéliser chacune des p variables comme une variable aléatoire xj. l’hypothèse d’indépendance conditionnelle signiﬁe que p(xj = xj|y = y, xm = xm) = p(xj = xj|y = y) ∀1 ≤ j (cid:54)= m ≤ p. (4.28) cette hypothèse permet d’écrire la vraisemblance de la façon suivante : p(y = y|x1 = x1, x2 = x2, . . . , xp = xp) = (cid:81)p j=1 p(xj = xj|y = y)p(y = y) p (x1 = x1, x2 = x2, . . . , xp = xp) (4.29) déﬁnition 4.8 (classiﬁeur bayésien naïf) on appelle classiﬁeur bayésien naïf, ou naive bayes classiﬁer en anglais, un classiﬁeur construit en appliquant la règle de décision par maximum a posteriori à des obser- vations multi-dimensionnelles dont on suppose que les variables sont indépendantes conditionnellement à l’étiquette. la règle de décision prend la forme (cid:98)y = arg max c=1,...,c p(y = c) p (cid:89) j=1 p(xj = xj|y = c). (4.30) (cid:4) 4.4.2 filtrage bayésien du spam un cas classique d’application de la classiﬁcation naïve bayésienne est celui du ﬁltrage de courier élec- tronique pour séparer les spams des e-mails légitimes. nous allons supposer disposer de n e-mails et de leurs étiquettes binaires, yi = 1 s’il s’agit d’un spam et yi = 0 sinon. représentation d’un e-mail par p variables notre première tâche est de trouver une représentation p-dimensionnelle d’e-mails. pour ce faire, nous allons choisir p mots clés, tels que « riche », « célibataire », « gagner », « cliquer », dont la présence sera informative sur la probabilité du message d’être un spam. en pratique, on peut utiliser l’ensemble des mots qui apparaissent dans les e-mails de notre base de données d’entraînement, en excluant éventuellement les mots neutres tels que les articles (« le », « la », « un »). un e-mail sera ensuite représenté par p variables binaires, chacune correspondant à un des mots de notre liste, et valant 1 si le mot apparaît dans l’e-mail, 0 sinon. nous allons donc modéliser chacune de ces variables comme la réalisation d’une variable aléatoire bi- naire xj suivant une loi de bernoulli de paramètre pj. nous modélisons aussi l’étiquette comme la réalisa- tion d’une variable aléatoire binaire y . pour pouvoir utiliser un classiﬁeur naïf bayésien, nous allons supposer que les variables sont indépen- dantes conditionnellement à l’étiquette. cette hypothèse est eﬀectivement naïve : parmi les spams, certains mots sont plus probables une fois qu’on sait qu’un autre mot est présent (par exemple, « célibataire » est plus susceptible d’apparaître une fois que l’on sait que le mot « rencontre » est présent.) en pratique, cela n’empêche pas le classiﬁeur d’avoir d’assez bons résultats. \f62 maximum a posteriori chapitre 4. inférence bayésienne la règle de décision par maximum a posteriori (équation 4.30) est (cid:40) 1 0 (cid:98)y = si p(y = 1) (cid:81)p sinon. j=1 p(xj = xj|y = 1) > p(y = 0) (cid:81)p j=1 p(xj = xj|y = 0) (4.31) inférence il nous reste donc à déterminer p(y = 1), p(y = 0), et pour chaque variable xj, p(xj = xj|y = 0) et p(xj = xj|y = 1). p(y = 1) est tout simplement la fréquence des spams dans notre jeu de données d’entraînement, et p(y = 0) = 1 − p(y = 1) la fréquence des e-mails légitimes. on peut aussi utiliser des statistiques sur la question, qui donnent la proportion de spams parmi les e-mails qui circulent aux alentours de 80%. comme nous avons fait le choix de modéliser xj comme une variable aléatoire suivant une loi de ber- j (1 − pj)1−xj . on pourrait utiliser, pour estimer pj, l’estimateur par où sj est le nombre de spams contenant le j-ème noulli, p(xj = xj|y = 1) = pxj maximum de vraisemblance (équation 4.20) : mot et s le nombre de spams dans notre jeu de données. (cid:98)pj = sj s cependant, cet estimateur est peu adapté aux mots rares : si notre j-ème mot n’apparaît pas du tout dans le jeu d’entraînement, sj = s = 0. pour cette raison, on préfèrera appliquer ce que l’on appelle un lissage de laplace pour obtenir l’estimateur suivant : (cid:98)pj = sj + 1 s + 2 . (4.32) pour un mot qui n’apparaît pas dans le jeu d’entraînement, (cid:98)pj = 0, 5. 4.5 sélection de modèle bayésienne le cadre bayésien donne un nouvel éclairage sur la notion de sélection de modèle (voir chapitre 3). en eﬀet, on peut appliquer la règle de bayes à la probabilité jointe d’un modèle m et d’un jeu de données d : p(m|d) = p(d|m)p(m) p(d) . en prenant le logarithme, on peut écrire de manière équivalente log p(m|d) = log p(d|m) + log p(m) − log p(d). (4.33) (4.34) la maximisation a posteriori de l’équation 4.34 est équivalente à minimiser la somme d’un terme carac- térisant l’erreur empirique (− log p(d|m)) et d’un terme caractérisant la complexité du modèle (− log p(m)). cette formulation est similaire à celle de la régularisation (voir section 2.5.4). la régularisation peut alors être comprise comme le fait de choisir une distribution a priori pour m qui favorise les modèles moins complexes. c’est dans ce cadre que s’inscrit par exemple le critère de sélection bayésien (bic). le détailler dépasse l’ambition de cet ouvrage. \f63 points clefs — la modélisation d’un problème d’apprentissage de manière probabiliste se divise en deux tâches : — un problème d’inférence, qui consiste à déterminer la loi de probabilité p(y = c|x) ; — une tâche de prédiction, qui consiste à appliquer une règle de décision. — le raisonnement probabiliste s’appuie sur la loi de bayes — généralement, la loi de probabilité p(y = c|x) est modélisée de manière paramétrique et le pro- blème d’inférence se résout en s’appuyant d’une part sur la loi de bayes, et d’autre part sur des estimateurs tels que l’estimateur par maximum de vraisemblance ou l’estimateur de bayes pour en déterminer les paramètres. — la règle de décision de bayes, qui consiste à minimiser l’espérance de la perte, contraste avec la règle de minimisation du risque empirique. pour aller plus loin • nous avons uniquement abordé dans ce chapitre des problèmes de classiﬁcation. cependant, la stra- tégie de minimisation du risque de bayes s’applique au cas de la régression, en utilisant une fonction de perte appropriée. dans ce cadre, les sommations sur y seront remplacées par des intégrales. • l’ouvrage de ross (1987) détaille l’estimation par maximum de vraisemblance et l’estimation bayé- sienne. • le parti pris dans cet ouvrage est de ne pas aborder en plus de détails l’approche bayésienne du machine learning. les curieux pourront se pencher sur les livres de murphy (2013) et barber (2012). • l’article de hand et yu (2001) analyse en détails la classiﬁcation naïve bayésienne. bibliographie barber, d. (2012). bayesian reasoning and machine learning. cambridge university press. http://www.cs. ucl.ac.uk/staff/d.barber/brml/. hand, d. j. et yu, k. (2001). idiot’s bayes : not so stupid after all ? international statistical review / revue internationale de statistique, 69(3) :385–398. murphy, k. p. (2013). machine learning : a probabilistic perspective. mit press, cambridge, ma, 4th edition. https://www.cs.ubc.ca/~murphyk/mlbook/. ross, s. m. (1987). introduction to probability and statistics for engineers and scientists. wiley, new york. \fchapitre 5 régressions paramétriques un modèle de régression paramétrique suppose que la forme analytique de la fonction de décision est connue. dans ce contexte, ce chapitre se concentre principalement sur des problèmes de régression linéaire, c’est-à-dire ceux pour lesquels la fonction de décision est une fonction linéaire des descripteurs. les modèles linéaires ont une longue histoire dans le domaine des statistiques, depuis bien avant le développement des ordinateurs ; mais nous ne les étudions pas que pour cette raison. en eﬀet, malgré leur simplicité, ils peuvent avoir de bonnes performances, meilleures parfois que celles de modèles non linéaires plus populaires (surtout dans le cas où la taille du jeu d’entraînement est faible). de plus, ces modèles sont facilement interprétables. enﬁn, leur compréhension est une excellente base sur laquelle construire des modèles non linéaires. ce chapitre présente en détail la régression linéaire et son application aux problèmes de classiﬁcation avec la régression logistique. il se termine par quelques mots sur la régression polynomiale. objectifs — distinguer un modèle d’apprentissage paramétrique d’un modèle non paramétrique — formuler un problème de régression linéaire ou logistique — résoudre un problème de régression paramétrique 5.1 apprentissage supervisé d’un modèle paramétrique 5.1.1 modèles paramétriques on parle de modèle paramétrique quand on utilise un algorithme d’apprentissage dont le but est de trou- ver les valeurs optimales des paramètres d’un modèle dont on a déﬁni la forme analytique en fonction des descripteurs (cf. section 4.1.3). la complexité d’un modèle paramétrique grandit avec le nombre de paramètres à apprendre, autre- ment dit avec le nombre de variables. à l’inverse, la complexité d’un modèle non paramétrique aura ten- dance à grandir avec le nombre d’observations. 64 \f5.1. apprentissage supervisé d’un modèle paramétrique 65 exemple un algorithme d’apprentissage qui permet d’apprendre les coeﬃcient α, β, γ dans la fonction de déci- 4 + γex3−x5 apprend un modèle paramétrique. quel que soit le nombre sion suivante : f : (cid:126)x (cid:55)→ αx1 + βx2x2 d’observations, ce modèle ne change pas. à l’inverse, la méthode du plus proche voisin, qui associe à (cid:126)x l’étiquette du point du jeu d’entraînement dont il est le plus proche en distance euclidienne, apprend un modèle non paramétrique : on ne sait pas écrire la fonction de décision comme une fonction des variables prédictives. plus il y a d’observations, plus le modèle pourra apprendre une frontière de décision complexe (cf. chapitre 8). étant donné un jeu d = {(cid:126)x i, yi}i=1,...,n de n observations en p dimensions et leurs étiquettes réelles, nous supposons ici que la fonction de décision f est paramétrée par le vecteur (cid:126)β ∈ rm. nous allons faire l’hypothèse que les erreurs, c’est-à-dire la diﬀérence entre les étiquettes réelles et les valeurs correspondantes de f , sont normalement distribuées, centrées en 0 : y = f ((cid:126)x|(cid:126)β) + (cid:15) (cid:15) ∼ n (0, σ2). (5.1) cette hypothèse est illustrée sur la ﬁgure 5.1 dans le cas d’une fonction de décision linéaire. figure 5.1 – pour une observation x∗ donnée (ici en une dimension) , la distribution des valeurs possibles de l’étiquette y∗ correspondante est une gaussienne centrée en f (x∗). selon cette hypothèse, les observations (cid:126)x sont la réalisation de p variables aléatoires x1, x2, . . . , xp à valeurs réelles, leurs étiquettes y sont la réalisation d’une variable aléatoire y à valeurs réelles, et ces variables aléatoires vériﬁent p(y = y|x = (cid:126)x) ∼ n (cid:16) f ((cid:126)x|(cid:126)β), σ2(cid:17) . (5.2) on note ici p(x = (cid:126)x) pour p(x1 = x1, x2 = x2, . . . , xp = xp). \f66 chapitre 5. régressions paramétriques 5.1.2 estimation par maximum de vraisemblance et méthode des moindres carrés sous l’hypothèse 5.2, et en supposant les n observations indépendantes et identiquement distribuées, le log de vraisemblance du paramètre (cid:126)β vaut : log p(d|(cid:126)β) = log = log n (cid:89) i=1 n (cid:89) i=1 p(x = (cid:126)x i|(cid:126)β) p(yi|(cid:126)x i) + log n (cid:89) i=1 p(x = (cid:126)x i) = − log 1 2σ2 n (cid:88) (cid:16) yi − f ((cid:126)x i|(cid:126)β) (cid:17)2 + c 2π i=1 dans cette dernière équation, c est une constante par rapport à (cid:126)β, et provient d’une part du coeﬃcient 1√ de la distribution normale et d’autre part des p(x = (cid:126)x i). ainsi, maximiser la vraisemblance revient à minimiser (cid:80)n : c’est ce que l’on appelle la minimisation des moindres carrés, une méthode bien connue depuis gauss et legendre. notons aussi que cela revient à minimiser le risque empirique quand il est déﬁni en utilisant la fonction de coût quadratique 2.4.3. yi − f ((cid:126)x i|(cid:126)β) i=1 (cid:16) (cid:17)2 5.2 régression linéaire commençons par considérer des modèles linéaires : nous cherchons à expliquer la variable cible y par une combinaison linéaire – en d’autres mots une somme pondérée – des descripteurs. 5.2.1 formulation nous choisissons une fonction de décision f de la forme f : (cid:126)x (cid:55)→ β0 + p (cid:88) j=1 βjxj. (5.3) ici, (cid:126)β ∈ rp+1 et donc m = p + 1. 5.2.2 solution déﬁnition 5.1 (régression linéaire) on appelle régression linéaire le modèle de la forme f : (cid:126)x (cid:55)→ j=1 βjxj dont les coeﬃcients sont obtenus par minimisation de la somme des moindres carrés, à β0 + (cid:80)p savoir : arg min (cid:126)β∈rp+1 n (cid:88) i=1  yi −  β0 +   2 βjxj   . p (cid:88) j=1 (5.4) (cid:4) nous pouvons réécrire le problème 5.4 sous forme matricielle, en ajoutant à gauche à la matrice d’ob- servations x ∈ rp une colonne de 1 : x ←    1 x1 1 ... ... 1 xn 1    . · · · x1 p ... · · · · · · xn p (5.5) \f5.2. régression linéaire la somme des moindres carrés s’écrit alors rss = (cid:16) (cid:126)y − x (cid:126)β (cid:17)(cid:62) (cid:16) (cid:126)y − x (cid:126)β (cid:17) . 67 (5.6) il s’agit d’une forme quadratique convexe en (cid:126)β, que l’on peut donc minimiser en annulant son gradient rss = −2x (cid:62) (cid:16) . on obtient alors (cid:126)y − x (cid:126)β (cid:17) ∇(cid:126)β théorème 5.1 si le rang de la matrice x est égal à son nombre de colonnes, alors la somme des moindres x (cid:62)x (cid:126)β ∗ = x (cid:62)(cid:126)y. (5.7) carrés 5.6 est minimisée pour (cid:126)β ∗ = (cid:16) x (cid:62)x (cid:17)−1 x (cid:62)(cid:126)y. démonstration. si x est de rang colonne plein, alors x (cid:62)x est inversible. (cid:4) (cid:3) si x (cid:62)x n’est pas inversible, on pourra néanmoins trouver une solution (non unique) pour (cid:126)β en utili- sant à la place de (cid:0)x (cid:62)x(cid:1)−1 un pseudo-inverse (par exemple, celui de moore-penrose) de x (cid:62)x, c’est-à- dire une matrice m telle que x (cid:62)xm x (cid:62)x = x (cid:62)x. on peut aussi (et ce sera préférable quand p est grand et que l’inversion de la matrice x (cid:62)x ∈ rp×p est donc coûteuse) obtenir une estimation de (cid:126)β par un algorithme à directions de descente (voir la sec- tion a.3.3). remarque on fera attention à ne pas confondre les variables, qui sont les p valeurs x1, x2, . . . , xp qui décrivent les données, et les paramètres, qui sont les p + 1 valeurs β0, β1, . . . , βp qui paramètrent le modèle. attention la régression linéaire produit un modèle interprétable, au sens où les βj permettent de comprendre l’importance relative des variables sur la prédiction. en eﬀet, plus |βj| est grande, plus la j-ème variable a un eﬀet important sur la prédiction, et le signe de βj nous indique la direction de cet eﬀet. attention ! cette interprétation n’est valide que si les variables ne sont pas corrélées, et que xj peut être modiﬁée sans perturber les autres variables. de plus, si les variables sont corrélées, x n’est pas de rang colonne plein et x (cid:62)x n’est donc pas inversible. ainsi la régression linéaire admet plusieurs solutions. intuitivement, on peut passer de l’une à l’autre de ces solutions car une perturbation d’un des poids βj peut être compensée en modiﬁant les poids des variables corrélées à xj. 5.2.3 théorème de gauss-markov déﬁnition 5.2 (blue) étant donné un vecteur de paramètres ω en q dimensions, et un vecteur (cid:126)y à partir duquel l’estimer, on dit que l’estimateur ω∗ est le meilleur estimateur non biaisé de ω, ou blue pour best linear unbiased estimator, de ω, si : 1. ω∗ est un estimateur linéaire de ω, autrement dit il existe une matrice a telle que ω∗ = a(cid:126)y ; 2. ω∗ est non biaisé, autrement dit, e[ω∗] = ω 3. quel que soit l’estimateur linéaire non biaisé ˜ω de ω, la variance de cet estimateur est supérieure ou égale à celle de ω∗. (cid:4) \f68 chapitre 5. régressions paramétriques théorème 5.2 soient n observations (cid:126)x 1, (cid:126)x 2, . . . , (cid:126)x n ∈ rp et leurs étiquettes y1, y2, . . . , yn ∈ r. sous j + (cid:15)i et que les (cid:15)i sont normalement distribuées et centrées en 0, (cid:4) l’hypothèse que, pour tout i, yi = β0 + (cid:80)p alors l’estimateur de (cid:126)β par la méthode des moindres carrés en est l’unique blue. j=1 βjxi démonstration. tout d’abord, (cid:126)β ∗ = (cid:0)x (cid:62)x(cid:1)−1 x (cid:62)(cid:126)y et (cid:126)β ∗ est donc linéaire. (cid:104)(cid:0)x (cid:62)x(cid:1)−1 x (cid:62) (cid:16) son espérance est e[(cid:126)β ∗] = e comme x, (cid:126)y et (cid:126)β ne sont pas aléatoires, et que (cid:15) a une espérance de 0, on obtient e[(cid:126)β ∗] = e x (cid:126)β + (cid:15) (cid:17)(cid:105) . (cid:104)(cid:0)x (cid:62)x(cid:1)−1 x (cid:62)x (cid:126)β (cid:105) = (cid:126)β. ainsi, (cid:126)β ∗ est non biaisé. sa variance vaut : var((cid:126)β ∗) = var = var (cid:16) (cid:16) (cid:17) (x (cid:62)x)−1x (cid:62)(cid:126)y (cid:17) (x (cid:62)x)−1x (cid:62)(cid:15) = var (cid:16) (x (cid:62)x)−1x (cid:62) (cid:16) (cid:17)(cid:17) x (cid:126)β + (cid:15) = σ2(x (cid:62)x)−1. enﬁn, supposons que ˜β = a(cid:126)y soit un autre estimateur linéaire et non biaisé de (cid:126)β. par déﬁnition, e[ ˜β] = (cid:17)(cid:105) = (cid:126)β et ainsi ax (cid:126)β = (cid:126)β. comme cela est vrai pour x (cid:126)β + (cid:15) a (cid:16) (cid:104) (cid:126)β. en remplaçant (cid:126)y par sa valeur, on a e tout (cid:126)β, on en conclut que ax = i. posons d = a − (x (cid:62)x)−1x (cid:62), de sorte à ce que ˜β − (cid:126)β ∗ = d(cid:126)y. la variance de ˜β vaut var( ˜β) = var(a(cid:126)y) = var(a(cid:15)) = aa(cid:62)σ2. nous pouvons exprimer aa(cid:62) de la façon suivante : aa(cid:62) = = (cid:16) (cid:16) d + (x (cid:62)x)−1x (cid:62)(cid:17) (cid:16) d + (x (cid:62)x)−1x (cid:62)(cid:17) (cid:16) d + (x (cid:62)x)−1x (cid:62)(cid:17)(cid:62) d(cid:62) + x(x (cid:62)x)−1(cid:17) = dd(cid:62) + dx(x (cid:62)x)−1 + (x (cid:62)x)−1x (cid:62)d(cid:62) + (x (cid:62)x)−1. comme ax = i, dx = ax − (x (cid:62)x)−1x (cid:62)x = 0 et donc x (cid:62)d(cid:62) = 0 de même. on obtient donc aa(cid:62) = dd(cid:62) + (x (cid:62)x)−1. ainsi, var( ˜β) = σ2dd(cid:62) + σ2(x (cid:62)x)−1 = var((cid:126)β ∗) + σ2dd(cid:62). comme σ2 > 0 et dd(cid:62) est semi-déﬁnie positive, var( ˜β) > var((cid:126)β ∗) sauf si d = 0, auquel cas (cid:3) ˜β = (cid:126)β ∗. l’hypothèse de normalité sur (cid:15) n’est pas nécessaire : il suﬃt que les erreurs {(cid:15)i}i=1,...,n aient une espé- rance nulle, aient la même variance ﬁnie σ2 (on parle d’homoscédasticité), et ne soient pas corrélées entre elles (cov((cid:15)i, (cid:15)l) = 0 pour i (cid:54)= l). remarque 5.3 régression logistique supposons maintenant que nous voulions résoudre un problème de classiﬁcation binaire de manière linéaire, c’est-à-dire modéliser y ∈ {0, 1} à l’aide d’une combinaison linéaire de variables. il ne semble pas raisonnable de modéliser directement y comme une combinaison linéaire de plusieurs variables réelles : une telle combinaison est susceptible de prendre non pas deux mais une inﬁnité de va- leurs. \f5.3. régression logistique 69 nous pourrions alors envisager un modèle probabiliste, dans lequel p(y = y|x = (cid:126)x) soit modélisé par une combinaison linéaire des variables de (cid:126)x. cependant, p(y = y|x = (cid:126)x) doit être comprise entre 0 et 1, et intuitivement, cette fonction n’est pas linéaire : si p(y = 0|x = (cid:126)x) est très proche de 1, autrement dit qu’il est très probable que (cid:126)x est négative, une petite perturbation de (cid:126)x ne doit pas beaucoup aﬀecter cette probabilité ; mais à l’inverse, si p(y = 0|x = (cid:126)x) est très proche de 0.5, autrement dit que l’on est très peu certain de l’étiquette de (cid:126)x, rien ne s’oppose à ce qu’une petite perturbation de (cid:126)x n’aﬀecte cette probabilité. c’est pour cela qu’il est classique de modéliser une transformation logit de p(y = y|x = (cid:126)x) comme une combinaison linéaire des variables. déﬁnition 5.3 (fonction logit) on appelle fonction logit la fonction logit : [0, 1] → r p (cid:55)→ log p 1 − p (cid:4) la fonction logit (ﬁgure 5.2a) est la réciproque de la fonction logistique (ﬁgure 5.2b). déﬁnition 5.4 (fonction logistique) on appelle fonction logistique la fonction σ : r → [0, 1] 1 1 + e−u = attention à ne pas confondre la fonction logistique avec l’écart-type, tous deux étant couramment notés (cid:4) σ. eu 1 + eu . u (cid:55)→ (a) fonction logit. (b) fonction logistique. figure 5.2 – fonctions logit et logistique. 5.3.1 formulation ainsi, nous cherchons donc à modéliser log p(y =1|(cid:126)x) 1−p(y =1|(cid:126)x) comme la combinaison linéaire (cid:126)β (cid:62)(cid:126)x, où, de manière équivalente, p(y = 1|(cid:126)x) comme σ((cid:126)β (cid:62)(cid:126)x). nous utilisons ici la transformation 5.5 de (cid:126)x. \f70 chapitre 5. régressions paramétriques étant données n observations d = {(cid:126)x i, yi}i=1,...,n, que l’on suppose iid, le log de la vraisemblance de (cid:126)β est log p(d|(cid:126)β) = log n (cid:89) i=1 p(x = (cid:126)x i, y = yi|(cid:126)β) = log n (cid:89) i=1 p(y = yi|(cid:126)x i, (cid:126)β) + log n (cid:89) i=1 p(x = (cid:126)x i) = = n (cid:88) i=1 n (cid:88) i=1 log p(y = 1|(cid:126)x i, (cid:126)β)yi (cid:16) 1 − p(y = 1|(cid:126)x i, (cid:126)β)1−yi(cid:17) + c (5.8) yi log σ((cid:126)β (cid:62)(cid:126)x i) + (1 − yi) log (cid:16) 1 − σ((cid:126)β (cid:62)(cid:126)x i) (cid:17) + c, où c est une constante par rapport à (cid:126)β. nous cherchons à maximiser cette vraisemblance. déﬁnition 5.5 (régression logistique) on appelle régression logistique le modèle f : x (cid:55)→ σ((cid:126)β (cid:62)(cid:126)x) dont les coeﬃcients sont obtenus par arg max (cid:126)β∈rp+1 n (cid:88) i=1 yi log σ((cid:126)β (cid:62)(cid:126)x i) + (1 − yi) log (cid:16) (cid:17) 1 − σ((cid:126)β (cid:62)(cid:126)x i) . (5.9) (cid:4) maximiser la vraisemblance de β sous ce modèle est équivalent à minimiser le risque empirique déﬁni en utilisant la fonction de coût logistique (voir section 2.4.1). 5.3.2 solution la vraisemblance de la régression logistique 5.8 est une fonction concave. théorème 5.3 le gradient en (cid:126)β du maximum de vraisemblance de la régression logistique vaut n (cid:88) (cid:18) yi − i=1 1 1 + e−β(cid:62)(cid:126)x i (cid:19) (cid:126)x i. (cid:4) démonstration. ce résultat s’obtient en utilisant une propriété remarquable de la fonction logistique : (cid:3) σ(cid:48)(u) = σ(u)(1 − σ(u)). ce gradient ne peut pas être annulé de manière analytique et la régression logistique n’admet donc pas de solution explicite. on trouvera généralement sa solution par l’algorithme du gradient ou une de ses variantes. ces algorithmes convergent vers la solution optimale car la vraisemblance est concave et n’admet donc pas de maximum local. 5.4 régression polynomiale dans le cas de la régression polynomiale de degré d, on cherche maintenant une fonction de décision de la forme suivante : f : (cid:126)x (cid:55)→ β00 + p (cid:88) j=1 β1jxj + p (cid:88) j=1 β2jx2 j + · · · + p (cid:88) j=1 βdjxd j . (5.10) \fil s’agit en fait d’une régression linéaire sur p × d variables : x1, x2, . . . , xp, x2 1, x2 2, . . . , p, . . . , xd (cid:126)x2 1, xd 2, . . . xd p . 71 points clefs — on peut apprendre les coeﬃcients d’un modèle de régression paramétrique par maximisation de vraisemblance, ce qui équivaut à minimiser le risque empirique en utilisant le coût quadratique comme fonction de perte, et revient à la méthode des moindres carrés. — la régression linéaire admet une unique solution (cid:126)β ∗ = (cid:0)x (cid:62)x(cid:1)−1 x (cid:62)(cid:126)y si et seulement si x (cid:62)x est inversible. dans le cas contraire, il existe une inﬁnité de solutions. — la régression polynomiale se ramène à une régression linéaire. — pour un problème de classiﬁcation binaire, utiliser le coût logistique comme fonction de perte dans la minimisation du risque empirique revient à maximiser la vraisemblance d’un modèle dans lequel la probabilité d’appartenir à la classe positive est la transformée logit d’une combinaison linéaire des variables. — la régression logistique n’a pas de solution analytique, mais on peut utiliser un algorithme à direc- tions de descente pour apprendre le modèle. pour aller plus loin • la régression logistique peut naturellement être étendue à la classiﬁcation multi-classe en utilisant l’entropie croisée comme fonction de coût. • nous avons vu comment construire des modèles polynomiaux en appliquant une transformation po- lynomiale aux variables d’entrées. ce n’est pas la seule transformation que l’on puisse leur appliquer, et l’on peut aussi considérer par exemple des fonctions polynomiales par morceaux (ou splines) ou des ondelettes. cet ensemble de techniques est connu sous le nom de basis expansions en anglais. on se référera par exemple au chapitre 5 de hastie et al. (2009). cet ouvrage dans son ensemble permet par ailleurs d’approfondir les notions abordées dans ce chapitre. bibliographie hastie, t., tibshirani, r., et friedman, j. (2009). the elements of statistical learning : data mining, inference, and prediction, second edition. springer-verlag, new york, 2nd edition. https://web.stanford.edu/ ~hastie/elemstatlearn/ \fchapitre 6 régularisation lorsque les variables explicatives sont corrélées, ou trop nombreuses, la complexité d’un modèle de régression linéaire est bien souvent trop élevée, ce qui conduit à une situation de sur-apprentissage. dans ce chapitre, nous verrons comment régulariser ces modèles en contrôlant les coeﬃcients de ré- gression, c’est-à-dire les poids aﬀectés à chacune des variables dans leur combinaison linéaire. nous pré- senterons le cas de la régression linéaire, mais les techniques présentées dans ce chapitre s’appliquent plus largement au cas de la régression logistique. objectifs — contrôler la complexité d’un modèle par la régularisation — déﬁnir le lasso, la régression ridge, et elastic net — comprendre le rôle des normes (cid:96)1 et (cid:96)2 comme régulariseurs — choisir un coeﬃcient de régularisation. 6.1 qu’est-ce que la régularisation ? lorsque les variables sont fortement corrélées, ou que leur nombre dépasse celui des observations, la matrice x ∈ rp+1 représentant nos données ne peut pas être de rang colonne plein. ainsi, la matrice x (cid:62)x n’est pas inversible et il n’existe pas de solution unique à une régression linéaire par minimisation des moindres carrés. il y a donc un risque de sur-apprentissage : le modèle n’étant pas unique, comment peut-on garantir que c’est celui que l’on a sélectionné qui généralise le mieux ? pour limiter ce risque de sur-apprentissage, nous allons chercher à contrôler simultanément l’erreur du modèle sur le jeu d’entraînement et les valeurs des coeﬃcients de régression aﬀectés à chacune des variables. contrôler ces coeﬃcients est une façon de contrôler la complexité du modèle : comme nous le verrons par la suite, ce contrôle consiste à contraindre les coeﬃcients à appartenir à un sous-ensemble strict de rp+1 plutôt que de pouvoir prendre n’importe quelle valeur dans cet espace, ce qui restreint l’espace des solutions possibles. 72 \f6.2. la régression ridge 73 déﬁnition 6.1 (régularisation) on appelle régularisation le fait d’apprendre un modèle en minimisant la somme du risque empirique sur le jeu d’entraînement et d’un terme de contrainte ω sur les solutions possibles : f = arg min h∈f 1 n n (cid:88) l(h((cid:126)x i), yi) + λω(h), i=1 où le coeﬃcient de régularisation λ ∈ r+ contrôle l’importance relative de chacun des termes. (6.1) (cid:4) dans le cas d’un modèle de régression linéaire, nous allons utiliser comme fonction de perte la somme des moindres carrés. les régulariseurs que nous allons voir sont fonction du vecteur de coeﬃcients de régression (cid:126)β : ou, de manière équivalente, (cid:16) (cid:126)y − x (cid:126)β (cid:17)(cid:62) (cid:16) (cid:126)y − x (cid:126)β (cid:17) + λω((cid:126)β) arg min (cid:126)β∈rp+1 arg min (cid:126)β∈rp+1 (cid:12) (cid:12) (cid:12)(cid:126)y − x (cid:126)β (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2 2 + λω((cid:126)β). (6.2) nous utilisons ici la transformation 5.5 de (cid:126)x qui consiste à ajouter à la matrice de design x une colonne de 1 pour simpliﬁer les notations. encart coefﬁcient de régularisation le coeﬃcient de régularisation λ est un hyperparamètre de la régression linéaire régularisée. quand λ tend vers +∞, le terme de régularisation prend de plus en plus d’importance, jusqu’à ce qu’il domine le terme d’erreur et que seule compte la minimisation du régulariseur. dans la plupart des cas, le régulariseur est minimisé quand (cid:126)β = (cid:126)0, et il n’y a plus d’apprentissage. à l’inverse, quand λ tend vers 0, le terme de régularisation devient négligeable devant le terme d’erreur, et (cid:126)β prendra comme valeur une solution de la régression linéaire non régularisée. comme tout hyperparamètre, λ peut être choisi par validation croisée. on utilisera généralement une grille de valeurs logarithmique. 6.2 la régression ridge une des formes les plus courantes de régularisation, utilisée dans de nombreux domaines faisant in- tervenir des problèmes inverses mal posés, consiste à utiliser comme régulariseur la norme (cid:96)2 du vecteur (cid:126)β : ωridge((cid:126)β) = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:126)β (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2 2 = p (cid:88) j=0 β2 j . (6.3) 6.2.1 formulation de la régression ridge déﬁnition 6.2 (régression ridge) on appelle régression ridge le modèle f : x (cid:55)→ (cid:126)β (cid:62)(cid:126)x dont les coeﬃ- cients sont obtenus par arg min (cid:126)β∈rp+1 (cid:12) (cid:12) (cid:12)(cid:126)y − x (cid:126)β (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2 2 + λ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:126)β (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2 2 . (6.4) (cid:4) \f74 chapitre 6. régularisation la régression ridge est un cas particulier de régularisation de tikhonov (développée pour résoudre des équations intégrales). elle intervient aussi dans les réseaux de neurones, où elle est appelée weight decay (dégradation / modération des pondérations, voir section 7.2.4). 6.2.2 solution le problème 6.4 est un problème d’optimisation convexe (voir section a) : il s’agit de minimiser une forme quadratique. il se résout en annulant le gradient en (cid:126)β de la fonction objective : (cid:18)(cid:12) (cid:12) (cid:12)(cid:126)y − x (cid:126)β (cid:12) (cid:12) (cid:12) ∇(cid:126)β + λ = 0 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:19) (cid:126)β 2 2 2 2 en notant ip ∈ rp×p la matrice identité en dimension p, on obtient : (cid:16) λip + x (cid:62)x (cid:17) (cid:126)β ∗ = x (cid:62)(cid:126)y. (6.5) (6.6) comme λ > 0, la matrice λip + x (cid:62)x est toujours inversible. notre problème admet donc toujours une unique solution explicite. la régularisation par la norme (cid:96)2 a permis de transformer un problème poten- tiellement mal posé en un problème bien posé, dont la solution est : (cid:16) (cid:126)β ∗ = (cid:17)−1 λip + x (cid:62)x remarque x (cid:62)(cid:126)y. (6.7) si l’on multiplie la variable xj par une constante α, le coeﬃcient correspondant dans la régression li- néaire non régularisée est divisé par α. en eﬀet, si on appelle x ∗ la matrice obtenue en remplaçant xj par (cid:126)y − x ∗ (cid:126)β ∗(cid:17) αxj dans x, la solution (cid:126)β ∗ de la régression linéaire correspondante vériﬁe x ∗ (cid:16) = 0, tandis que la solution (cid:126)β de la régression linéaire sur x vériﬁe x = 0. ainsi, changer l’échelle d’une va- riable a comme seul impact sur la régression linéaire non régularisée d’ajuster le coeﬃcient correspondant de manière inversement proportionnelle. (cid:16) (cid:126)y − x (cid:126)β (cid:17) à l’inverse, dans le cas de la régression ridge, remplacer xj par αxj aﬀecte aussi le terme de régulari- sation, et a un eﬀet plus complexe. l’échelle relative des diﬀérentes variables peut donc fortement aﬀecter la régression ridge. il est ainsi recommandé de standardiser les variables avant l’apprentissage, c’est-à-dire de toutes les ramener à avoir un écart-type de 1 en les divisant par leur écart-type : xi j ← (cid:114) xi j (cid:16) 1 n (cid:80)n i=1 j − 1 xi n (cid:80)n i=1 xi j (cid:17)2 (6.8) attention : pour éviter le sur-apprentissage, il est important que cet écart-type soit calculé sur le jeu d’en- traînement uniquement, puis appliqué ensuite aux jeux de test ou validation. la régression ridge a un eﬀet de « regroupement » sur les variables corrélées, au sens où des variables corrélées auront des coeﬃcients similaires. 6.2.3 chemin de régularisation déﬁnition 6.3 (chemin de régularisation) on appelle chemin de régularisation l’évolution de la valeur (cid:4) du coeﬃcient de régression d’une variable en fonction du coeﬃcient de régularisation λ. le chemin de régularisation permet de comprendre l’eﬀet de la régularisation sur les valeurs de (cid:126)β. en voici un exemple sur la ﬁgure 6.1. \f6.2. la régression ridge 75 figure 6.1 – chemin de régularisation de la régression ridge pour un jeu de données avec 12 variables. chaque ligne représente l’évolution du coeﬃcient de régression d’une de ces variables quand λ augmente : le coeﬃcient évolue de sa valeur dans la régression non régularisée vers 0. 6.2.4 interprétation géométrique théorème 6.1 étant donnés λ ∈ r+, x ∈ rn×p et (cid:126)y ∈ rn, il existe un unique t ∈ r+ tel que le problème 6.4 soit équivalent à arg min (cid:126)β∈rp+1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)(cid:126)y − x (cid:126)β (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2 (cid:12) (cid:12) 2 tel que (cid:12) (cid:12) (cid:12) (cid:12) (cid:126)β (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2 (cid:12) (cid:12) 2 ≤ t. (6.9) (cid:4) démonstration. l’équivalence s’obtient par dualité et en écrivant les conditions de karun-kush-tucker. (cid:3) la régression ridge peut donc être formulée comme un problème d’optimisation quadratique (minimi- (cid:12) (cid:12) (cid:12)(cid:126)y − x (cid:126)β (cid:12) (cid:12) (cid:12) ≤ t) : la solution doit être contenue dans la boule (cid:96)2 de rayon t. sauf dans le cas où l’optimisation sans contrainte vériﬁe déjà la condition, cette solution sera sur la ) sous contraintes ( (cid:12) (cid:126)β (cid:12) (cid:12) (cid:12) 2 (cid:12) (cid:12) 2 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2 2 ser √ frontière de cette boule, comme illustré sur la ﬁgure 6.2. \f76 chapitre 6. régularisation figure 6.2 – la solution du problème d’optimisation sous contraintes 6.9 (ici en deux dimensions) se situe sur une ligne de niveau de la somme des moindres carrés tangente à la boule (cid:96)2 de rayon √ t. 6.3 le lasso 6.3.1 parcimonie dans certaines applications, il peut être raisonnable de supposer que l’étiquette que l’on cherche à prédire n’est expliquée que par un nombre restreint de variables. il est dans ce cas souhaitable d’avoir un modèle parcimonieux, ou sparse, c’est-à-dire dans lequel un certain nombre de coeﬃcients sont nuls : les variables correspondantes peuvent être retirées du modèle. pour ce faire, robert tibshirani a proposé en 1996 d’utiliser comme régulariseur la norme (cid:96)1 du coeﬃ- cient (cid:126)β : ωlasso((cid:126)β) = (cid:12) (cid:12) (cid:12) (cid:12) (cid:126)β (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)1 = p (cid:88) |βj|. j=0 (6.10) pour comprendre pourquoi ce régulariseur permet de « pousser » certains coeﬃcients vers 0, on se reportera à la section 6.3.4 et à la ﬁgure 6.3. 6.3.2 formulation du lasso déﬁnition 6.4 (lasso) on appelle lasso le modèle f : x (cid:55)→ (cid:126)β (cid:62)(cid:126)x dont les coeﬃcients sont obtenus par arg min (cid:126)β∈rp+1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)(cid:126)y − x (cid:126)β (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2 (cid:12) (cid:12) 2 + λ (cid:12) (cid:12) (cid:12) (cid:12) (cid:126)β (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)1 . (6.11) (cid:4) \f6.3. le lasso 77 le nom de lasso est en fait un acronyme, pour least absolute shrinkage and selection operator : il s’agit d’une méthode qui utilise les valeurs absolues des coeﬃcients (la norme (cid:96)1) pour réduire (shrink) ces co- eﬃcients, ce qui permet de sélectionner les variables qui n’auront pas un coeﬃcient nul. en traitement du signal, le lasso est aussi connu sous le nom de poursuite de base (basis pursuit en anglais). en créant un modèle parcimonieux et en permettant d’éliminer les variables ayant un coeﬃcient nul, le lasso est une méthode de sélection de variable supervisée. il s’agit donc aussi d’une méthode de réduction de dimension. remarque 6.3.3 solution le lasso 6.11 n’admet pas de solution explicite. on pourra utiliser un algorithme à directions de descente (voir section a.3.3) pour le résoudre. de plus, il ne s’agit pas toujours d’un problème strictement convexe (en particulier, quand p > n) et il n’admet donc pas nécessairement une unique solution. en pratique, cela pose surtout problème quand les variables ne peuvent pas être considérées comme les réalisations de lois de probabilité continues. néanmoins, il est possible de montrer que les coeﬃcients non nuls dans deux solutions ont nécessairement le même signe. ainsi, l’eﬀet d’une variable a la même direction dans toutes les solutions qui la considèrent, ce qui facilite l’interprétation d’un modèle appris par le lasso. 6.3.4 interprétation géométrique comme précédemment, le problème 6.11 peut être reformulé comme un problème d’optimisation qua- dratique sous contraintes : théorème 6.2 étant donnés λ ∈ r+, x ∈ rn×p et (cid:126)y ∈ rn, il existe un unique t ∈ r+ tel que le problème 6.11 soit équivalent à arg min (cid:126)β∈rp+1 (cid:12) (cid:12) (cid:12)(cid:126)y − x (cid:126)β (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2 (cid:12) (cid:12) (cid:12) (cid:12) 2 tel que (cid:12) (cid:12) (cid:126)β (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)1 ≤ t. (6.12) (cid:4) la solution doit maintenant être contenue dans la boule (cid:96)1 de rayon t. comme cette boule a des « coins », les lignes de niveau de la forme quadratique sont plus susceptibles d’y être tangente en un point où une ou plusieurs coordonnées sont nulles (voir ﬁgure 6.3). 6.3.5 chemin de régularisation sur le chemin de régularisation du lasso (par exemple ﬁgure 6.4, sur les mêmes données que pour la ﬁgure 6.1), on observe que les variables sortent du modèle les unes après les autres, jusqu’à ce que tous les coeﬃcients soient nuls. on remarquera aussi que le chemin de régularisation pour n’importe quelle variable est linéaire par morceaux ; c’est une propriété du lasso. remarque (stabilité) si plusieurs variables corrélées contribuent à la prédiction de l’étiquette, le lasso va avoir tendance à choisir une seule d’entre elles (aﬀectant un poids de 0 aux autres), plutôt que de répartir les poids équitable- ment comme la régression ridge. c’est ainsi qu’on arrive à avoir des modèles très parcimonieux. cependant, \f78 chapitre 6. régularisation figure 6.3 – la solution du problème d’optimisation sous contraintes 6.12 (ici en deux dimensions) se situe sur une ligne de niveau de la somme des moindres carrés tangente à la boule (cid:96)1 de rayon t. le choix de cette variable est aléatoire, et peut changer si l’on répète la procédure d’optimisation. le lasso a donc tendance à être instable. 6.4 elastic net la régularisation par la norme (cid:96)1 permet d’obtenir un modèle parcimonieux et donc plus facilement interprétable, tandis que la régularisation par la norme (cid:96)2 permet elle d’éviter le sur-apprentissage et de grouper les variables corrélées. il est assez naturel de souhaiter combiner les deux normes dans le régula- riseur suivant : ωenet((cid:126)β) = (cid:18) (1 − α) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:126)β (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)1 (cid:12) + α (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:126)β (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:19) 2 2 . (6.13) ce régulariseur est paramétré par α ∈ [0, 1]. quand α = 0, on retrouve la régularisation (cid:96)1, et quand α = 1, la régularisation (cid:96)2. déﬁnition 6.5 (elastic net) on appelle elastic net le modèle f : x (cid:55)→ (cid:126)β (cid:62)(cid:126)x dont les coeﬃcients sont obtenus par arg min (cid:126)β∈rp+1 (cid:12) (cid:12) (cid:12)(cid:126)y − x (cid:126)β (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2 2 (cid:18) + λ (1 − α) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:126)β (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)1 + α (cid:19) . (cid:12) (cid:12) (cid:12) (cid:12) (cid:126)β (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2 (cid:12) (cid:12) 2 (6.14) (cid:4) \f6.4. elastic net 79 figure 6.4 – chemin de régularisation du lasso pour un jeu de données avec 12 variables. chaque ligne représente l’évolution du coeﬃcient de régression d’une de ces variables quand λ augmente : les variables sont éliminées les unes après les autres. on obtient la solution de l’elastic net par un algorithme à directions de descente. cette solution est parcimonieuse, mais moins que celle du lasso. en eﬀet, quand plusieurs variables fortement corrélées sont pertinentes, le lasso sélectionnera une seule d’entre elles, tandis que, par eﬀet de la norme (cid:96)2, l’elastic net les sélectionnera toutes et leur aﬀectera le même coeﬃcient. points clefs — ajouter un terme de régularisation, fonction du vecteur des coeﬃcients (cid:126)β, au risque empirique de la régression linéaire permet d’éviter le sur-apprentissage — la régression ridge utilise la norme (cid:96)2 de (cid:126)β comme régulariseur ; elle admet toujours une unique solution analytique, et a un eﬀet de regroupement sur les variables corrélées. — le lasso utilise la norme (cid:96)1 de (cid:126)β comme régulariseur ; il crée un modèle parcimonieux, et permet donc d’eﬀectuer une réduction de dimension supervisée. — de nombreux autres régulariseurs sont possibles en fonction de la structure du problème. pour aller plus loin • au-delà des normes (cid:96)1 et (cid:96)2, il est possible d’utiliser des régulariseurs de la forme ω(cid:96)q ((cid:126)β) = ||(cid:126)β||q q. \f80 chapitre 6. régularisation • une famille de régulariseurs appelés « structurés » permettent de sélectionner des variables qui respectent une structure (graphe, groupes, ou arbre) donnée a priori. ces approches sont utilisées en particulier dans des applications bio-informatiques, par exemple quand on cherche à construire des modèles parcimonieux basés sur l’expression de gènes sous l’hypothèse que seul un petit nombre de voies métaboliques (groupes de gènes) est pertinent. pour plus de détails, on se référera à l’article de huang et al. (2011). • en ce qui concerne l’unicité de la solution du lasso, on se reportera à l’article de tibshirani (2013). • l’ouvrage de hastie et al. (2015) est entièrement consacré au lasso et ses généralisations. bibliographie hastie, t., tibshirani, r., et wainwright, m. (2015). statistical learning with sparsity : the lasso and generaliza- tions. crc press. http://web.stanford.edu/~hastie/statlearnsparsity/. huang, j., zhang, t., et metaxas, d. (2011). learning with structured sparsity. journal of machine learning research, 12 :3371–3412. tibshirani, r. j. (2013). the lasso problem and uniqueness. electronic journal of statistics, 7 :1456–1490. \fchapitre 7 réseaux de neurones artiﬁciels de l’annotation automatique d’images à la reconnaissance vocale, en passant par des ordinateurs ca- pables de battre des champions de go et par les véhicules autonomes, les récents succès de l’intelligence artiﬁcielle sont nombreux à reposer sur les réseaux de neurones profonds, et le deep learning (ou appren- tissage profond) fait beaucoup parler de lui. les réseaux de neurones artiﬁciels ne sont au fond rien d’autre que des modèles paramétriques, poten- tiellement complexes : contrairement à la régression linéaire, ils permettent de construire facilement des modèles très ﬂexibles. dans ce chapitre, nous aborderons les principes de base d’une classe particulière de réseaux de neurones artiﬁciels, les perceptrons multi-couche, et de leur entraînement. nous eﬄeurerons la question des réseaux de neurones profonds, que nous considérons comme un sujet avancé n’ayant pas sa place dans cet ouvrage. objectifs — écrire la fonction de décision correspondant à un perceptron uni- ou multi-couche — implémenter la procédure d’apprentissage d’un perceptron — expliciter les actualisations d’un perceptron multi-couche dans une procédure de rétropropagation — connaître quelques grands principes de la conception et de l’entraînement d’un perceptron multi- couche — appréhender les enjeux du deep learning. 7.1 le perceptron l’histoire des réseaux de neurones artiﬁciels remonte aux années 1950 et aux eﬀorts de psychologues comme franck rosenblatt pour comprendre le cerveau humain. initialement, ils ont été conçus dans le but de modéliser mathématiquement le traitement de l’information par les réseaux de neurones biologiques qui se trouvent dans le cortex des mammifères. de nos jours, leur réalisme biologique importe peu et c’est leur eﬃcacité à modéliser des relations complexes et non linéaires qui fait leur succès. 81 \f82 chapitre 7. réseaux de neurones artiﬁciels le premier réseau de neurones artiﬁciels est le perceptron (rosenblatt, 1957). loin d’être profond, il comporte une seule couche et a une capacité de modélisation limitée. 7.1.1 modèle le perceptron (ﬁgure 7.1) est formé d’une couche d’entrée de p neurones, ou unités, correspondant chacune à une variable d’entrée. ces neurones transmettent la valeur de leur entrée à la couche suivante. à ces p neurones on rajoute généralement une unité de biais, qui transmet toujours la valeur 1. cette unité correspond à la colonne de 1 que nous avons ajoutée aux données dans les modèles linéaires (équation 5.5). on remplacera dans ce qui suit tout vecteur (cid:126)x = (x1, x2, . . . , xp) par sa version augmentée d’un 1 : (cid:126)x = (1, x1, x2, . . . , xp). la première et unique couche du perceptron (après la couche d’entrée) contient un seul neurone, au- quel sont connectées toutes les unités de la couche d’entrée. ce neurone calcule une combinaison linéaire o((cid:126)x) = w0 + (cid:80)p j=1 wjxj des signaux x1, x2, . . . , xp qu’il reçoit en entrée, auquel il applique une fonction d’activation a, dont il transmet en sortie le résultat. cette sortie met en œuvre la fonction de décision du perceptron. ainsi, si l’on appelle wj le poids de connexion entre l’unité d’entrée j et le neurone de sortie, ce neurone calcule  f ((cid:126)x) = a(o((cid:126)x)) = a w0 +  wjxj  = a ((cid:104) (cid:126)w, (cid:126)x(cid:105)) . p (cid:88) j=1 (7.1) il s’agit donc bien d’un modèle paramétrique. figure 7.1 – architecture d’un perceptron. fonctions d’activation dans le cas d’un problème de régression, on utilisera tout simplement l’identité comme fonction d’ac- tivation. dans le cas d’un problème de classiﬁcation binaire, on pourra utiliser : — pour prédire directement une étiquette binaire, une fonction de seuil : f : (cid:126)x (cid:55)→ (cid:40) 0 1 si o((cid:126)x) ≤ 0 sinon. (7.2) \f7.1. le perceptron 83 — pour prédire la probabilité d’appartenir à la classe positive, comme dans le cas de la régression lo- gistique (section 5.3), une fonction logistique : f : (cid:126)x (cid:55)→ 1 1 + eo((cid:126)x) = 1 1 + exp ((cid:104) (cid:126)w, (cid:126)x(cid:105)) . (7.3) classiﬁcation multi-classe dans le cas d’un problème de classiﬁcation multi-classe, on modiﬁera l’architecture du perceptron de sorte à n’avoir non plus 1 mais c neurones dans la couche de sortie, où c est le nombre de classes. les p + 1 neurones de la couche d’entrée seront ensuite tous connectés à chacun de ces neurones de sortie (on aura donc (p + 1)c poids de connexion, notés wc j .) cette architecture est illustrée sur la ﬁgure 7.2. on utilise alors comme fonction d’activation la fonction softmax : déﬁnition 7.1 (fonction softmax) on appelle fonction softmax, ou fonction exponentielle normalisée, la fonction σ : rc → [0, 1]c déﬁnie par : σ(o1, o2, . . . , oc)c = eoc k=1 eok (cid:80)c . (cid:4) dans le cas d’un perceptron classique, ok est la combinaison linéaire calculée au k-ème neurone de sortie : ok = (cid:104) (cid:126)w k, (cid:126)x(cid:105). la fonction softmax généralise la fonction logistique utilisée dans le cas binaire. elle produit c nombres réels positifs de somme 1, et peut être considérée comme une version régulière de la fonction arg max : quand oc > ok(cid:54)=c, σ((cid:126)o)c ≈ 1 et σ((cid:126)o)k(cid:54)=c ≈ 0. figure 7.2 – architecture d’un perceptron multi-classe. 7.1.2 entraînement pour entraîner un perceptron, nous allons chercher, comme par exemple pour la régression paramé- trique, à minimiser le risque empirique. cependant, nous allons supposer que les observations ((cid:126)x i, yi) ne sont pas disponibles simultanément, mais qu’elles sont observées séquentiellement. cette hypothèse dé- coule de la plasticité des réseaux de neurones biologiques : ils s’adaptent constamment en fonction des softmax\f84 chapitre 7. réseaux de neurones artiﬁciels signaux qu’ils reçoivent. nous allons donc utiliser un algorithme d’entraînement incrémental, qui s’adapte à des observations arrivant les unes après les autres. déﬁnition 7.2 (apprentissage incrémental vs hors-ligne) un algorithme d’apprentissage qui opère sur un unique ensemble de n observations est appelé hors-ligne. en anglais, on parlera de batch learning. par contraste, un algorithme d’apprentissage qui eﬀectue une ou plusieurs opérations à chaque nou- velle observation qui lui est fournie est appelé incrémental ou en ligne. en anglais, on parlera de online learning. (cid:4) pour minimiser le risque empirique de manière itérative, nous allons nous appuyer sur l’algorithme du gradient (voir section a.3.3). l’algorithme commence par une initialisation aléatoire du vecteur de poids de connexion w(0) p , par exemple, (cid:126)w = (cid:126)0. 0 , w(0) 1 , . . . , w(0) puis, à chaque observation, on ajuste ce vecteur dans la direction opposée au gradient du risque em- pirique. en eﬀet, ce gradient indique la direction de plus forte pente du risque empirique ; le redescendre nous rapproche d’un point où ce risque est minimal. formellement, à une itération de l’algorithme, on tire une nouvelle observation ((cid:126)x i, yi) et on actualise, pour tout j, les poids de connexion de la façon suivante : wj ← wj − η ∂l(f ((cid:126)x i), yi) ∂wj . (7.4) il est possible (et même recommandé dans le cas où les données ne sont pas extrêmement volumineuses) d’itérer plusieurs fois sur l’intégralité du jeu de données. typiquement, on itère jusqu’à ce que l’algorithme converge à (cid:15) près. vitesse d’apprentissage cet algorithme a un hyperparamètre, η > 0, qui est le pas de l’algorithme du gradient et que l’on appelle la vitesse d’apprentissage (ou learning rate) dans le contexte des réseaux de neurones artiﬁciels. cet hyperparamètre joue un rôle important : s’il est trop grand, l’algorithme risque d’osciller autour de la so- lution optimale, voire de diverger. à l’inverse, s’il est trop faible, l’algorithme va converger très lentement. il est donc essentiel de bien choisir sa vitesse d’apprentissage. en pratique, on utilise souvent une vitesse d’apprentissage adaptative : relativement grande au début, puis de plus en plus faible au fur et à mesure que l’on se rapproche de la solution. cette approche est à rapprocher d’algorithmes similaires développés dans le cas général de l’algorithme du gradient (comme par exemple la recherche linéaire par rebroussement, voir section a.3.4). classiﬁcation binaire le cas de la classiﬁcation binaire, utilisant un seuil comme fonction d’activation, est historiquement le premier à avoir été traité. la fonction de coût utilisée est connue sous le nom de critère du perceptron : l(f ((cid:126)x i), yi) = max(0, −yio((cid:126)x i)) = max(0, −yi(cid:104) (cid:126)w, (cid:126)x(cid:105)) (7.5) ce critère est proche de la fonction d’erreur hinge. quand la combinaison linéaire des entrées a le bon signe, le critère du perceptron est nul. quand elle a le mauvais signe, le critère du perceptron est d’autant plus grand que cette combinaison linéaire est éloignée de 0. en utilisant ce critère, la règle d’actualisation 7.4 devient : wj ← (cid:40) 0 −yixi j si yio((cid:126)x i) > 0 sinon. ainsi, quand le perceptron fait une erreur de prédiction, il déplace la frontière de décision de sorte à cor- riger cette erreur. \f7.1. le perceptron 85 théorème 7.1 (théorème de convergence du perceptron) étant donné un jeu de n observations étique- tées d = {((cid:126)x i, yi)i=1,...,n} et d, γ ∈ r∗ +. si : — ∀i = 1, . . . , n, (cid:12) (cid:12) (cid:12)(cid:126)x i(cid:12) (cid:12) (cid:12)2 ≤ d (cid:12) (cid:12) — il existe (cid:126)u ∈ rp+1 tel que — ||(cid:126)u||2 = 1 et — ∀i = 1, . . . , n, yi(cid:104)(cid:126)u, (cid:126)x(cid:105) ≥ γ alors l’algorithme du perceptron converge en au plus (1962). régression (cid:17)2 (cid:16) d γ étapes. ce théorème est un résultat de albert novikoﬀ (cid:4) dans le cas de la régression, on utilise pour le coût empirique la fonction de coût quadratique : l(f ((cid:126)x i), yi) = (yi − f ((cid:126)x i))2 = 1 2 (cid:0)yi − (cid:104) (cid:126)w, (cid:126)x(cid:105)(cid:1)2 . 1 2 la règle d’actualisation 7.4 devient : wj ← wj − η(f ((cid:126)x i) − yi)xi j. (7.6) (7.7) classiﬁcation probabiliste dans le cas de la classiﬁcation (binaire ou multi-classe), on utilise l’entropie croisée comme fonction de coût : l(f ((cid:126)x i), yi) = − c (cid:88) c=1 δ(yi, c) log fc((cid:126)x i) = − c (cid:88) c=1 δ(yi, c) log exp((cid:104) (cid:126)w c, (cid:126)x(cid:105) k=1 exp((cid:104) (cid:126)w k, (cid:126)x(cid:105)) (cid:80)c . quelques lignes de calcul montrent que l’on obtient alors la même règle d’actualisation que pour la régres- sion (7.7) : wk j ← wk j − η(yi k − f ((cid:126)x i))xi j. ces calculs sont aussi valables dans le cas de la classiﬁcation binaire, et la règle est exactement celle de la formule 7.7. 7.1.3 modélisation de fonctions booléennes les perceptrons, étant capables d’apprendre des fonctions linéaires, sont donc capables d’exécuter certaines fonctions logiques sur des vecteurs d’entrées binaires. ainsi, un perceptron dont les poids sont w0 = −1.5, w1 = 1 et w2 = 1 calcule l’opérateur ∧ (dont la table de vérité est rappelée sur la ﬁgure 7.3) et entre x1 et x2. x1 x2 x1 ∧ x2 0 0 1 1 0 1 0 1 0 0 0 1 figure 7.3 – table de vérité du connecteur logique « et » (∧). par contraste, aucun perceptron ne peut apprendre la fonction « ou exclusif » (xor), dont la table de vérité est rappelée sur la ﬁgure 7.4 : en eﬀet, aucune droite ne peut séparer les points (0, 0) et (1, 1) d’un côté et (0, 1) et (1, 0) de l’autre. \f86 chapitre 7. réseaux de neurones artiﬁciels x1 x2 x1 ⊕ x2 0 0 1 1 0 1 1 0 0 1 0 1 figure 7.4 – table de vérité du connecteur logique xor. 7.2 perceptron multi-couche la capacité de modélisation du perceptron est limitée car il s’agit d’un modèle linéaire. après l’en- thousiasme généré par les premiers modèles connexionistes, cette réalisation a été à l’origine d’un certain désenchantement au début des années 1970. dans leur ouvrage de 1969 sur le sujet, seymour papert (mathématicien et éducateur) et marvin minksy (spécialiste en sciences cognitives) évoquent même leur intuition de l’inutilité d’étendre le perceptron à des architectures multi-couche : « l’étude du perceptron s’est révélée fort intéressante en dépit de ses fortes limita- tions, et peut-être même grâce à elles. plusieurs de ses caractéristiques attirent l’attention : sa linéarité, son intriguant théorème d’apprentissage, et la simplicité de son paradigme de calcul parallèle. il n’y a aucune raison de supposer qu’aucune de ses vertus soient transposables à la version multi-couche. néanmoins, nous considérons qu’il s’agit d’un problème de recherche important que d’expliciter (ou de rejeter) notre jugement intuitif de la stérilité d’une extension multi-couche.» (minsky and papert, 1972) dans cette section, nous verrons comment l’histoire leur a donné tort. 7.2.1 architecture on appelle perceptron multi-couche, ou multi-layer perceptron (mlp) en anglais, un réseau de neurones construit en insérant des couches intermédiaires entre la couche d’entrée et celle de sortie d’un perceptron. on parlera parfois de couches cachées par référence à l’anglais hidden layers. chaque neurone d’une couche intermédiaire ou de la couche de sortie reçoit en entrée les sorties des neurones de la couche précédente. il n’y a pas de retour d’une couche vers une couche qui la précède ; on parle ainsi aussi d’un réseau de neurones à propagation avant, ou feed-forward en anglais. en utilisant des fonctions d’activation non linéaires, telles que la fonction logistique ou la fonction tangente hyperbolique, on crée ainsi un modèle paramétrique hautement non linéaire. exemple prenons l’exemple d’un perceptron avec deux couches intermédiaires comme illustré sur la ﬁgure 7.5. le poids de la connexion du neurone j de la couche h − 1 au neurone q de la couche h, ah la notons wh jq fonction d’activation utilisée en sortie de la couche h, et ph le nombre de neurones dans la couche h. la sortie z1 q du q-ème neurone de la première couche cachée vaut  p (cid:88) z1 q = a1   w1 jqxj  . la sortie z2 q du q-ème neurone de la deuxième couche cachée vaut j=0 z2 q = a2   p1 (cid:88) j=1  w2 jqz1 j  . \f7.2. perceptron multi-couche enﬁn, la sortie du perceptron vaut f ((cid:126)x) = a3 87  w3 j z2 j  .   p2 (cid:88) ainsi, en supposant qu’on utilise une fonction logistique pour tous les neurones des couches cachées, j=1 la sortie du perceptron vaut  f ((cid:126)x) = a3 p2 (cid:88)    w3 jq j=0 1 + exp 1 (cid:18) − (cid:80)p1 j=0 w2 jq     , (cid:19) 1 1+exp(− (cid:80)p j=0 w1 jqxj) ce qui devrait vous convaincre de la capacité du perceptron multi-couche à modéliser des fonctions non linéaires. figure 7.5 – architecture d’un perceptron multi-couche. attention le perceptron multi-couche est un modèle paramétrique dont les paramètres sont les poids de connexions . (le nombre de couches et leurs nombres de neurones font partie des hyperparamètres : on les suppose wh jq ﬁxés, ce ne sont pas eux que l’on apprend). ce modèle a donc d’autant plus de paramètres (c’est-à-dire de poids de connexion) qu’il y a de couches intermédiaires et de neurones dans ces couches. c’est pour éviter le sur-apprentissage que les réseaux de neurones profonds requièrent souvent des quantités massives de données pour apprendre de bons modèles. 7.2.2 approximation universelle théorème 7.2 (approximation universelle) soit a : r → r une fonction non constante, bornée, continue et croissante et k un sous-ensemble compact de rp . étant donné (cid:15) > 0 et une fonction f continue sur k, il existe un \f88 chapitre 7. réseaux de neurones artiﬁciels entier m, m scalaires {di}i=1,...,m, m scalaires {bi}i=1,...,m, et m vecteurs { (cid:126)wi}i=1,...,m de rp tels que pour tout (cid:126)x ∈ k, m (cid:88) |f ((cid:126)x) − di a ((cid:104) (cid:126)wi, (cid:126)x(cid:105) + bi)| < (cid:15). en d’autres termes, toute fonction continue sur un sous-ensemble compact de rp peut être approchée avec un degré de précision arbitraire par un perceptron multi-couche à une couche intermédiaire contenant un nombre ﬁni de neurones. (cid:4) i=1 ce théorème, dû à george cybenko (1989) et aﬃné par kurt hornik (1991), contraste fortement avec l’intuition de papert et minsky : nous pouvons maintenant apprendre toutes sortes de fonctions ! atten- tion cependant, ce résultat ne nous donne ni le nombre de neurones qui doivent composer cette couche intermédiaire, ni les poids de connexion à utiliser. les réseaux de neurones à une seule couche cachée sont généralement peu eﬃcaces, et on aura souvent de meilleurs résultats en pratique avec plus de couches. 7.2.3 modéliser xor avec un perceptron multi-couche la fonction xor, dont la table de vérité est donnée sur la ﬁgure 7.4, peut être facilement modélisée par un perceptron à une couche intermédiaire utilisant uniquement des fonctions de seuil comme fonctions d’activation. on peut par exemple utiliser une couche intermédiaire à 3 neurones, dont un biais, avec les poids suivants : — w1 1 = −0.5 + x1 − x2, ce qui permet de séparer (1, 0) 21 = −1, de sorte que o1 01 = −0.5, w1 11 = 1, w1 des autres possibilités ; — w1 02 = −0.5, w1 12 = −1, w1 (0, 1) des autres possibilités ; 22 = +1, de sorte que o1 2 = −0.5 − x1 + x2, ce qui permet de séparer — w2 0 = −0.5, w2 1 = 1, w2 2 = 1, de sorte que f ((x1, x2)) = −0.5 + sign(o1 1) + sign(z1 2) pour ﬁnir. 7.2.4 entraînement par rétropropagation conditionnement et instabilité du gradient il est important de remarquer que la minimisation du risque empirique pour un perceptron multi- couche n’est pas un problème d’optimisation convexe. ainsi, nous n’avons pas d’autres choix que d’utiliser un algorithme à directions de descente (voir section a.3.3), sans aucune garantie de converger vers un minimum global. en eﬀet, le gradient de la fonction de coût par rapport à un poids de connexion est généralement mal conditionné, ce qui signiﬁe qu’une petite perturbation des conditions initiales de l’algorithme conduira à des résultats très diﬀérents. de plus, le gradient par rapport à un poids dans une des premières couches intermédiaires est souvent instable. il aura tendance à être soit très faible, ce qui ralentira l’apprentissage (c’est ce que l’on appelle le problème de la disparition du gradient, ou vanishing gradient en anglais), soit à prendre des valeurs de plus en plus élevées à chaque itération, conduisant à l’inverse à un problème d’explosion du gradient (exploding gradient en anglais.) l’initialisation des poids de connexion, la standardisation des variables, le choix de la vitesse d’appren- tissage et celui des fonctions d’activation ont tous un impact sur la capacité du perceptron multi-couche à converger vers une bonne solution. c’est pour cela que l’entraînement d’un réseau de neurones à plusieurs couches est délicat. il a ainsi fallu attendre 2006 pour que les travaux de geoﬀ hinton et d’autres, ainsi que l’amélioration de la puissance de calcul des ordinateurs, permettent de commencer à surmonter cette diﬃculté, remettant les réseaux de neurones sur le devant de la scène. \f7.2. perceptron multi-couche saturation 89 un autre problème du perceptron multi-couche apparaît quand la plupart des unités logistiques re- tournent des valeurs soit très proches de 0 soit très proches de 1. cela arrive quand la somme pondérée des signaux qu’ils reçoivent en entrée a une amplitude trop grande, autrement dit quand les poids de oh j connexion ont eux-mêmes une amplitude trop élevée. dans ces conditions, les actualisations de ces poids n’auront plus qu’un impact marginal sur les prédictions. on dit alors que le réseau est saturé. pour éviter cette situation, on peut se tourner vers la régularisation (cid:96)2 (cf. section 6.2), appelée dégrada- tion des pondérations ou weight decay dans le contexte des réseaux de neurones artiﬁciels. il s’agira d’ajouter au risque empirique l(f ((cid:126)x i), yi) la norme euclidienne du vecteur de poids de connexion. rétropropagation néanmoins, le principe fondamental de l’apprentissage d’un perceptron multi-couche, connu sous le nom de rétropropagation ou backpropagation (souvent raccourci en backprop), est connu depuis des décennies. il repose, comme pour le perceptron sans couche intermédiaire, sur l’utilisation de l’algorithme du gradient pour minimiser, à chaque nouvelle observation, le risque l(f ((cid:126)x i), yi). pour actualiser le poids de connexion wh jq du neurone j de la couche h−1 vers le neurone q de la couche h, nous devons donc calculer ∂l(f ((cid:126)x i,yi)) . pour ce faire, nous pouvons appliquer le théorème de dérivation des fonctions composées (chain rule en anglais). nous notons oh la combinaison linéaire des entrées du j- j j ). par convention, nous considérerons que ème neurone de la couche h ; en d’autres termes, zh j = ah(oh j = xj. ainsi, z0 ∂wh jq ∂l(f ((cid:126)x i), yi) ∂wh jq = = = (cid:32)ph+1 (cid:88) r=1 (cid:32)ph+1 (cid:88) r=1 ∂l(f ((cid:126)x i), yi) ∂oh q ∂oh q ∂wh jq ∂l(f ((cid:126)x i), yi) ∂oh+1 r ∂zh q ∂oh q ∂oh q ∂wh jq = ∂l(f ((cid:126)x i), yi) ∂zh q (cid:33) ∂zh q ∂oh q ∂oh+1 r ∂zh q ∂oh q ∂wh jq (7.8) ∂l(f ((cid:126)x i), yi) ∂oh+1 r (cid:33) wh+1 qr h(oh a(cid:48) q ) zh−1 j . ainsi, le gradient nécessaire à l’actualisation des poids de la couche h se calcule en fonction des gra- dients ∂l(f ((cid:126)x i,yi)) ∂oh+1 r nécessaires pour actualiser les poids de la couche (h + 1). cela va nous permettre de simpliﬁer nos calculs en utilisant une technique de mémoïsation, c’est-à-dire en évitant de recalculer des termes qui reviennent plusieurs fois dans notre procédure. plus précisément, l’entraînement d’un perceptron multi-couche par rétropropagation consiste à alterner, pour chaque observation ((cid:126)x i, yi) traitée, une phase de propagation avant qui permet de calculer les sorties de chaque neurone, et une phase de rétropropagation des erreurs dans laquelle on actualise les poids en partant de ceux allant de la dernière couche intermédiaire vers l’unité de sortie et en « remontant »le réseau vers les poids allant de l’entrée vers la première couche intermédiaire. reprenons le réseau à deux couches intermédiaires décrit sur la ﬁgure 7.5, en utilisant l’identité comme dernière fonction d’activation a3, une fonction d’erreur quadratique, et des activations logistiques pour a1 et a2. nous rappelons que la dérivée de la fonction logistique peut s’écrire σ(cid:48)(u) = u(cid:48)σ(u)(1 − σ(u)). exemple \f90 chapitre 7. réseaux de neurones artiﬁciels lors de la propagation avant, nous allons eﬀectuer les calculs suivants : o1 q = o2 q = o3 = p (cid:88) j=0 p1 (cid:88) j=1 p2 (cid:88) j=1 w1 jqxj ; z1 q = σ(o1 q) w2 jqz1 j ; z2 q = σ(o2 q) w3 j z2 j ; f ((cid:126)x i) = z3 = o3. lors de la rétropropagation, nous calculons tout d’abord ∂l(f ((cid:126)x i), yi) ∂w3 j = (cid:0)f ((cid:126)x i) − yi(cid:1) ∂f ((cid:126)x i) ∂w3 j = (cid:0)f ((cid:126)x i) − yi(cid:1) z2 j en utilisant les valeurs de f ((cid:126)x i) et z2 j que nous avons mémorisées lors de la propagation avant. ainsi w3 j ← w3 j − η (cid:0)f ((cid:126)x i) − yi(cid:1) z2 j . nous pouvons ensuite appliquer 7.8 et calculer ∂l(f ((cid:126)x i), yi) ∂w2 jq = ∂l(f ((cid:126)x i), yi) ∂o2 q ∂o2 q ∂w2 jq où ∂l(f ((cid:126)x i), yi) ∂o2 q = ∂l(f ((cid:126)x i), yi) ∂f ((cid:126)x i) w3 q σ(cid:48)(o2 q) = (cid:0)f ((cid:126)x i) − yi(cid:1) w3 q z2 q (1 − z2 q ) (7.9) et ∂o2 q ∂w2 jq nous pouvons donc utiliser les valeurs de f ((cid:126)x i), z2 q par que nous venons d’actualiser, pour actualiser w2 jq = z1 j . et z1 j mémorisées lors de la propagation avant, et w3 q jq − η (cid:0)f ((cid:126)x i) − yi(cid:1) w3 enﬁn, nous pouvons de nouveau appliquer 7.8 et calculer jq ← w2 w2 q z2 q (1 − z2 q ) z1 j . ∂l(f ((cid:126)x i), yi) ∂w1 jq = (cid:32) p2 (cid:88) r=1 ∂l(f ((cid:126)x i), yi) ∂o2 r (cid:33) w2 qr q (1 − z1 z1 q ) xj. encore une fois, nous disposons de tous les éléments nécessaires : z1 q avant, les poids w2 qr aussi été calculées à l’étape précédente (7.9). nous pouvons donc eﬀectuer aisément notre dernière étape de rétropropagation : ont été actualisés à l’étape précédente, et les dérivées partielles ∂l(f ((cid:126)x i),yi) a été calculé lors de la propagation ont elles ∂o2 q w1 jq ← w1 jq − η (cid:32) p2 (cid:88) r=1 ∂l(f ((cid:126)x i), yi) ∂o2 r (cid:33) w2 qr q (1 − z1 z1 q ) xj. \f7.2. perceptron multi-couche 91 il est possible d’ajouter une unité de biais à chaque couche intermédiaire ; les dérivations se font alors sur le même principe. remarque 7.2.5 et le deep learning dans tout ça ? de l’analyse d’images à la voiture autonome en passant par la reconnaissance vocale, la plupart des avancées récentes mises en avant par la presse grand public dans le domaine de l’apprentissage automa- tique reposent sur les réseaux de neurones profonds, ou deep neural nets. rentrer en détail dans ce domaine dépasse le cadre de cet ouvrage introductif, ainsi nous n’en abordons ici que les grandes lignes. le domaine de l’apprentissage profond repose fondamentalement sur les principes que nous venons de voir. en eﬀet, un perceptron multi-couche est profond dès lors qu’il contient suﬃsamment de couches – la déﬁnition de « suﬃsamment » dépendant des auteurs. le domaine de l’apprentissage profond s’intéresse aussi à de nombreuses autres architectures comme les réseaux récurrents (rnn, pour recursive neural nets), et en particulier long short-term memory (lstm) networks pour modéliser des données séquentielles (telles que du texte ou des données temporelles) et les réseaux convolutionnels (cnn, pour convolutional neural nets) ou les réseaux de capsules (capsnets) pour le traitement d’images. dans tous les cas, il s’agit essentiellement — d’utiliser ces architectures pour créer des modèles paramétriques (potentiellement très complexes) ; — d’en apprendre les poids par un algorithme à directions de descente. les déﬁs du deep learning l’apprentissage des poids de connexion d’un réseau de neurones profond pose cependant des diﬃcul- tés techniques : en eﬀet, le problème d’optimisation à résoudre n’est pas convexe, et converger vers un « bon » minimum local n’est pas une tâche facile. cette tâche est d’autant plus diﬃcile que le réseau est complexe, et les progrès dans ce domaine ne sont possibles que grâce au développement de méthodes pour la rendre plus aisée. de plus, les réseaux de neurones profonds ont de nombreux paramètres, et requièrent donc l’utilisation de grands volumes de données pour éviter le sur-apprentissage. il est donc généralement nécessaire de les déployer sur des architectures distribuées. ainsi, malgré son succès dans certains domaines d’application, le deep learning est délicat à mettre en place et n’est pas toujours la meilleure solution pour résoudre un problème de machine learning, en particulier face à un petit jeu de données. l’apprentissage de représentations on associe souvent aux réseaux de neurones profond la notion de representation learning. en eﬀet, il est possible de considérer que chacune des couches intermédiaires successives apprend une nouvelle repré- sentation des données (zh ) à partir de la représentation de la couche précédente, et ce jusqu’à pouvoir appliquer entre la dernière couche intermédiaire et la sortie du réseau un algorithme linéaire. cet aspect est exploité en particulier dans les auto-encodeurs que nous verrons à la section 11.3.3 2 , . . . , zh ph 1 , zh points clefs — le perceptron permet d’apprendre des modèles paramétriques linéaires et est entraîné par des ac- tualisations itératives de ses poids grâce à un algorithme à directions de descente. \f92 chapitre 7. réseaux de neurones artiﬁciels — le perceptron multi-couche permet d’apprendre des modèles paramétriques non linéaires et oﬀre une grande ﬂexibilité de modélisation. — le perceptron multi-couche est entraîné par rétropropagation, qui combine le théorème de dériva- tion des fonctions composées avec une mémoïsation pour une grande eﬃcacité de calcul. — le problème d’optimisation du perceptron multi-couche et, plus généralement, de tout réseau de neurones artiﬁciel profond, n’est pas convexe, et il n’est pas facile d’obtenir un bon minimum. pour aller plus loin • nous n’avons dans ce chapitre qu’esquissé les briques de bases du deep learning. pour aller plus loin, nous recommandons la lecture de l’ouvrage de goodfellow et al. (2016). • http://playground.tensorflow.org/ permet de jouer avec l’architecture et l’entraînement d’un réseau de neurones profond. bibliographie cybenko, g. (1989). approximation by superpositions of a sigmoidal function. mathematics of control, signals and systems, 2(4) :303–314. goodfellow, i., bengio, y., and courville, a. (2016). deep learning. mit press, cambridge, ma. http://www. deeplearningbook.org/. hornik, k. (1991). approximation capabilities of multilayer feedforward networks. neural networks, 4(2) :251–257. minsky, m. and papert, s. (1972). perceptrons : an introduction to computational geometry. mit press, cam- bridge, ma. novikoﬀ, a. b. j. (1962). on convergence proofs on perceptrons. in symposium on the mathematical theory of automata, pages 615–622. rosenblatt, f. (1957). the perceptron–a perceiving and recognizing automaton. technical report 85-460-1, cornell aeronautical laboratory. \fchapitre 8 méthode des plus proches voisins ce chapitre présente un algorithme de prédiction non paramétrique conceptuellement très simple mais néanmoins puissant, qui permet de construire des frontières de décision complexes sans faire aucune hy- pothèse sur la distribution des données. cet algorithme, dit des plus proches voisins, se base sur le principe de « qui se ressemble s’assemble », et utilise les étiquettes des exemples les plus proches pour prendre une décision. la puissance de cet algorithme venant du choix de la fonction utilisée pour déﬁnir cette proximité entre observations, ce chapitre sera aussi l’occasion de déﬁnir des distances et similarités pour diﬀérentes représentations des données. dans ce chapitre, nous étudierons aussi des cas où la représentation par un vecteur réel de dimension p n’est pas nécessairement la plus appropriée. enﬁn, nous montrerons comment les plus proches voisins peuvent être utilisés en pratique dans le cadre du ﬁltrage collaboratif. objectifs — implémenter l’algorithme des k plus proches voisins — calculer distances et similarités pour diﬀérents types de représentations des données — déﬁnir la frontière de décision de l’algorithme du plus proches voisins — expliquer pourquoi l’algorithme des k plus proches voisins est susceptible de ne pas marcher en haute dimension. 8.1 méthode du plus proche voisin 8.1.1 méthode déﬁnition 8.1 (algorithme du plus proche voisin) étant donné un jeu d = {((cid:126)x i, yi)i=1,...,n} de n observations étiquetées, et une distance d sur x , on appelle algorithme du plus proche voisin l’algorithme consistant à étiqueter une nouvelle observation (cid:126)x par l’étiquette du point du jeu d’entraînement qui en est la plus proche : f ((cid:126)x) = yarg mini=1,...,n d((cid:126)x,(cid:126)x i). 93 (cid:4) \f94 chapitre 8. méthode des plus proches voisins cet algorithme s’applique aussi bien à un problème de classiﬁcation qu’à un problème de régression. 8.1.2 diagramme de voronoï l’algorithme du plus proche voisin crée une partition de x en n parties : la i-ème de ces parties est l’ensemble des points de x dont le plus proche voisin dans d est (cid:126)x i. n (cid:91) x = {(cid:126)x ∈ x : d((cid:126)x, (cid:126)x i) ≤ d((cid:126)x, (cid:126)x l) ∀(cid:126)x l ∈ d} i=1 cette partition est ce que l’on appelle un diagramme de voronoï. déﬁnition 8.2 (diagramme de voronoï) soient un espace métrique x , muni de la distance d : x × x → r, et un ensemble ﬁni s d’éléments de x . étant donné un élément (cid:126)u ∈ s (appelé germe), on appelle cellule de voronoï l’ensemble déﬁni par vor((cid:126)u) = {(cid:126)x ∈ x : ∀ (cid:126)v ∈ s, d((cid:126)x, (cid:126)u) ≤ d((cid:126)x, (cid:126)v)}. une cellule de voronoï est un ensemble convexe, et l’union de toutes les cellules de voronoï forme une partition de x appelée diagramme de voronoï : x = (cid:91) (cid:126)u∈s vor((cid:126)u). (cid:4) la ﬁgure 8.1 présente un exemple de diagramme de voronoï, calculé dans le plan en utilisant la distance euclidienne. les cellules sont des polygones convexes. tous les points à l’intérieur d’une même cellule ont la même étiquette que leur graine. figure 8.1 – diagramme de voronoï pour 25 points en deux dimensions, en utilisant la distance euclidienne. une autre façon de présenter l’algorithme du plus proche voisin est de le voir comme une partition de x en autant de parties qu’il existe d’étiquettes distinctes dans le jeu d’entraînement. la partie correspondant à l’étiquette y est l’union des cellules de voronoï correspondant à une observation de d étiquetée par y. x = (cid:91) y {(cid:126)x ∈ x : yarg mini=1,...,n d((cid:126)x,(cid:126)x i) = y} \f8.2. méthode des plus proches voisins 95 ainsi, cet algorithme très simple permet de construire une frontière de décision beaucoup plus com- plexe qu’un algorithme paramétrique linéaire, comme on le voit sur la ﬁgure 8.2. figure 8.2 – frontière de décision d’un algorithme du plus proche voisin pour les données de la ﬁgure 8.1, maintenant étiquetées. 8.2 méthode des plus proches voisins la méthode du plus proche voisin a le défaut d’être très sensible au bruit : si une observation est mal étiquetée, ou (ce qui est assez probable en raison d’une part du bruit de mesure et d’autre part de la nature vraisemblablement incomplète de notre représentation) mal positionnée, tous les points dans sa cellule de voronoï seront mal étiquetés. pour rendre cette méthode plus robuste, on se propose de combiner les « opinions » de plusieurs voisins de l’observation que l’on cherche à étiqueter. 8.2.1 méthode des k plus proches voisins déﬁnition 8.3 (algorithme des k plus proches voisins) étant donné un jeu d = {((cid:126)x i, yi)i=1,...,n} de n observations étiquetées, une distance d sur x , et un hyperparamètre k ∈ n∗, on appelle algorithme des k plus proches voisins, ou knn pour k nearest neighbors en anglais, l’algorithme consistant à étiqueter une nouvelle observation (cid:126)x en fonction des étiquettes des k points du jeu d’entraînement dont elle est la plus proche. en notant nk((cid:126)x) l’ensemble des k plus proches voisins de (cid:126)x dans d : — pour un problème de classiﬁcation, on applique le vote de la majorité, et (cid:126)x prend l’étiquette majoritaire parmi celles de ses k plus proches voisins : f ((cid:126)x) = arg max (cid:88) δ(yi, c). c i:(cid:126)x i∈nk((cid:126)x) — pour un problème de régression, (cid:126)x prend comme étiquette la moyenne des étiquettes de ses k plus proches voisins : f ((cid:126)x) = 1 k (cid:88) yi. i:(cid:126)x i∈nk((cid:126)x) (cid:4) \f96 chapitre 8. méthode des plus proches voisins dans le cas ou k = 1, on retrouve l’algorithme du plus proche voisin. l’algorithme des k plus proches voisins est un exemple d’apprentissage non paramétrique : la fonction de décision s’exprime en fonction des données observées et non pas comme une formule analytique fonction des variables. on peut rapprocher son fonctionnement de celui d’un raisonnement par cas, qui consiste à agir en se remémorant les choix déjà eﬀectuées dans des situations semblables précédemment rencontrées, qui se retrouve par exemple lorsqu’un médecin traite un patient en se remémorant comment d’autres patients avec des symptômes similaires ont guéri. la frontière de décision de l’algorithme des k plus proches voisins est linéaire par morceaux ; quand k augmente, elle devient plus « simple », car le vote de la majorité permet de lisser les aspérités créées par les exemples individuels, comme on le voit sur la ﬁgure 8.3. figure 8.3 – frontière de décision d’un algorithme des 5 plus proches voisins pour les données des ﬁ- gures 8.1 et 8.2. 8.2.2 apprentissage paresseux on parle parfois d’apprentissage paresseux, ou lazy learning en anglais, pour qualiﬁer l’algorithme des k plus proches voisins. en eﬀet, la procédure d’apprentissage consiste uniquement à stocker les données du jeu d’entraînement et ne comporte aucun calcul. attention, cela peut être un facteur limitant, au niveau de la mémoire, si le jeu d’entraînement est très grand. à l’inverse, la procédure de prédiction requiert de calculer la distance de l’observation à étiqueter à chaque observation du jeu d’entraînement, ce qui peut être intensif en temps de calcul si ce jeu d’entraî- nement est très grand. une prédiction requiert en eﬀet de calculer n distances, une opération d’une com- plexité de l’ordre de o(np) en p dimensions, puis de trouver les k plus petites de ces distances, une opéra- tion d’une complexité en o(n log k). 8.2.3 nombre de plus proches voisins comme nous l’avons décrit ci-dessus, nous proposons d’utiliser k > 1 pour rendre l’algorithme des k plus proches voisins plus robuste au bruit. cependant, à l’inverse, si k = n, l’algorithme prédira, dans le cas d’un problème de classiﬁcation, la classe majoritaire dans d, et dans le cas d’un problème de régression, la moyenne des étiquettes de d, ce qui paraît tout aussi peu satisfaisant. \f8.2. méthode des plus proches voisins 97 il faudra donc choisir une valeur de k intermédiaire, ce que l’on fera généralement en utilisant une √ n est aussi parfois utilisée. validation croisée. l’heuristique k ≈ 8.2.4 variantes (cid:15)-voisins plutôt que de considérer un nombre ﬁxe de voisins les plus proches, on peut préférer considérer tous les exemples d’apprentissage suﬃsamment proches de l’observation à étiqueter : cela permet de mieux utiliser le jeu d’entraînement dans les zones où il est dense. de plus, les prédictions faites en se basant sur des exemples proches (non pas relativement, mais dans l’absolu) sont intuitivement plus ﬁables que celles faites en se basant sur des exemples éloignés. déﬁnition 8.4 (algorithme des (cid:15)-voisins) étant donné un jeu d = {((cid:126)x i, yi)i=1,...,n} de n observa- tions étiquetées, une distance d sur x , et un hyperparamètre (cid:15) ∈ r+, on appelle algorithme des (cid:15)-voisins, ou (cid:15)-ball neighbors en anglais, l’algorithme consistant à étiqueter une nouvelle observation (cid:126)x en fonction des étiquettes de tous les points du jeu d’entraînement situés à une distance inférieure à (cid:15) de (cid:126)x. — pour un problème de classiﬁcation : f ((cid:126)x) = arg max (cid:88) δ(yi, c). c i:d((cid:126)x,(cid:126)x i)≤(cid:15) — pour un problème de régression : f ((cid:126)x) = 1 |{i : d((cid:126)x, (cid:126)x i) ≤ (cid:15)}| (cid:88) yi. i:d((cid:126)x,(cid:126)x i)≤(cid:15) une limitation évidente de cet algorithme est qu’il est nécessaire de déﬁnir une stratégie alternative en l’absence d’exemples dans la boule de rayon (cid:15) choisi. pondération des voisins alternativement, pour prendre en compte la notion que les voisins véritablement proches sont plus ﬁables pour la prédiction que ceux plus éloignés, on peut pondérer la contribution de chacun des voisins en fonction de sa distance à l’observation à étiqueter, typiquement par (cid:4) wi = 1 d((cid:126)x, (cid:126)x i) ou wi = e−( 1 2 d((cid:126)x,(cid:126)x i)). dans le cas d’un problème de classiﬁcation, on compare, pour chaque classe c, la somme des contributions de chaque voisin appartenant à cette classe : f ((cid:126)x) = arg max (cid:88) c i:xi∈nk((cid:126)x) δ(yi, c)wi. dans le cas d’un problème de régression, il s’agit simplement de pondérer la moyenne : f ((cid:126)x) = 1 k (cid:88) wiyi. (cid:126)x i∈nk((cid:126)x) \f98 chapitre 8. méthode des plus proches voisins 8.3 distances et similarités l’ingrédient essentiel de l’algorithme des k plus proches voisin est la distance permettant de déterminer quelles sont les observations du jeu d’entraînement les plus proches du point à étiqueter. attention l’algorithme des k plus proches voisins est sensible aux attributs non pertinents ; en eﬀet, ceux-ci se- ront pris en compte dans le calcul de la distance et pourront la biaiser. de plus, en haute dimension, cet algorithme souﬀre du ﬂéau de la dimension (cf. 11.1.3) : tous les exemples seront loin de l’observation que l’on cherche à étiqueter, et l’intuition selon laquelle on peut utiliser les étiquettes des exemples proches pour faire une prédiction ne fonctionnera plus. 8.3.1 distances rappelons ici la déﬁnition d’une distance : déﬁnition 8.5 (distance) étant donné un ensemble x , on appelle distance sur x toute fonction d : x × x → r vériﬁant les trois propriétés suivantes : 1. séparation : ∀(cid:126)u, (cid:126)v ∈ x × x , d((cid:126)u, (cid:126)v) = 0 ⇔ (cid:126)u = (cid:126)v 2. symétrie : ∀(cid:126)u, (cid:126)v ∈ x × x , d((cid:126)u, (cid:126)v) = d((cid:126)v, (cid:126)u) 3. inégalité triangulaire : ∀(cid:126)t, (cid:126)u, (cid:126)v ∈ x 3, d((cid:126)u, (cid:126)v) ≤ d((cid:126)u, (cid:126)t) + d((cid:126)t, (cid:126)v). (cid:4) dans le cas où x = rp, on utilisera le plus souvent une distance de minkowski : déﬁnition 8.6 (distance de minkowski) étant donné q ﬁxé, q ≥ 1, on appelle distance de minkowski la distance déﬁnie par dq : rp × rp → r (cid:126)u, (cid:126)v (cid:55)→ ||(cid:126)u − (cid:126)v||q =   p (cid:88) |uj − vj|q  1/q  . j=1 la distance euclidienne est le cas particulier q = 2 : d2((cid:126)u, (cid:126)v) = quand q = 1, on parle de distance de manhattan, ou distance du chauﬀeur de taxi, car dans le plan elle consiste à calculer la distance parcourue entre (cid:126)u et (cid:126)v en se déplaçant uniquement parallèlement aux axes, comme ce serait le cas d’un taxi dans manhattan, où les rues forment un quadrillage : d1((cid:126)u, (cid:126)v) = (cid:80)p j=1 |uj− vj|. j=1(uj − vj)2. enﬁn, la distance (cid:96)∞, ainsi nommée en référence à l’espace de suites (cid:96)∞, et notée d∞, est la diﬀérence (cid:113)(cid:80)p maximale entre (cid:126)u et (cid:126)v sur une dimension : on l’appelle aussi distance de tchebychev. d∞((cid:126)u, (cid:126)v) = max j=1,...,p |uj − vj|. théorème 8.1 la distance (cid:96)∞ est la limite quand q → ∞ de la distance de minkowski. (cid:4) (cid:4) \f8.3. distances et similarités 99 démonstration. posons k = arg maxj=1,...,p |uj − vj|. alors — soit |uk − vk| = 0, et donc |uj − vj| = 0 ∀j, auquel cas la distance de minkowski entre (cid:126)u et (cid:126)v vaut 0 quel que soit q et donc limq→∞ dq((cid:126)u, (cid:126)v) = 0 = d∞((cid:126)u, (cid:126)v); — soit |uk − vk| > 0, et on peut écrire dq((cid:126)u, (cid:126)v) = |uk − vk|   p (cid:88) j=1 (cid:18) |uj − vj| |uk − vk|  1/q (cid:19)q  ≤ |uj − vj|p1/q car pour tout j, |uj − vj| ≤ |uk − vk|, avec égalité au maximum p fois. d’autre part, dq((cid:126)u, (cid:126)v) =  |uk − vk|q + (cid:88) j(cid:54)=k  1/q |uj − vj|q  ≥ |uk − vk| puisque |uj − vj|q > 0 pour tout j. ainsi |uk − vk| ≤ dq((cid:126)u, (cid:126)v) ≤ |uj − vj|p1/q. comme limq→∞ p1/q = 1, limq→∞ dq((cid:126)u, (cid:126)v) = |uk − vk|. (cid:3) 8.3.2 similarités entre vecteurs réels cependant, il n’est pas nécessaire pour appliquer l’algorithme des k plus proches voisins d’utiliser une distance : une notion de similarité est suﬃsante. déﬁnition 8.7 (similarité) on appelle similarité sur x toute fonction s : x ×x (cid:55)→ r+ qui est d’autant (cid:4) plus grande que les deux éléments de x qu’elle compare sont semblables. contrairement à une distance, une similarité n’a pas de propriétés mathématiques particulières. il est possible de transformer n’importe quelle distance en similarité, en utilisant par exemple la transformation s((cid:126)u, (cid:126)v) = −d((cid:126)u, (cid:126)v), ou s((cid:126)u, (cid:126)v) = 1 1+d((cid:126)u,(cid:126)v) . une notion de similarité fréquemment utilisée lorsque x = rp est celle du coeﬃcient de corrélation. déﬁnition 8.8 (coefﬁcient de corrélation) étant donnés (cid:126)u, (cid:126)v ∈ rp, on appelle coeﬃcient de corrélation ou corrélation de pearson entre (cid:126)u et (cid:126)v la valeur ρ((cid:126)u, (cid:126)v) = (cid:16) (cid:80)p j=1 uj − 1 p (cid:80)p (cid:17) (cid:16) vj − 1 p (cid:17) (cid:80)p k=1 vk (cid:114) (cid:80)p j=1 (cid:16) uj − 1 p (cid:80)p k=1 uk (cid:80)p j=1 (cid:16) vj − 1 p (cid:80)p k=1 vk k=1 uk (cid:17)2(cid:114) . (cid:17)2 (cid:4) remarquons que si l’on appelle (cid:126)u(cid:48) (resp. (cid:126)v(cid:48)) la version centrée de (cid:126)u (resp. (cid:126)v), c’est-à-dire le vecteur j=1 uj, ce coeﬃcient se simpliﬁe obtenu en lui retranchant la moyenne de ses coordonnées : (cid:126)u(cid:48) = (cid:126)u − 1 p en (cid:80)p ρ((cid:126)u, (cid:126)v) = (cid:80)p j=1 u(cid:48) jv(cid:48) j 2(cid:113)(cid:80)p j=1 v(cid:48) j (cid:113)(cid:80)p j=1 u(cid:48) j = 2 (cid:104)(cid:126)u(cid:48), (cid:126)v(cid:48)(cid:105) ||(cid:126)u(cid:48)||2 ||(cid:126)v(cid:48)||2 \f100 chapitre 8. méthode des plus proches voisins et vaut donc le cosinus de l’angle entre (cid:126)u(cid:48) et (cid:126)v(cid:48). c’est pour cette raison qu’on appelle parfois ρ la similarité cosinus. si en plus d’être centrés, les vecteurs (cid:126)u et (cid:126)v sont normalisés, la similarité cosinus se réduit au produit scalaire entre (cid:126)u et (cid:126)v. la valeur absolue de ce produit scalaire est d’autant plus grande que les vecteurs (cid:126)u et (cid:126)v sont colinéaires, et vaut 0 quand ces vecteurs sont orthogonaux. nous verrons au chapitre 10 comment étendre la notion de produit scalaire sur rp à celle de noyau sur x , ce qui nous permettra de déﬁnir aisément d’autres mesures de similarité. remarque en général, la similarité cosinus et la distance euclidienne ne déﬁnissent pas les mêmes voisinages. 8.3.3 similarités entre ensembles jusqu’à présent dans cet ouvrage, nous avons uniquement traité de données qui peuvent être repré- sentées par un vecteur réel. cependant, il arrive souvent que nos observations soient plus aisément re- présentées comme des sous-ensembles d’un même ensemble ﬁni d’éléments. par exemple, une chaîne de caractères peut être représentée par l’ensemble des lettres qu’elle contient, ou un document texte par l’en- semble des mots qu’il contient. ces représentations peuvent être transformées en vecteurs binaires : un sous-ensemble s de e peut être représenté comme un vecteur binaire de taille |e| dont chaque bit correspond à un élément e de e et vaut 1 si e ∈ s et 0 sinon. on peut alors appliquer les distances ou similarités précédentes à ces représentations vectorielles. cependant, on peut aussi déﬁnir des distances ou des similarités directement sur ces ensembles, sans passer par une représentation vectorielle dont la dimension pourrait être très grande (autant que le nombre de mots dans le dictionnaire, dans le cas de la représentation d’un document par les mots qu’il contient), bien que potentiellement parcimonieuse (la plupart des documents contiendront peu de mots, relative- ment au dictionnaire). on peut choisir de comparer deux ensembles en fonction du nombre d’éléments qui apparaissent dans un seul d’entre eux. déﬁnition 8.9 (distance de hamming) la distance de hamming entre deux sous-ensembles s et t de e est déﬁnie comme h(s, t ) = |s (cid:52) t |, où s (cid:52) t = {e ∈ s \\ t } ∪ {e ∈ t \\ s} est la diﬀérence symétrique entre s et t . en utilisant une représentation binaire de s et t , la distance de hamming est le nombre de bits diﬀé- (cid:4) rents entre ces deux vecteurs, et est équivalente à leur distance de manhattan. deux ensembles sont considérés être d’autant plus semblables qu’ils ont d’éléments en commun. at- tention, si l’on compare des ensembles susceptibles d’être de tailles très diﬀérentes, cela doit être comparé au nombre d’éléments qu’ils pourraient avoir en commun : deux ensembles de grande taille auront naturel- lement plus d’éléments communs que deux ensembles de petite taille. on utilisera pour cela la similarité de jaccard. déﬁnition 8.10 (similarité de jaccard/tanimoto) étant donné un ensemble e d’éléments, on appelle similarité de jaccard, similarité de tanimoto, ou encore index de jaccard la fonction j : 2e × 2e → [0, 1] s, t (cid:55)→ |s ∩ t | |s ∪ t | . \f8.3. distances et similarités 101 ici 2e désigne l’ensemble des parties de e, autrement dit l’ensemble de ses sous-ensembles. cette notation est choisie car, du moins dans le cas ﬁni, un sous-ensemble s ⊆ e peut être représenté comme une fonction (cid:4) binaire de e vers {0, 1} qui associe 1 à un élément de e présent dans s, et 0 aux autres. si les éléments sont susceptibles d’apparaître plusieurs fois dans chaque « ensemble » (il ne s’agira alors plus d’ensembles, mais de multi-ensembles), on peut prendre en compte la multiplicité des éléments en utilisant la similarité minmax : déﬁnition 8.11 (similarité minmax) étant donné un multi-ensemble s de e, et un élément e ∈ e, on note ms(e) la multiplicité de e dans s, autrement dit son nombre d’occurrences dans s. étant donné un ensemble e d’éléments, on appelle similarité minmax entre deux multi-ensembles s et t de e la fonction qui retourne minmax(s, t ) = (cid:80) (cid:80) e∈s∩t min(ms(e), mt (e)) e∈s∪t max(ms(e), mt (e)) . (cid:4) 8.3.4 similarités entre données catégoriques un autre type de variable que l’on peut fréquemment rencontrer est celui des variables catégoriques, comme le genre ou la catégorie socio-professionnelle d’un individu, le jour de la semaine d’un événement, ou le stade d’une tumeur. ces variables sont représentées par un élément d’une liste de possibilités plutôt que par un nombre. bien que l’on puisse arbitrairement associer un nombre à chacune de ces possibilités, et représenter la variable par ce nombre, ce n’est généralement pas une bonne idée. en eﬀet, deux catégories représentées par des nombres voisins seront considérées par l’algorithme d’apprentissage comme plus proches que deux catégories représentées par des nombres plus éloignées. il n’existe pas toujours de distance très claire entre deux catégories : la profession « cadre de la fonction publique » est-elle plus proche de « artisan » ou de « employé de commerce » ? il nous est surtout possible, a priori, de distinguer deux cas : soit deux observations appartiennent à la même catégorie, soit elles appar- tiennent à deux catégories distinctes. pour pouvoir appliquer une méthode des plus proches voisins avec une distance déﬁnie par une norme (cid:96)q, ou une méthode paramétrique, à une telle variable, on utilisera souvent un encodage one-hot. déﬁnition 8.12 (encodage one-hot) étant donnée une variable catégorique pouvant prendre m va- leurs distinctes, on appelle encodage one-hot la représentation de cette variable par un vecteur binaire de dimension m, dans lequel chaque bit correspond à une des valeurs, et est mis à 1 si la variable prend cette (cid:4) valeur et à 0 sinon. supposons que nos observations soient des articles de journaux, et que nous disposions d’une variable catégorique identiﬁant leur sujet comme un parmi « politique », « société », « économie », « sport », « science » ou « culture ». l’encodage one-hot de cette variable consistera à la remplacer par 6 variables binaires, valant (1, 0, 0, 0, 0, 0) si l’article parle de politique et (0, 0, 0, 0, 0, 1) s’il parle de science. exemple une fois les données ainsi encodées, on peut leur appliquer les distances et similarités déﬁnies sur les vecteurs réels. \f102 chapitre 8. méthode des plus proches voisins remarque dans le cas des jours de la semaine (ou des mois de l’année), le problème de l’encodage par un nombre de 1 à 7 (ou de 1 à 12) est la nature cyclique de ces catégories : février (2) est certes plus proche – temporel- lement – de mai (5-2=3) que de juillet (7-2=5), mais il est encore plus proche de décembre (12-2=10). pour conserver cette proximité temporelle, on pourra positionner ces valeurs de manière équidistante sur un cercle de rayon 1, et représenter chacune d’entre elles par deux variables, le cosinus et le sinus de cette position. on représentera ainsi février par (cos( 2π 6 )) = √ 2 (− tandis que celle de février à décembre est de 1. 6 ), sin( 5π 6 ), sin( 2π 2 ), et décembre par (cos(2π), sin(2π)) = (1, 0). la distance euclidienne de février à mai est de 2 ), mai par (cos( 5π 6 )) = ( 1 2 , 2 , 1 √ √ 3 3 8.4 filtrage collaboratif le ﬁltrage collaboratif, ou collaborative ﬁltering en anglais, est à la base des systèmes de recommandation, tels que ceux utilisés par netﬂix ou amazon, qui utilisent les évaluations d’un groupe pour proposer des recommandations à un individu. il existe de nombreuses méthodes pour résoudre ce problème. néanmoins, l’une d’entre elles repose sur les principes que nous venons de voir dans ce chapitre, en faisant l’hypothèse que des utilisateurs aux goûts similaires vont continuer à aimer les mêmes choses. plus précisément, étant donnés un ensemble s d’objets (ﬁlms, livres, achats, etc.), un ensemble x d’uti- lisateurs, nous supposons l’existence d’une fonction r : x × s (cid:55)→ r telle que r(u, a) est la note donnée par l’utilisateur u à l’objet a. cette fonction est partiellement connue, au sens où tous les utilisateurs n’ont pas noté tous les objets. en notant, pour tout (a, b) ∈ s × s, u(a, b) le sous-ensemble de x contenant uniquement les utilisateurs qui ont noté à la fois a et b, et pour tout u ∈ x , ¯r(u) la note moyenne donnée par u, on peut alors déﬁnir une similarité entre objets sur la base de la similarité cosinus : s(a, b) = (cid:80) u∈u (a,b) (r(u, a) − ¯r(u)) (r(u, b) − ¯r(u)) (cid:113)(cid:80) u∈u (a,b) (r(u, a) − ¯r(u))2 (cid:80) u∈u (a,b) (r(u, b) − ¯r(u))2 . (8.1) appelons maintenant n k u (a) les k plus proches voisins de a parmi les objets notés par u. on peut re- commander l’objet a pour l’utilisateur u par : f (u, a) = points clefs (cid:80) b∈n k (cid:80) u (a) s(a, b)r(u, b) u (a)|s(a, b)| b∈n k . — l’algorithme des k plus proches voisins a un entraînement paresseux ; pour compenser, le coût al- gorithmique d’une prédiction peut être élevé si la base de données d’entraînement est grande. — la qualité des prédictions d’un algorithme des k plus proches voisins dépend principalement du choix d’une bonne distance ou similarité. — l’algorithme des k plus proches voisins fonctionne mieux en faible dimension : — son exécution est plus rapide ; — il est moins susceptible d’être biaisé par des variables qui ne sont pas pertinentes ; — il est moins susceptible de souﬀrir du ﬂéau de la dimension. \f103 pour aller plus loin • l’ouvrage de webb (1999) présente un éventail de distances qui peuvent être utilisées dans le cadre de l’algorithme des k plus proches voisins. • des structures de données telles que les arbres kd (bentley, 1957) ou l’utilisation de tables de hachage permettent de stocker le jeu d’entraînement de sorte à retrouver plus facilement les plus proches voisins. l’idée sous-jacente de ces structures est de regrouper les exemples qui sont proches les uns des autres. • des ouvrages entiers ont été écrits sur le sujet des systèmes de recommandation. on pourra notam- ment se référer à ceux de ricci et al. (2016) ou aggarwal (2016). • un ensemble d’articles sur les k plus proches voisins a été rassemblé dans aha (1997). • l’article de hechenbichler et schliep (2006) fournit une bonne revue des méthodes de pondération des plus proches voisins. bibliographie aggarwal, c. c. (2016). recommender systems. springer. aha, d. w. (1997). special issue on lazy learning. artiﬁcial intelligence review, 11(1–5) :7–423. bentley, j. l. (1957). multidimensional binary search trees used for associative searching. communications of the acm, 18(9) :509–517. hechenbichler, k. et schliep, k. (2006). weighted k-nearest-neighbor techniques and ordinal classiﬁcation. in sfb 386. discussion paper 399. ricci, f., rokach, l., et shapira, b. (2016). recommender systems handbook. springer, 2 edition. webb, a. (1999). statistical pattern recognition. arnold, london. \fchapitre 9 arbres et forêts l’algorithme des plus proches voisins permet de construire des modèles non paramétriques métriques, c’est-à-dire qu’ils reposent sur la déﬁnition d’une distance ou similarité pertinente entre les observations. cela n’est pas toujours chose aisée, et les arbres de décision approchent le problème diﬀéremment. non mé- triques, hiérarchiques, et non paramétriques, ils présentent d’intéressantes propriétés, en particulier en ce qui concerne les attributs discrets. cependant, ils ont tendance à mal apprendre, et à avoir de faibles pro- priétés de généralisation. dans ce chapitre, après avoir exploré plus en détail les propriétés des arbres et leur construction, nous verrons comment les combiner pour en faire des modèles puissants, dits modèles ensemblistes, qui sont bien plus que la somme de leurs parties. objectifs — détailler les avantages et inconvénients d’un arbre de décision — construire un arbre de décision — savoir combiner des apprenants faibles parallèlement ou séquentiellement et en comprendre l’in- térêt 9.1 arbres de décision 9.1.1 apprentissage hiérarchique les modèles que nous avons vus jusqu’à présent procèdent en une seule opération pour étiqueter une observation (cid:126)x. de tels modèles utilisent le même ensemble de variables pour toutes les classes, et leur accordent la même importance pour toutes les observations. ces modèles ne sont pas bien adaptés aux cas où les classes ont une distribution multi-modale, c’est-à-dire qu’elles sont déterminées par diﬀérentes variables dans diﬀérentes régions de l’espace, ou quand les variables sont discrètes. par contraste, les arbres de décision sont des modèles hiérarchiques, qui se comportent comme une série successive de tests conditionnels, dans laquelle chaque test dépend de ses antécédents. ils sont couram- ment utilisés en dehors du monde du machine learning, par exemple pour décrire les étapes d’un diagnos- tic ou d’un choix de traitement pour un médecin, ou les chemins possibles dans un « livre dont vous êtes le héros ». la ﬁgure 9.1 présente un tel arbre de décision. 104 \f9.1. arbres de décision 105 figure 9.1 – exemple d’arbre de décision pour étiqueter un fruit. la ﬁgure 9.1 nous permet d’illustrer trois propriétés des arbres de décision : — ils permettent de traiter des attributs discrets (comme ici la forme, la taille et la couleur), sans né- cessiter une notion de similarité ou d’ordre sur ces attributs (on parle d’apprentissage non métrique) ; — ils permettent de traiter un problème de classiﬁcation multi-classe sans passer par des classiﬁca- tions binaires ; — ils permettent de traiter des classes multi-modales (comme ici pour l’étiquette « pomme », qui est aﬀectée à un fruit grand et rouge ou à un fruit jaune et rond.) déﬁnition 9.1 (arbre de décision) on appelle arbre de décision un modèle de prédiction qui peut être représenté sous la forme d’un arbre. chaque nœud de l’arbre teste une condition sur une variable et chacun de ses enfants correspond à une réponse possible à cette condition. les feuilles de l’arbre correspondent à une étiquette. pour prédire l’étiquette d’une observation, on « suit » les réponses aux tests depuis la racine de l’arbre, (cid:4) et on retourne l’étiquette de la feuille à laquelle on arrive. 9.1.2 partition de l’espace par un arbre de décision un arbre de décision partitionne l’espace x des observations en autant de régions qu’il a de feuilles ; au sein d’une même région, toutes les observations reçoivent alors la même étiquette. dans le cas d’un problème de classiﬁcation, cette étiquette est l’étiquette majoritaire dans la région. en supposant n obser- vations (cid:126)x 1, (cid:126)x 2, . . . , (cid:126)x n de x étiquetées par y1, y2, . . . , yn, et r régions r1, r2, . . . , rr, on peut écrire f ((cid:126)x) = r (cid:88) r=1 δ(cid:126)x∈rr arg max c=1,...,c (cid:88) δ(yi, c). i:(cid:126)x i∈rr (9.1) dans ce chapitre, nous ne traiterons pas séparément du cas binaire et du cas multi-classe car le premier découle du second en étiquetant les deux classes 1 et 2 plutôt que 0 et 1 comme nous en avons l’habitude. pour un problème de régression, cette étiquette est l’étiquette moyenne des observations dans cette région : f ((cid:126)x) = r (cid:88) r=1 δ(cid:126)x∈rr 1 |rr| (cid:88) yi. i:(cid:126)x i∈rr (9.2) rondecouleur ?taille ?forme ?rougejaunepommebananeforme ?pommecerisefraisepetitegrandeincurvéerondepointuecitronovale\f106 chapitre 9. arbres et forêts ces notations sont lourdes et on préférera éviter d’essayer d’écrire la fonction de décision d’un arbre de décision sous forme analytique. cependant, il est important de retenir qu’un arbre de décision partitionne le jeu d’entraînement de manière récursive en des sous-ensembles de plus en plus petits. une telle partition est illustrée sur la ﬁgure 9.2 pour des variables réelles. figure 9.2 – l’arbre de décision (à gauche) partitionne r2 en 5 zones (à droite) 9.2 comment faire pousser un arbre 9.2.1 cart l’algorithme utilisé pour entraîner un arbre de décision est appelé cart, pour classiﬁcation and regres- sion tree (breiman et al., 1984). il s’agit d’un algorithme de partitionnement de l’espace par une approche gloutonne, récursive et divisive. cart peut être utilisé pour entraîner un arbre de décision binaire, c’est-à-dire dans lequel chaque nœud a exactement deux enfants, sans perte de généralité. en eﬀet, tout arbre peut être ré-exprimé sous la forme d’un arbre binaire, comme l’illustre la ﬁgure 9.3. comme illustré sur la ﬁgure 9.2, cart partitionne les données une variable à la fois, ce qui produit une frontière de décision orthogonale aux axes. nous supposons par la suite que les données sont déﬁnies dans un espace x de dimension p. déﬁnition 9.2 (variable séparatrice) à chaque nœud d’un arbre de décision construit par cart cor- respond une variable séparatrice (splitting variable) j ∈ {1, . . . , p} selon laquelle vont être partitionnées les données. cette variable séparatrice déﬁnit deux régions, correspondant aux enfants du nœud considéré. dans le cas où la variable de séparation est binaire, ces deux régions sont rl(j, s) = {(cid:126)x : xj = 0}; rr(j, s) = {(cid:126)x : xj = 1}. dans le cas où la variable de séparation est une variable discrète pouvant prendre plus de deux valeurs (ou modalités), elle s’accompagne alors d’un sous-ensemble de ces valeurs ⊂ dom(xj) les deux régions sont rl(j, s) = {(cid:126)x : xj ∈ }; rr(j, s) = {(cid:126)x : xj /∈ }.  x2 < v2 ? x1 < u1 ?ouinonr4r1r3r5ouinonouinonr2ouinon x1 < u2 ?x2 < v1 ?\f9.2. comment faire pousser un arbre 107 figure 9.3 – version binaire de l’arbre de décision de la ﬁgure 9.1. enﬁn, dans le cas où la variable de séparation est une variable réelle, elle s’accompagne alors d’un point de séparation (splitting point) s qui est la valeur de l’attribut par rapport à laquelle va se faire la décision. les deux régions sont alors rl(j, s) = {(cid:126)x : xj < s}; rr(j, s) = {(cid:126)x : xj ≥ s}. (cid:4) à chaque itération de l’algorithme cart, on itère sur toutes les valeurs possibles de j et, le cas échéant, toutes les valeurs possibles de s ou s pour déterminer le couple (j, s) qui minimise un critère prédéﬁni. remarque dans le cas d’une variable continue xj, si l’on suppose les valeurs prises par cette variable dans d or- , alors les valeurs possibles de s sont xi+1 pour toutes les valeurs de i j −xi j 2 données : x1 telles que xi+1 j ≤ x2 j ≤ . . . , ≤ xn j (cid:54)= xi j. j dans le cas d’un problème de régression, ce critère est généralement l’erreur quadratique moyenne : à quel point la partition en (j, s) permet-elle de regrouper des observations d’étiquettes proches ? on choisit donc la variable et le point de séparation comme arg min j,s   (cid:88) (yi − yl(j, s))2 + (cid:88) (yi − yr(j, s))2   i:(cid:126)x i∈rl(j,s) i:(cid:126)x i∈rr(j,s) où yl(j, s) (resp. yr(j, s)) est l’étiquette associée à la région rl(j, s) (resp. rr(j, s)) à ce stade, soit donc la moyenne des étiquettes de cette région. dans le cas d’un problème de classiﬁcation, on utilise plutôt que l’erreur quadratique moyenne un critère d’impureté, ainsi appelé car il quantiﬁe à quel point la région considérée est « polluée » par des éléments des classes qui n’y sont pas majoritaires. en notant imp ce critère, on choisit donc la variable et le point de séparation comme arg min j,s (cid:18) |rl(j, s)| n imp(rl(j, s)) + |rr(j, s)| n imp(rr(j, s)). (cid:19) rouge ?petit ?incurvé ?ouinonpommecitronrond ?pommecerisefraiseouinonnonouinonbananeoui rond ?nonoui\f108 chapitre 9. arbres et forêts encore une fois, il s’agit d’un algorithme glouton : il n’y a aucune garantie que cette stratégie aboutisse à l’arbre de décision dont l’impureté ou l’erreur quadratique moyenne est minimale. 9.2.2 critères d’impureté il existe plusieurs critères d’impureté, que nous détaillons dans cette section : l’erreur de classiﬁcation, l’entropie croisée et l’impureté de gini. pour les déﬁnir, nous allons utiliser la notation pc(r) pour indiquer la proportion d’exemples d’entraînement de la région r qui appartiennent à la classe c : pc(r) = 1 |r| (cid:88) δ(yi, c). i:(cid:126)x i∈r tout d’abord, l’erreur de classiﬁcation permet de déﬁnir l’impureté d’une région r comme la proportion d’exemples de cette région qui n’appartiennent pas à la classe majoritaire. imp(r) = 1 − max c=1,...,c pc(r). (9.3) si tous les exemples d’une région appartiennent à la même classe, l’erreur de classiﬁcation de cette région vaut 0 ; à l’inverse, si une région contient autant d’exemples de chacune des c classes, pc(r) ≈ 1 quelle c que soit la classe c, et l’erreur de classiﬁcation vaut 1 − 1 c dans le cas d’une classiﬁcation binaire. ensuite, l’entropie croisée permet de déﬁnir l’impureté d’une région r de sorte à choisir la séparation qui maximise le gain d’information : le but de la construction est alors de minimiser la quantité d’information supplémentaire nécessaire pour étiqueter correctement les exemples d’entraînement de r. , soit 1 2 imp(r) = − c (cid:88) c=1 pc(r) log2 pc(r). (9.4) si tous les exemples d’une région appartiennent à la même classe, l’entropie croisée de cette région vaut 0 ; à l’inverse, si une région contient autant d’exemples de chacune des c classes, l’entropie croisée vaut log2(c), soit 1 dans le cas d’une classiﬁcation binaire. enﬁn, la déﬁnition la plus utilisée de l’impureté est l’impureté de gini, qui permet de quantiﬁer la proba- bilité qu’un exemple du jeu d’entraînement soit mal étiqueté s’il était étiqueté aléatoirement en fonction de la distribution des étiquettes dans r. déﬁnition 9.3 (impureté de gini) l’impureté de gini d’une région r est déﬁnie comme imp(r) = c (cid:88) c=1 pc(r) (1 − pc(r)) . (cid:4) si tous les exemples d’une région appartiennent à la même classe, l’impureté de gini de cette région vaut 0 ; à l’inverse, si une région contient autant d’exemples de chacune des c classes, l’impureté de gini vaut 1 − 1 c dans le cas d’une classiﬁcation binaire. , soit 1 2 9.2.3 élaguer un arbre nous avons établi une stratégie pour construire un arbre. il nous faut maintenant pouvoir décider quand l’arrêter. en eﬀet, si un arbre peu profond risque de mal modéliser le problème, un arbre trop pro- fond est susceptible de sur-apprendre. \f9.3. méthodes ensemblistes : la sagesse des foules 109 une approche classique du problème est d’arrêter de diviser une région quand celle-ci contient un nombre minimum d’exemples d’entraînement ﬁxé par avance. cela évite de construire des feuilles trop spéciﬁques ne contenant qu’un seul point. on peut aussi choisir de limiter la profondeur de l’arbre. il est également possible d’utiliser une méthode de régularisation (cf. chapitre 6) pour contrôler la com- plexité de l’arbre, mesurée par le nombre de régions qu’il déﬁnit. c’est ce qu’on appelle l’élagage par coût en complexité, ou cost-complexity pruning : le coût en complexité d’un arbre t est donné par cλ(t ) = |t | (cid:88) r=1 nrimp(rr) + λ|t |, (9.5) où |t | est le nombre de régions déﬁnies par t , rr la r-ème de ces régions, et nr le nombre d’exemples d’en- traînement qu’elle contient. λ > 0 est un hyperparamètre permettant de contrôler l’importance relative de la mesure d’erreur et de la complexité. cette procédure requiert de développer complètement l’arbre, puis de l’élaguer en regroupant séquentiellement certaines régions jusqu’à ce que le critère soit optimal. malheureusement, les arbres de décision ont tendance à donner des modèles trop simples et à avoir des performances de prédiction à peine supérieures à des modèles aléatoires et peu robustes aux variations dans les données. on les qualiﬁe d’apprenants faibles (weak learners en anglais). heureusement, il est possible d’y remédier grâce aux méthodes ensemblistes. 9.3 méthodes ensemblistes : la sagesse des foules les méthodes ensemblistes sont des méthodes très puissantes en pratique, qui reposent sur l’idée que combiner de nombreux apprenants faibles permet d’obtenir une performance largement supérieure aux performances individuelles de ces apprenants faibles, car leurs erreurs se compensent les unes les autres. cette idée est similaire au concept de sagesse des foules, ou wisdom of crowd : si je demande à mes élèves l’année de la mort de pompidou, il est probable que la moyenne de leurs réponses soit assez proche de la bonne (1974) ; cependant, si je demande cette date à une seule personne au hasard, je n’aurais aucun moyen de savoir a priori si cette personne connaît la date ou me répond au hasard. exemple pour illustrer ce concept, imaginons une tâche de classiﬁcation en deux dimensions, dans laquelle les deux classes sont séparées par une diagonale, mais que le seul algorithme d’apprentissage dont nous dis- posions ne puisse apprendre qu’une frontière de décision en escalier, avec un nombre limité de paliers. combiner des dizaines voire des centaines de ces frontières de décision en escalier peut nous donner une bien meilleure approximation de la véritable frontière de décision. cet exemple est illustré sur la ﬁgure 9.4. attention la théorie des méthodes ensemblistes montre que lorsque les modèles que l’on combine ont été appris par des apprenants faibles, c’est-à-dire simples à entraîner et peu performants, ces méthodes permettent d’améliorer la performance par rapport à celle du meileur de ces modèles individuels. en pratique, si les modèles individuels sont déjà performants et robustes au bruit, le modèle ensembliste ne sera pas néces- sairement meilleur. on utilise le plus souvent des arbres de décision comme modèles individuels. deux grandes familles d’approches permettent de créer plusieurs modèles faibles à partir d’un unique jeu de données. elles sont toutes deux basées sur un ré-échantillonage du jeu de données, mais sont concep- tuellement très diﬀérentes. la première, le bagging, est une méthode parallèle dans laquelle les apprenants \f110 chapitre 9. arbres et forêts figure 9.4 – chacune des frontières de décision en escalier est une mauvaise approximation de la vraie frontière qui est la diagonale en trait plein. cependant, combiner ces escalier permet une meilleure ap- proximation de la diagonale. faibles sont indépendants les uns des autres, tandis que la deuxième, le boosting, est une méthode séquentielle dans laquelle chaque nouvel apprenant est construit en fonction des performances du précédent. 9.3.1 méthodes parallèles : le bagging supposons un jeu de données d de n observations (cid:126)x 1, (cid:126)x 2, . . . , (cid:126)x n de x étiquetées par y1, y2, . . . , yn. le bagging, proposé par leo breiman (1996), consiste à former b versions de d par échantillonage bootstrap (cf. section 3.1.4). chaque modèle faible est entraîné sur un des échantillons, ce qui peut être fait en parallèle. les b prédictions sont ensuite combinées — par vote de la majorité dans le cas d’un problème de classiﬁcation ; — en prenant la moyenne dans le cas d’un problème de régression. le bagging permet de réduire la variance des estimateurs individuels. c’est ce qui lui permet d’atteindre une plus grande stabilité et une meilleure prédiction qu’eux. la ﬁgure 9.5 illustre ce principe : les 5 premiers arbres qui composent le classiﬁeur entraîné par bagging séparent nettement moins bien le jeu de données que le bagging, mais ils font des erreurs sur des régions diﬀérentes de l’espace, qui se compensent lorsqu’on les combine. forêts aléatoires la puissance des méthodes ensemblistes se révèle lorsque les apprenants faibles sont indépendants conditionnellement aux données, autrement dit aussi diﬀérents les uns des autres que possible, aﬁn que leurs erreurs puissent se compenser les unes les autres. pour atteindre cet objectif, l’idée des forêts aléa- toires, proposée toujours par leo breiman, est de construire les arbres individuels non seulement sur des échantillons diﬀérents (comme pour le bagging), mais aussi en utilisant des variables diﬀérentes (breiman, 2001). plus précisément, les arbres construits pour former une forêt aléatoire diﬀèrent de ceux appris par cart en ce que, à chaque nœud, on commence par sélectionner q < p variables aléatoirement, avant de choisir la variable séparatrice parmi celles-ci. en classiﬁcation, on utilise typiquement q ≈ p, ce qui permet aussi de réduire considérablement les temps de calculs puisqu’on ne considère que peu de variables √ \f9.3. méthodes ensemblistes : la sagesse des foules 111 figure 9.5 – performance sur un jeu de test d’un classiﬁeur entraîné par bagging (en bas à droite) et des 5 premiers arbres qui le composent. à chaque nœud (5 pour un problème à 30 variables, 31 pour un problème avec 1000 variables). pour la régression, le choix par défaut est plutôt de q ≈ p 3 . remarque le fait de moyenner les réponses des diﬀérents arbres permet d’utiliser les forêts aléatoires aussi bien pour des problèmes de régression que de classiﬁcation. en pratique, les forêts aléatoires sont un des algorithmes les plus performants et les plus simples à mettre en place. elles ont aussi l’avantage de peu dépendre de leurs hyperparamètres, à savoir du nombre q de variables considérées à chaque nœud, du nombre d’observations utilisées pour chaque arbre (n dans la procédure que nous avons décrite, mais que l’on pourrait réduire), du nombre maximum d’observations dans les feuilles de l’arbre (généralement ﬁxé à 1 en classiﬁcation et 5 en régression), et du nombre d’arbres, à partir du moment où celui-ci est suﬃsamment grand. 9.3.2 méthodes séquentielles : le boosting alors que le bagging repose sur l’hypothèse que des apprenants ayant de bonnes performances sur des régions diﬀérentes de l’espace x des observations auront des erreurs décorrélées, et qu’il est donc pos- sible de faire conﬁance à la majorité d’entre eux pour ne pas faire d’erreur pour une prédiction donnée, \f112 chapitre 9. arbres et forêts l’approche séquentielle de la construction d’un ensemble cherche à créer des modèles faibles qui se concen- treront sur les erreurs du modèle. on parle alors de boosting : par itérations successives, des apprenants faibles viennent exalter (« booster ») les performances du modèle ﬁnal qui les combine. la première proposition dans ce sens est l’algorithme adaboost, proposé par schapire et al. (1997). adaboost adaboost, dont le nom vient de adaptive boosting, est un algorithme qui permet de construire un classi- ﬁeur de manière itérative, en forçant un classiﬁeur faible à se concentrer sur les erreurs du modèle grâce à un système de pondération des exemples d’entraînement. déﬁnition 9.4 (adaboost) supposons un jeu de données de classiﬁcation binaire d = {((cid:126)x i, yi)}i=1,...,n, un nombre d’itérations m et un algorithme d’apprentissage. étant donné un jeu de données s, on note fs la fonction de décision retournée par cet algorithme. nous supposons y = {−1, 1} et que fs est à valeur dans y. nous appelons jeu de données pondérées d(cid:48) = {(wi, (cid:126)x i, yi)}i=1,...,n ∈ rn × x n × y n un jeu de données {((cid:126)x i, yi)}i=1,...,n dans lequel un poids wi a été aﬀecté à la i-ème observation. nous supposons ici que l’algo- rithme d’apprentissage que nous utilisons est capable d’intégrer ces pondérations. dans le cas des arbres de décision, la pondération des exemples d’apprentissage se reﬂète par leur pondération dans le critère d’impureté ; la décision est également prise par vote majoritaire pondéré. on appelle adaboost la procédure de construction d’un ensemble d’apprenants suivante : 1. initialiser les pondérations w1 2. pour m = 1, 2, . . . , m : 2, . . . , w1 n 1, w1 à 1 n . (a) apprendre sur le jeu de données pondéré dm = {(wm i , (cid:126)x i, yi)}i=1,...,n la fonction de décision fm = fdm (b) calculer l’erreur pondérée de ce modèle : (cid:15)m = m (cid:88) i=1 wm i δ(fm((cid:126)x i), yi) (c) en déduire la conﬁance que l’on peut lui associer : αm = 1 2 log 1 − (cid:15)m (cid:15)m . (9.6) (9.7) αm est d’autant plus élevé que l’erreur globale du modèle est faible : on pourra alors lui faire plus conﬁance. (d) actualiser les poids, de sorte à donner plus d’importance à un exemple d’entraînement sur lequel fm se trompe : wm+1 i = 1 zm i e−αmyifm((cid:126)x i) où zm = wm n (cid:88) l=1 l e−αmylfm((cid:126)x l). wm (9.8) le rôle de zm est d’assurer que la somme des coeﬃcients wm+1 vaut 1. i 3. retourner la fonction de décision ﬁnale f : (cid:126)x (cid:55)→ m (cid:88) m=1 αmfm((cid:126)x). il est classique de considérer comme apprenants faibles pour adaboost un type bien particulier d’arbres de décision : les arbres de décision de profondeur 1, appelés stumps (qui signiﬁe souche en anglais). (cid:4) \f9.3. méthodes ensemblistes : la sagesse des foules 113 boosting du gradient adaboost se trouve en fait être un cas particulier d’une famille de techniques appelées boosting du gra- dient (gradient boosting, ou gboost), dont le cadre théorique a été développé en 1999 par, d’une part jerome h. friedman, et d’autre part llew mason, jonathan baxter, peter bartlett et marcus frean (friedman, 2001; mason et al., 1999). ce cadre permet tout d’abord de mieux comprendre le fonctionnement d’adaboost, en considérant la minimisation du risque empirique sur d et la fonction d’erreur exponentielle comme fonction de coût. déﬁnition 9.5 (erreur exponentielle) étant donné un problème de classiﬁcation, on appelle fonction d’erreur exponentielle, ou exponential loss, la fonction de coût suivante : {−1, 1} × r → r y, f ((cid:126)x) (cid:55)→ e−yf ((cid:126)x). (cid:80)m reprenons l’algorithme adaboost, et appelons fm la fonction de décision cumulative fm : (cid:126)x (cid:55)→ k=1 αkfk((cid:126)x). à l’étape m, l’erreur exponentielle de fm sur le jeu d’entraînement vaut (cid:4) em = = 1 n 1 n n (cid:88) i=1 n (cid:88) i=1 (cid:32) exp −yi m−1 (cid:88) k=1 (cid:33) αkfk((cid:126)x i) exp (cid:0)−αm yifm((cid:126)x i)(cid:1) exp (cid:0)−yifm−1((cid:126)x i)(cid:1) exp (cid:0)−αm yifm((cid:126)x i)(cid:1) . déﬁnissons maintenant wm i = exp (cid:0)−yifm−1((cid:126)x i)(cid:1) ; alors em = 1 n n (cid:88) i=1 i exp (cid:0)−αm yifm((cid:126)x i)(cid:1) . wm comme fm est à valeur dans {−1, 1}, le produit yifm((cid:126)x i) vaut 1 si la prédiction est correcte et −1 sinon, et donc em = 1 n n (cid:88) i=1 wm i e−αm + 1 n (cid:88) wm i (cid:0)eαm − e−αm(cid:1) . i:fm((cid:126)x i)(cid:54)=yi cette erreur est minimale quand αm a la forme donnée par l’équation 9.7. ainsi, adaboost combine les apprenants faibles de sorte à minimiser, à chaque étape, l’erreur exponentielle du classiﬁeur global. l’erreur exponentielle peut être remplacée par une autre fonction de coût, telle que l’entropie croisée ou l’erreur quadratique : c’est ce que l’on appelle le boosting du gradient. gboost est aujourd’hui un des algorithmes les plus populaires en machine learning. points clefs — les arbres de décision sont des modèles interprétables, qui manient naturellement des variables de plusieurs natures (réelles, discrètes et binaires) et se prêtent aisément à l’apprentissage multi-classe de distributions multi-modales. \f114 chapitre 9. arbres et forêts — les arbres de décision ont le grand inconvénient d’être des apprenants faibles, et d’avoir en général une capacité de modélisation trop limitée pour avoir de bonnes performances en pratique. — les méthodes ensemblistes permettent de remédier aux limitations des apprenants faibles tels que les arbres de décision, en combinant ces modèles de sorte à compenser leurs erreurs respectives. — les méthodes ensemblistes parallèles, telles que le bagging ou les forêts aléatoires, construisent un grand nombre de modèles faibles entraînés sur un échantillonage bootstrap des données. — les forêts aléatoires entraînent leurs arbres de façon à ce qu’ils soient indépendants les uns des autres conditionnelement aux données, en sélectionnant aléatoirement les variables à considérer à la création de chaque nœud. — les algorithmes de boosting combinent des modèles faibles entraînés séquentiellement pour donner plus d’importance aux exemples d’entraînement sur lesquels les prédictions sont les moins bonnes. pour aller plus loin • les forêts aléatoires permettent aussi d’établir une mesure d’importance de chaque variable, qui se base sur la fréquence à laquelle elle est utilisée comme variable séparatrice parmi tous les arbres de la forêt. plus précisément, l’importance par permutation mesure la diﬀérence de performance entre un arbre évalué sur un jeu de test et un arbre évalué sur ce même jeu de test dans lequel on a permuté aléatoirement les entrées de la j-ème variable : si cette variable est importante, alors le deuxième arbre devrait être moins performant. l’autre mesure parfois utilisée, celle de la diminution du critère d’impureté, fait la somme pondérée de la diminution du critère de gini pour toutes les coupures de l’arbre réalisées selon la variable dont on mesure l’importance. • l’implémentation la plus connue de gboost, au point d’ailleurs d’en devenir synonyme, est l’im- plémentation xgboost (chen et guestrin, 2016), disponible à l’url https://github.com/dmlc/ xgboost. bibliographie breiman, l. (1996). bagging predictors. machine learning, 26 :123–140. breiman, l. (2001). random forests. machine learning, 45(1) :5–32. breiman, l., friedman, j. h., olshen, r. a., and stone, c. j. (1984). classiﬁcation and regression trees. wad- sworth international group, belmont, ca. chen, t. et guestrin, c. (2016). xgboost : a scalable tree boosting system. in proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785–794, new york, ny, usa. acm. friedman, j. h. (2001). greedy function approximation : a gradient boosting machine. the annals of statistics, 29(5) :1189–1232. mason, l., baxter, j., bartlett, p., et frean, m. (1999). boosting algorithms as gradient descent. in proceedings of the 12th international conference on neural information processing systems, pages 512–518, cambridge, ma, usa. mit press. \fschapire, r., freund, y., bartlett, p., et lee, w. s. (1997). boosting the margin : a new explanation for the eﬀectiveness of voting methods. the annals of statistics, 26 :322–330. 115 \fchapitre 10 machines à vecteurs de support et méthodes à noyaux les machines à vecteurs de support (aussi appelées machines à vecteurs supports), ou svm de l’anglais sup- port vector machines, sont de puissants algorithmes d’apprentissage automatique. elles se basent sur un al- gorithme linéaire proposé par vladimir vapnik et aleksandr lerner en 1963 (vapnik et lerner, 1963), mais permettent d’apprendre bien plus que des modèles linéaires. en eﬀet, au début des années 1990, vladimir vapnik, bernhard boser, isabelle guyon et corinna cortes (boser et al., 1992; cortes et vapnik, 1995) ont trouvé comment les étendre eﬃcacement à l’apprentissage de modèles non linéaires grâce à l’astuce du noyau. ce chapitre présente cette approche dans ses diﬀérentes versions pour un problème de classiﬁca- tion, et introduit ainsi la famille des méthodes à noyaux objectifs — déﬁnir un classiﬁeur à large marge dans le cas séparable ; — écrire les problèmes d’optimisation primal et dual correspondants ; — réécrire ces problèmes dans le cas non séparable ; — utiliser l’astuce du noyau pour appliquer une svm à marge souple dans le cas non linéaire ; — déﬁnir des noyaux pour des données à valeurs réelles ou des chaînes de caractères. 10.1 le cas linéairement séparable : svm à marge rigide nous nous plaçons dans ce chapitre dans le cas d’un problème de classiﬁcation binaire. dans cette sec- tion, nous allons supposer qu’il est possible de trouver un modèle linéaire qui ne fasse pas d’erreur sur nos données : c’est ce qu’on appelle un scénario linéairement séparable. déﬁnition 10.1 (séparabilité linéaire) soit d = {((cid:126)x i, yi)}i=1,...,n un jeu de données de n observa- tions. nous supposons que (cid:126)x i ∈ rp et yi ∈ {−1, 1}. on dit que d est linéairement séparable s’il existe au moins un hyperplan dans rp tel que tous les points positifs (étiquetés +1) soient d’un côté de cet hyperplan (cid:4) et tous les points négatifs (étiquetés −1) de l’autre. 116 \f10.1. le cas linéairement séparable : svm à marge rigide 117 dans ce cas, il existe en fait une inﬁnité d’hyperplans séparateurs qui ne font aucune erreur de classiﬁ- cation (voir ﬁgure 10.1). ces hyperplans sont des modèles équivalents du point de vue de la minimisation du risque empirique. figure 10.1 – une inﬁnité d’hyperplans (en deux dimensions, des droites) séparent les points négatifs (x) des points positifs (+). 10.1.1 marge d’un hyperplan séparateur en l’absence d’information supplémentaire, l’hyperplan séparateur en pointillés sur la ﬁgure 10.1 semble préférable. en eﬀet, celui-ci, étant équidistant de l’observation positive la plus proche et de l’observation négative la plus proche, coupe en quelque sorte la poire en deux pour la région située « entre » les points positifs et les points négatifs. la marge d’un hyperplan séparateur permet de formaliser cette intuition. déﬁnition 10.2 (marge) la marge γ d’un hyperplan séparateur est la distance de cet hyperplan à (cid:4) l’observation du jeu d’entraînement la plus proche. l’hyperplan séparateur que nous cherchons est donc celui qui maximise la marge. il y a alors au moins une observation négative et une observation positive qui sont à une distance γ de l’hyperplan séparateur : dans le cas contraire, si par exemple toutes les observations négatives étaient à une distance supérieure à γ de l’hyperplan séparateur, on pourrait rapprocher cet hyperplan des observations négatives et augmenter la marge. nous pouvons alors déﬁnir, en plus de l’hyperplan séparateur h, les hyperplans h+ et h− qui lui sont parallèles et situés à une distance γ de part et d’autre. h+ contient au moins une observation positive, tandis que h− contient au moins une observation négative. déﬁnition 10.3 (vecteurs de support) on appelle vecteurs de support les observations du jeu d’entraî- nement situés à une distance γ de l’hyperplan séparateur. elles « soutiennent » les hyperplans h+ et h−. \f118 (cid:4) chapitre 10. machines à vecteurs de support et méthodes à noyaux c’est de là que vient le nom de la méthode, appelée support vector machine ou svm en anglais, et machine à vecteurs de support en français. on rencontre aussi parfois le nom de « séparatrice à vaste marge », qui respecte les initiales svm. si l’on venait à déplacer légèrement une observation qui est vecteur de support, cela déplacerait la zone d’indécision et l’hyperplan séparateur changerait. à l’inverse, si l’on déplace légèrement une observation qui n’est pas vecteur de support, h n’est pas aﬀecté : les vecteurs de support sont les observations qui soutiennent la solution. toutes les observations positives sont situées à l’extérieur de h+, tandis que toutes les observations négatives sont situées à l’extérieur de h−. déﬁnition 10.4 (zone d’indécision) on appelle zone d’indécision la zone située entre h− et h+. cette (cid:4) zone ne contient aucune observation. la ﬁgure 10.2 illustre ces concepts. figure 10.2 – la marge γ d’un hyperplan séparateur (ici en trait plein) est sa distance à l’observation la plus proche. quand cette marge est maximale, au moins une observation négative et une observation positive sont à une distance γ de l’hyperplan séparateur. les hyperplans (ici en pointillés) parallèles à l’hyperplan séparateur et passant par ces observations déﬁnissent la zone d’indécision. les observations situées sur ces hyperplans (cerclées) sont les vecteurs de support. 10.1.2 formulation de la svm à marge rigide l’équation de l’hyperplan séparateur h que nous cherchons est de la forme (cid:104) (cid:126)w, (cid:126)x(cid:105) + b = 0, où (cid:104), (cid:105) représente le produit scalaire sur rp . l’hyperplan h+ lui est parallèle, et a donc pour équation (cid:104) (cid:126)w, (cid:126)x(cid:105)+b = constante. nous pouvons ﬁxer cette constante à 1 sans perdre en généralité : en eﬀet, si nous multiplions (cid:126)w et b par une constante, cela n’aﬀectera pas l’équation de h. h− étant symétrique de h par rapport à . h+, son équation est alors (cid:104) (cid:126)w, (cid:126)x(cid:105) + b = −1. enﬁn, la marge qui est la distance de h à h+ vaut γ = 1 || (cid:126)w||2 \f10.1. le cas linéairement séparable : svm à marge rigide 119 les observations positives vériﬁent donc toutes (cid:104) (cid:126)w, (cid:126)x(cid:105) + b ≥ 1. de même, les points négatifs vériﬁent tous (cid:104) (cid:126)w, (cid:126)x(cid:105) + b ≤ 1. ainsi, pour chacun des points de notre jeu d’entraînement, yi (cid:0)(cid:104) (cid:126)w, (cid:126)x i(cid:105) + b(cid:1) ≥ 1. l’égalité est vériﬁée pour les vecteurs de support. nous cherchons donc ici à maximiser peut donc se formaliser comme suit : 1 || (cid:126)w||2 sous les n contraintes yi (cid:0)(cid:104) (cid:126)w, (cid:126)x i(cid:105) + b(cid:1) ≥ 1. notre problème déﬁnition 10.5 (formulation primale de la svm à marge rigide) on appelle svm à marge rigide le problème d’optimisation suivant : arg min (cid:126)w∈rp,b∈r 1 2 || (cid:126)w||2 2 t.q. yi (cid:0)(cid:104) (cid:126)w, (cid:126)x i(cid:105) + b(cid:1) ≥ 1, i = 1, . . . , n. supposons (cid:126)w ∗, b∗ solutions de l’équation 10.1. la fonction de décision est alors donnée par f ((cid:126)x) = (cid:104) (cid:126)w ∗, (cid:126)x(cid:105) + b∗. (10.1) (cid:4) (10.2) 10.1.3 formulation duale le problème déﬁni par l’équation 10.1 est un problème d’optimisation convexe sous n contraintes, cha- cune des contraintes correspondant à un des points du jeu d’entraînement. de plus, ces contraintes sont toutes aﬃnes. les conditions de slater nous garantissent donc que la fonction objective de la svm à marge rigide (minimisée dans l’équation 10.1) est minimisée aux mêmes points que son dual (voir la section a.4.3). nous pouvons donc en déduire une deuxième formulation équivalente du problème : théorème 10.1 (formulation duale de la svm à marge rigide) le problème déﬁni par l’équation 10.1 est équivalent à : max (cid:126)α∈rn t. q. n (cid:88) i=1 n (cid:88) i=1 αi − 1 2 n (cid:88) n (cid:88) i=1 l=1 αiαlyiyl(cid:104)(cid:126)x i, (cid:126)x l(cid:105) αiyi = 0; αi ≥ 0, i = 1, . . . , n. (10.3) (cid:4) démonstration. il s’agit de la formulation duale d’un problème d’optimisation convexe sous contraintes (voir section a.4). introduisons n multiplicateurs de lagrange {αi}i=1,...,n, un pour chaque contrainte. le lagrangien est donc la fonction l : rp × r × rn + → r 1 2 (cid:126)w, b, (cid:126)α (cid:55)→ || (cid:126)w||2 2 − n (cid:88) i=1 (cid:0)yi (cid:0)(cid:104) (cid:126)w, (cid:126)x i(cid:105) + b(cid:1) − 1(cid:1) . αi la fonction duale de lagrange est donc la fonction q : rn + → r (cid:126)α (cid:55)→ inf (cid:126)w∈rp,b∈r l( (cid:126)w, b, (cid:126)α) (10.4) (10.5) \f120 chapitre 10. machines à vecteurs de support et méthodes à noyaux enﬁn, le problème dual de celui présenté par l’équation 10.1 est donc max (cid:126)α∈rn + inf (cid:126)w∈rp,b∈r 1 2 || (cid:126)w||2 2 − n (cid:88) i=1 (cid:0)yi (cid:0)(cid:104) (cid:126)w, (cid:126)x i(cid:105) + b(cid:1) − 1(cid:1) αi (10.6) le lagrangien est convexe en (cid:126)w et est donc minimal quand son gradient en (cid:126)w est nul, à savoir quand (cid:126)w = n (cid:88) i=1 αiyi(cid:126)x i. (10.7) de plus, il est aﬃne en b. son inﬁmum est donc −∞, sauf si son gradient en b est nul (auquel cas la fonction aﬃne est « plate »), à savoir si n (cid:88) i=1 αiyi = 0. (10.8) la fonction duale q est donc maximisée dans ce deuxième cas. en remplaçant (cid:126)w par son expression (équation 10.7) dans l’écriture de la fonction duale, l’équation 10.6 peut donc être reformulée comme n (cid:88) n (cid:88) αiαlyiyl(cid:104)(cid:126)x i, (cid:126)x l(cid:105) − n (cid:88) i=1 αiyi n (cid:88) l=1 αlyl(cid:104)(cid:126)x l, (cid:126)x i(cid:105) − n (cid:88) i=1 αiyib + n (cid:88) i=1 αi 1 2 max (cid:126)α∈rn t. q. l=1 i=1 n (cid:88) αiyi = 0; αi ≥ 0, i = 1, . . . , n. i=1 on obtient le résultat recherché en utilisant l’équation 10.8. (cid:3) i=1 α∗ i yi(cid:126)x i. pour résoudre le problème primal (équation 10.1), on peut donc commencer par résoudre le problème dual (équation 10.3). supposons (cid:126)α ∗ solution du problème 10.3. l’équation 10.7 nous donne la solution en (cid:126)w de l’équation 10.1 : (cid:126)w ∗ = (cid:80)n pour trouver b∗, on peut revenir à la formulation initiale de la svm : les vecteurs de support positifs sont situés sur l’hyperplan h+ et vériﬁent (cid:104) (cid:126)w ∗, (cid:126)x i(cid:105) + b∗ = 1. de plus, étant les observations positives les plus proches de l’hyperplan séparateur h, les vecteurs de support positifs minimisent (cid:104) (cid:126)w ∗, (cid:126)x i(cid:105). ainsi b∗ = 1 − min i:yi=+1 (cid:104) (cid:126)w ∗, (cid:126)x i(cid:105). enﬁn, la fonction de décision est donnée par : f ((cid:126)x) = n (cid:88) i=1 i yi(cid:104)(cid:126)x i, (cid:126)x(cid:105) + b∗. α∗ encart complexité algorithmique (10.9) la formulation primale de la svm est un problème d’optimisation en p + 1 dimensions, tandis que la formulation duale est un problème d’optimisation en n dimensions. si l’on a peu de données et beaucoup de variables, on préférera la formulation duale ; dans le cas inverse, on préférera résoudre le problème primal. \f10.2. le cas linéairement non séparable : svm à marge souple 121 10.1.4 interprétation géométrique les conditions de karush-kuhn-tucker (voir section a.4.4) nous permettent de caractériser plus précisé- ment la relation entre (cid:126)α ∗ et ( (cid:126)w ∗, b∗). appelons φ la fonction qui à (cid:126)w associe 1 , et gi la fonction qui à (cid:126)w, b associe gi( (cid:126)w, b) = yi (cid:0)(cid:104) (cid:126)w, (cid:126)x i(cid:105) + b(cid:1) − 1. pour tout 1 ≤ i ≤ n, la condition d’écart complémentaire signiﬁe que α∗ 2 || (cid:126)w||2 2 i gi( (cid:126)w ∗, b∗) = 0. plus précisément, deux cas sont possibles pour chacune des conditions i : — α∗ i = 0 : le minimiseur de φ vériﬁe la contrainte et gi( (cid:126)w ∗, b∗) > 0, autrement dit le point (cid:126)x i est à l’extérieur des hyperplans h+ ou h− ; i > 0 : la contrainte est vériﬁée en bordure de la zone de faisabilité, autrement dit à l’égalité gi( (cid:126)w ∗, b∗) = 0, et (cid:126)x i est un vecteur de support. — α∗ ainsi, les vecteurs de support sont les observations du jeu de données correspondant à un multiplicateur de la- grange α∗ i non nul. 10.2 le cas linéairement non séparable : svm à marge souple nos données ne sont généralement pas linéairement séparables. dans ce cas, quel que soit l’hyperplan séparateur que l’on choisisse, certains des points seront mal classiﬁés ; d’autres seront correctement clas- siﬁés, mais à l’intérieur de la zone d’indécision. ces concepts sont illustrés sur la ﬁgure 10.3. figure 10.3 – aucun classiﬁeur linéaire ne peut séparer parfaitement ces données. les observations mar- quées d’un carré sont des erreurs de classiﬁcation. l’observation marquée d’un triangle est correctement classiﬁée mais est située à l’intérieur de la zone d’indécision. si elle était à sa frontière, autrement dit, si elle était vecteur de support, la marge serait beaucoup plus étroite. 10.2.1 formulation de la svm à marge souple notre but est maintenant de trouver un compromis entre les erreurs de classiﬁcation et la taille de la marge. comme dans la svm à marge rigide (équation 10.1), nous cherchons à minimiser l’inverse du carré \f122 chapitre 10. machines à vecteurs de support et méthodes à noyaux de la marge 1 hyperparamètre de la svm, et l représente une fonction de coût : , auquel nous rajoutons un terme d’erreur c × (cid:80)n 2 || (cid:126)w||2 2 i=1 l(f ((cid:126)x i), yi). ici c ∈ r+ est un arg min (cid:126)w∈rp,b∈r 1 2 || (cid:126)w||2 2 + c n (cid:88) i=1 l((cid:104) (cid:126)w, (cid:126)x i(cid:105) + b, yi). (10.10) l’hyperparamètre de coût c permet de contrôler l’importance relative de la marge et des erreurs du modèle sur le jeu d’entraînement. il confère ainsi une certaine souplesse à la marge ; on parle alors de svm à marge souple. nous souhaitons, autant que possible, que toute observation (cid:126)x d’étiquette y soit située à l’extérieur de la zone d’indécision, autrement dit vériﬁe yf ((cid:126)x) ≥ 1. nous allons donc utiliser l’erreur hinge (cf. 2.12) comme fonction de coût. ainsi l’équation 10.10 peut se réécrire comme suit : déﬁnition 10.6 (svm à marge souple) on appelle svm à marge souple la solution du problème d’opti- misation suivant : arg min (cid:126)w∈rp,b∈r 1 2 || (cid:126)w||2 2 + c n (cid:88) i=1 (cid:2)1 − yif ((cid:126)x i)(cid:3) + . (10.11) (cid:4) cette formulation est celle d’une régularisation de la minimisation d’un risque empirique par un terme de norme (cid:96)2. elle est similaire à la régression ridge (voir le chapitre 6), c étant inversement proportionnel à λ, mais la fonction d’erreur est diﬀérente. remarque déﬁnition 10.7 (formulation primale de la svm à marge souple) en introduisant une variable d’ajus- pour chaque obser- tement (ou variable d’écart ; on parlera de slack variable en anglais) ξi = (cid:2)1 − yif ((cid:126)x i)(cid:3) vation du jeu d’entraînement, le problème d’optimisation 10.11 est équivalent à + arg min (cid:126)w∈rp,b∈r,(cid:126)ξ∈rn 1 2 || (cid:126)w||2 2 + c n (cid:88) i=1 ξi t. q. yi (cid:0)(cid:104) (cid:126)w, (cid:126)x i(cid:105) + b(cid:1) ≥ 1 − ξi, i = 1, . . . , n ξi ≥ 0, i = 1, . . . , n. (10.12) (cid:4) 10.2.2 formulation duale comme dans le cas de la svm à marge rigide, il s’agit d’un problème d’optimisation convexe, cette fois sous 2n contraintes, toutes aﬃnes et les conditions de slater s’appliquent. théorème 10.2 (formulation duale de la svm à marge souple) le problème 10.12 est équivalent à max (cid:126)α∈r t. q. n (cid:88) i=1 n (cid:88) i=1 αi − 1 2 n (cid:88) n (cid:88) i=1 l=1 αiαlyiyl(cid:104)(cid:126)x i, (cid:126)x l(cid:105) αiyi = 0; 0 ≤ αi ≤ c, i = 1, . . . , n. (10.13) (cid:4) \f10.2. le cas linéairement non séparable : svm à marge souple 123 démonstration. introduisons 2n multiplicateurs de lagrange {αi, βi}i=1,...,n et écrivons le lagrangien : l : rp × r × rn × rn + × rn (cid:126)w, b, (cid:126)ξ, (cid:126)α, (cid:126)β (cid:55)→ + → r 1 2 || (cid:126)w||2 2 + c n (cid:88) i=1 ξi − n (cid:88) i=1 (cid:0)yi (cid:0)(cid:104) (cid:126)w, (cid:126)x i(cid:105) + b(cid:1) − 1 + ξi (cid:1) αi (10.14) − n (cid:88) i=1 βiξi. la fonction duale de lagrange est donc la fonction q : rn + × rn + → r (10.15) (cid:126)α, (cid:126)β (cid:55)→ inf (cid:126)w∈rp,b∈r,(cid:126)ξ∈rn l( (cid:126)w, b, (cid:126)ξ, (cid:126)α, (cid:126)β) le problème dual de celui présenté par l’équation 10.12 est donc max +,(cid:126)β∈rn + (cid:126)α∈rn inf (cid:126)w∈rp,b∈r,(cid:126)ξ∈rn 1 2 || (cid:126)w||2 2 + c n (cid:88) i=1 ξi − n (cid:88) i=1 (cid:0)yi (cid:0)(cid:104) (cid:126)w, (cid:126)x i(cid:105) + b(cid:1) − 1 + ξi (cid:1) − αi n (cid:88) i=1 βiξi. (10.16) comme dans le cas de la svm à marge rigide, le lagrangien est minimal quand son gradient en (cid:126)w est nul, à savoir quand (cid:126)w = n (cid:88) i=1 αiyi(cid:126)x i. (10.17) toujours comme précédemment, il est aﬃne en b et son inﬁmum est donc −∞, sauf si son gradient en b est nul, à savoir si n (cid:88) αiyi = 0. i=1 de plus, il est aﬃne en (cid:126)ξ et son inﬁmum est donc −∞, sauf si son gradient en (cid:126)ξ est nul, à savoir si βi = c − αi, i = 1, . . . , n. (10.18) (10.19) la fonction duale q est donc maximisée quand les équations 10.18 et 10.19 sont vériﬁés. en remplaçant (cid:126)w par son expression (équation 10.17) dans l’expression de la fonction duale, l’équa- tion 10.16 peut donc être reformulée comme max (cid:126)α∈rn − 1 2 n (cid:88) n (cid:88) i=1 l=1 αiαlyiyl(cid:104)(cid:126)x i, (cid:126)x l(cid:105) + n (cid:88) i=1 αi + c n (cid:88) i=1 ξi − n (cid:88) i=1 (c − αi)ξi − n (cid:88) i=1 αiξi t. q. n (cid:88) i=1 αiyi = 0; αi ≥ 0, i = 1, . . . , n; c − αi ≥ 0, i = 1, . . . , n. (cid:3) la seule diﬀérence avec la formulation duale de la svm à marge rigide (équation 10.3) est la contrainte remarque αi ≤ c, i = 1, . . . , n. \f124 chapitre 10. machines à vecteurs de support et méthodes à noyaux 10.2.3 interprétation géométrique comme précédemment, les conditions de karush-kuhn-tucker nous permettent de caractériser plus précisément la relation entre (cid:126)α ∗ et ( (cid:126)w ∗, b∗). pour chaque observation i, nous avons maintenant deux conditions d’écart complémentaire : i gi( (cid:126)w ∗, b∗) = 0 et i hi( (cid:126)w ∗, b∗) = 0, ou encore (c − α∗ hi( (cid:126)w, b) = (cid:2)1 − yi (cid:0)(cid:104) (cid:126)w, (cid:126)x i(cid:105) + b(cid:1)(cid:3) + . i )hi( (cid:126)w ∗, b∗) = 0, où — α∗ — β∗ nous avons ainsi, pour chaque observation i, trois possibilités : 2 || (cid:126)w||2 — α∗ i = 0 : le minimiseur de 1 encore une fois, à l’extérieur de la zone d’indécision. 2 vériﬁe la contrainte et yi (cid:0)(cid:104) (cid:126)w, (cid:126)x i(cid:105) + b(cid:1) > 1. l’observation (cid:126)x i est, — 0 < α∗ i < c : comme précédemment, (cid:126)x i est un vecteur de support situé sur la bordure de la zone — β∗ d’indécision. i = 0 : α∗ frontière de la zone d’indécision. i = c, auquel cas (cid:2)1 − yi (cid:0)(cid:104) (cid:126)w, (cid:126)x i(cid:105) + b(cid:1)(cid:3) + > 0. dans ce cas, (cid:126)x i est du mauvais côté de la il est possible d’utiliser les svm pour construire un classiﬁeur multi-classe, grâce à une approche une- contre-toutes ou une-contre-une (cf. section 2.1.2). remarque 10.3 le cas non linéaire : svm à noyau il est fréquent qu’une fonction linéaire ne soit pas appropriée pour séparer nos données (voir par exemple la ﬁgure 10.4a). que faire dans ce cas ? (a) un cercle semble bien mieux indiqué qu’une droite pour séparer ces données. (b) après transformation par l’application φ (x1, x2) (cid:55)→ (x2 séparables dans l’espace de redescription. : 2), les données sont linéairement 1, x2 figure 10.4 – transformer les données permet de les séparer linéairement dans un espace de redescription. \f10.3. le cas non linéaire : svm à noyau 10.3.1 espace de redescription 125 dans le cas des données représentées sur la ﬁgure 10.4a, un cercle d’équation x2 1 + x2 2 = r2 semble bien mieux indiqué qu’une droite pour séparer les deux classes. 2). déﬁnissons donc l’application φ : r2 → r2, (x1, x2) (cid:55)→ (x2 or si la fonction f : r2 → r, (cid:126)x (cid:55)→ x2 1, x2 2 − r2 n’est pas linéaire en (cid:126)x = (x1, x2), elle est linéaire en 2). la fonction de décision f est (x2 linéaire en φ((cid:126)x) : f ((cid:126)x) = φ((cid:126)x)1 + φ((cid:126)x)2 − r2. nous pouvons donc l’apprendre en utilisant une svm sur les images des données par l’application φ. 1 + x2 1, x2 plus généralement, nous allons maintenant supposer que les observations sont déﬁnies sur un espace quelconque x , qui peut être rp mais aussi par exemple l’ensemble des chaînes de caractères sur un alpha- bet donné, l’espace de tous les graphes, ou un espace de fonctions. déﬁnition 10.8 (espace de redescription) on appelle espace de redescription l’espace de hilbert h dans lequel il est souhaitable de redécrire les données, au moyen d’une application φ : x → h, pour y entraîner (cid:4) une svm sur les images des observations du jeu d’entraînement. la redescription des données dans un espace de hilbert nous permet d’utiliser un algorithme linéaire, comme la svm à marge souple, pour résoudre un problème non linéaire. 10.3.2 svm dans l’espace de redescription pour entraîner une svm sur les images de nos observations dans l’espace de redescription h, il nous faut donc résoudre, d’après l’équation 10.13, le problème suivant : n (cid:88) i=1 n (cid:88) max (cid:126)α∈r t. q. αi − 1 2 n (cid:88) n (cid:88) i=1 l=1 αiαlyiyl(cid:104)φ((cid:126)x i), φ((cid:126)x l)(cid:105)h (10.20) αiyi = 0; 0 ≤ αi ≤ c, i = 1, . . . , n. i=1 la fonction de décision sera ensuite donnée par (équation 10.9) f ((cid:126)x) = n (cid:88) i=1 i yi(cid:104)φ((cid:126)x i), φ((cid:126)x)(cid:105)h + b∗. α∗ (10.21) 10.3.3 astuce du noyau dans les équations 10.20 et 10.21, les images des observations dans h apparaissent uniquement dans des produits scalaires sur h. nous pouvons donc, plutôt que la fonction φ : x → h, utiliser la fonction suivante, appelée noyau : k : x × x → r (cid:126)x, (cid:126)x(cid:48) (cid:55)→ (cid:104)φ((cid:126)x), φ((cid:126)x(cid:48))(cid:105)h. nous pouvons maintenant déﬁnir une svm à noyau : déﬁnition 10.9 (svm à noyau) on appelle svm à noyau la solution du problème d’optimisation suivant : max (cid:126)α∈r t. q. n (cid:88) i=1 n (cid:88) i=1 αi − 1 2 n (cid:88) n (cid:88) i=1 l=1 αiαlyiylk((cid:126)x i, (cid:126)x l) αiyi = 0; 0 ≤ αi ≤ c, i = 1, . . . , n. (10.22) \f126 chapitre 10. machines à vecteurs de support et méthodes à noyaux la fonction de décision sera ensuite donnée par (équation 10.9) : f ((cid:126)x) = n (cid:88) i=1 i yik((cid:126)x i, (cid:126)x) + b∗. α∗ (cid:4) (10.23) que ce soit pour entraîner la svm ou pour l’appliquer, nous n’avons pas besoin de connaître φ expli- citement, mais il nous suﬃt de connaître le noyau k. cela signiﬁe que nous n’avons pas besoin de faire de calcul dans h, qui est généralement de très grande dimension : c’est ce que l’on appelle l’astuce du noyau. l’astuce du noyau s’applique de manière générale à d’autres algorithmes d’apprentissage linéaires, comme la régression ridge (cf. section 6.2), l’acp (cf. section 11.3.1) ou encore la méthode des k-moyennes (cf. section 12.4). 10.3.4 noyaux caractérisation mathématique déﬁnition 10.10 (noyau) nous appelons noyau toute fonction k de deux variables s’écrivant sous la forme d’un produit scalaire des images dans un espace de hilbert de ses variables. ainsi, un noyau est une fonction continue, symétrique, et semi-déﬁnie positive : ∀ n ∈ n, ∀ ((cid:126)x 1, (cid:126)x 2, . . . (cid:126)x n ) ∈ x n et (a1, a2, . . . an ) ∈ rn , n (cid:88) n (cid:88) i=1 l=1 aialk((cid:126)x i, (cid:126)x l) ≥ 0. (cid:4) déﬁnition 10.11 (matrice de gram) étant données n observations (cid:126)x 1, (cid:126)x 2, . . . , (cid:126)x n ∈ x n, et un noyau k sur x , on appelle matrice de gram de ces observations la matrice k ∈ rn×n telle que kil = k((cid:126)x i, (cid:126)x l). (cid:4) cette matrice est semi-déﬁnie positive. théorème 10.3 moore-aronszajn pour toute fonction symétrique semi-déﬁnie positive κ : x × x (cid:55)→ r, il existe un espace de hilbert f et une application ψ : x → f telle que pour tout (cid:126)x, (cid:126)x(cid:48) ∈ x × x , κ((cid:126)x, (cid:126)x(cid:48)) = (cid:4) (cid:104)ψ((cid:126)x), ψ((cid:126)x(cid:48))(cid:105)f . démonstration. on trouvera une preuve de ce résultat dans l’article original de nachman aronszajn, (cid:3) qui l’attribue à e. hastings moore (aronszajn, 1950). intuitivement, un noyau peut être interprété comme un produit scalaire sur un espace de hilbert, au- trement dit comme une fonction qui mesure la similarité entre deux objets de x . ainsi, on peut déﬁnir des noyaux en construisant une similarité entre objets, puis en vériﬁant qu’elle est semi-déﬁnie positive. noyaux pour vecteurs réels quand x = rp, le théorème de moore-aronszajn nous permet de déﬁnir les noyaux suivants. déﬁnition 10.12 (noyau quadratique) on appelle noyau quadratique le noyau déﬁni par k((cid:126)x, (cid:126)x(cid:48)) = (cid:4) ((cid:104)(cid:126)x, (cid:126)x(cid:48)(cid:105) + c)2 où c ∈ r+. \f10.3. le cas non linéaire : svm à noyau 127 l’application correspondant à ce noyau est φ : (cid:126)x (cid:55)→ (x2 √ √ 1, . . . , x2 p, 2cxp, c). φ a ainsi valeur dans un espace de dimension 2p + p(p−1) . . . 2 + 1 : utiliser k et l’astuce du noyau sera plus eﬃcace que de calculer les images des observations par φ avant de leur appliquer une svm. 2xp−1xp, . . . 2x1x2, . . . , 2cx1, . . . 2x1xp, √ √ √ déﬁnition 10.13 (noyau polynomial) on appelle noyau polynomial de degré d ∈ n le noyau déﬁni par (cid:4) k((cid:126)x, (cid:126)x(cid:48)) = ((cid:104)(cid:126)x, (cid:126)x(cid:48)(cid:105) + c)d . comme pour le noyau quadratique, c ∈ r+ est un paramètre permettant d’inclure des termes de degré inférieur à d. ce noyau correspond à un espace de redescription comptant autant de dimensions qu’il existe de monômes de p variables de degré inférieur ou égal à d, soit (cid:0)p+d d (cid:1). déﬁnition 10.14 (noyau radial gaussien) on appelle noyau radial gaussien, ou noyau rbf (pour radial (cid:4) basis function), de bande passante σ > 0 le noyau déﬁni par k((cid:126)x, (cid:126)x(cid:48)) = exp (cid:16) (cid:17) . − ||(cid:126)x−(cid:126)x(cid:48)||2 2σ2 ce noyau correspond à un espace de redescription de dimension inﬁnie. en eﬀet, en utilisant le déve- loppement en série entière de la fonction exponentielle, on peut le réécrire comme k((cid:126)x, (cid:126)x(cid:48)) = exp (cid:18) − ||(cid:126)x||2 2σ2 (cid:18) − (cid:19) (cid:18) exp − (cid:104)(cid:126)x, (cid:126)x(cid:48)(cid:105) σ2 (cid:19) (cid:18) exp − (cid:19) ||(cid:126)x(cid:48)||2 2σ2 (cid:19) (cid:104)(cid:126)x, (cid:126)x(cid:48)(cid:105)r σ2rr! ψ((cid:126)x(cid:48)) = (cid:32) − +∞ (cid:88) r=0 (cid:104)ψ((cid:126)x)1/r(cid:126)x, ψ((cid:126)x(cid:48))1/r(cid:126)x(cid:48)(cid:105)r σ2rr! (cid:33) = ψ((cid:126)x) +∞ (cid:88) r=0 (cid:16) − ||(cid:126)x||2 2σ2 (cid:17) . avec ψ : rp → r, (cid:126)x (cid:55)→ exp pour ces trois noyaux, il est plus facile et eﬃcace de calculer directement le noyau entre deux vecteurs de rp que de calculer le produit scalaire de leur image dans l’espace de redescription correspondant, qui peut même être de dimension inﬁnie. noyaux pour chaînes de caractères l’astuce du noyau nous permet aussi de travailler sur des données complexes sans avoir à les exprimer tout d’abord en une représentation vectorielle de longueur ﬁxe. c’est le cas en particulier pour les données représentées par des chaînes de caractères, comme du texte ou des séquences biologiques telles que de l’adn (déﬁnies sur un alphabet de 4 lettres correspondant aux 4 bases nucléiques) ou des protéines (déﬁnies sur un alphabet de 21 acides aminés.) étant donné un alphabet a, nous utilisons maintenant x = a∗ (c’est-à-dire l’ensemble des chaînes de caractères déﬁnies sur a.) la plupart des noyaux sur x sont déﬁnis en utilisant l’idée que plus deux chaînes x et x(cid:48) ont de sous-chaînes en commun, plus elles sont semblables. étant donnée une longueur k ∈ n de sous-chaînes, nous transformons une chaîne x en un vecteur de longueur |a|k grâce à l’application φ : x (cid:55)→ (ψu(x))u∈ak , où ψu(x) est le nombre d’occurrences de u dans x. ψ peut être modiﬁée pour permettre les alignements inexacts, ou autoriser en les pénalisant les « trous » (ou gaps.) on peut alors déﬁnir le noyau pour chaînes de caractères suivant : k : a∗ × a∗ → r x, x(cid:48) (cid:55)→ (cid:88) ψu(x)ψu(x(cid:48)). formellement, ce noyau nécessite de calculer une somme sur |ak| = |a|k termes. cependant, il peut être calculé de manière bien plus eﬃcace en itérant uniquement sur les (|x| + 1 − k) chaînes de longueur u∈ak \f128 chapitre 10. machines à vecteurs de support et méthodes à noyaux k présentes dans x, les autres termes de la somme valant nécessairement 0. il s’agit alors d’un calcul en o(|x| + |x(cid:48)|). dans le cas des protéines humaines, si l’on choisit k = 8, on remplace ainsi un calcul dans un espace de redescription de dimension supérieure à 37 milliards (218) par une somme sur moins de 500 termes (la longueur moyenne d’une protéine humaine étant de 485 acides aminés.) exemple ces idées peuvent aussi être appliquées à la déﬁnition de noyaux pour graphes, en remplaçant les sous- chaînes de caractères par des sous-graphes. cependant, le problème de l’isomorphisme de sous-graphes est np-complet dans le cas général, ce qui limite le type de sous-graphes que l’on peut considérer. noyaux pour ensembles certaines des similarités entre ensembles que nous avons déﬁnies à la section 8.3.3 sont en fait des noyaux. théorème 10.4 la similarité de jaccard et la similarité minmax sont des noyaux. démonstration. la preuve de ce théorème repose sur un résultat de gower (1971). 10.4 régression ridge à noyau théorème 10.5 l’astuce du noyau s’applique aussi à la régression ridge linéaire. démonstration. rappelons que la fonction de prédiction de la régression ridge est de la forme où (cid:126)β ∗ est donné par l’équation 6.7 : f ((cid:126)x) = (cid:104)(cid:126)x, (cid:126)β ∗(cid:105), (cid:16) (cid:126)β ∗ = λip + x (cid:62)x (cid:17)−1 x (cid:62)(cid:126)y. (cid:4) (cid:3) (cid:4) (10.24) (10.25) ici x ∈ rn×p est la matrice de design potentiellement augmentée d’une colonne de 1 selon la transforma- tion 5.5. l’expression 10.25 peut se réécrire, en multipliant à gauche par (cid:0)λip + x (cid:62)x(cid:1), comme ainsi λ(cid:126)α = y − xx (cid:62)(cid:126)α et donc (cid:126)β ∗ = x (cid:62)(cid:126)α avec (cid:126)α = (cid:16) y − x (cid:126)β ∗(cid:17) . 1 λ (cid:126)α = (cid:16) λin + xx (cid:62)(cid:17)−1 y. l’équation 10.24 peut donc s’écrire f ((cid:126)x) = (cid:126)xx (cid:62)(cid:126)α = xx (cid:62)(λin + xx (cid:62))−1y. (10.26) soit maintenant k : x × x (cid:55)→ r un noyau. il existe un espace de hilbert h et une application φ : x → h telle que, pour tout (cid:126)x, (cid:126)x(cid:48) ∈ x × x , k((cid:126)x, (cid:126)x(cid:48)) = (cid:104)(cid:126)x, (cid:126)x(cid:48)(cid:105)h. \f10.4. régression ridge à noyau 129 appliquer la régression ridge aux images des observations dans h revient à calculer fφ((cid:126)x) = φ((cid:126)x)φ(cid:62)(λin + φφ(cid:62))−1y, où φ ∈ rn×d est la matrice dont la i-ème ligne est l’image par φ dans h (supposé ici de dimension d) de (cid:126)x i. on peut réécrire cette fonction comme fφ((cid:126)x) = κ(λin + k)−1y, (10.27) où κ ∈ rn est le vecteur dont la i-ème entrée est et k ∈ rn×n est la matrice dont l’entrée kil est κi = (cid:104)φ((cid:126)x), φ((cid:126)x i)(cid:105)h = k((cid:126)x, (cid:126)x i) kil = (cid:104)φ((cid:126)x i), φ((cid:126)x l)(cid:105)h = k((cid:126)x i, (cid:126)x l). il n’est donc pas nécessaire d’utiliser φ et h explicitement pour entraîner une régression ridge dans (cid:3) l’espace de redescription h déﬁni par un noyau. points clefs — la svm à marge souple est un algorithme linéaire de classiﬁcation binaire qui cherche à maximiser la séparation entre les classes, formalisée par le concept de marge, tout en contrôlant le nombre d’erreurs de classiﬁcation sur le jeu d’entraînement. — l’astuce du noyau permet d’appliquer eﬃcacement cet algorithme, ainsi que d’autres algorithmes linéaires comme la régression ridge, dans un espace de redescription, sans calculer explicitement l’image des observations dans cet espace. — les noyaux quadratique, polynomial et radial gaussien permettent d’utiliser des espaces de redes- cription de dimensions de plus en plus grandes. pour aller plus loin • le site web http://www.kernel-machines.org propose de nombreuses ressources bibliographiques et logicelles autour des méthodes à noyaux. • de nombreux ouvrages ont été consacrés aux svm ; parmi eux, on pourra se référer à schölkopf et smola (2002), ainsi qu’au chapitre sur les svm de cherkassky et mulier (1998), à vert et al. (2004), ou au tutoriel de burges (1998). • les svm ont été étendues aux problèmes de régression. il s’agit alors d’un problème de régression linéaire régularisée par une norme (cid:96)2, mais avec une perte (cid:15)-insensible (voir section 2.4.3) comme fonction de coût. elles sont alors parfois appelées svr pour support vector regression. pour plus de détails, on pourra se référer au tutoriel de smola et schölkopf (1998). • une alternative aux solveurs d’optimisation quadratique générique est l’algorithme smo (sequen- tial minimal optimization). proposé par john platt (1998), il a participé à l’engouement pour les svm en accélérant leur optimisation. c’est cet algorithme qui est implémentée dans la librairie libsvm (chang et lin, 2008), qui reste une des plus utilisées pour entraîner les svm ; scikit-learn ou shogun, par exemple, la réutilisent. \f130 chapitre 10. machines à vecteurs de support et méthodes à noyaux • la fonction de décision des svm ne modélise pas une probabilité. platt (1999) propose d’apprendre une fonction sigmoïde qui convertit ces prédictions en probabilité. • il est possible d’étendre les svm aﬁn de modéliser des observations appartenant à une seule et même classe (sans exemples négatifs). cette méthode permet ensuite de classiﬁer de nouvelles observa- tions selon qu’elles appartiennent ou non à la classe, et ainsi être utilisées pour détecter des anoma- lies, autrement dit des observations qui n’appartiennent pas à la classe. il en existe deux variantes : celle appelée one-class svm (schölkopf et al., 2000), qui sépare les observations de l’origine ; et celle appelée support vector data description (tax et duin, 2004), qui permet de trouver une frontière sphé- rique (dans l’espace de redescription, dans le cas de la version à noyau) qui enveloppe de près les données. • pour plus de détails sur les noyaux pour chaînes de caractères et leur utilisation en bioinformatique, on pourra se référer au chapitre de saunders et demco (2007). en ce qui concerne les noyaux pour graphe, le chapitre 2 de la thèse de benoît gaüzère (2013) donne un bon aperçu du domaine. bibliographie aronszajn, n. (1950). theory of reproducing kernels. transactions of the american mathematical society, 68(3) :337–404. boser, b. e., guyon, i. m., and vapnik, v. n. (1992). a training algorithm for optimal margin classiﬁers. in proceedings of the fifth annual workshop on computational learning theory, pages 144–152, pittsburgh, pennsylvania, united states. acm. burges, c. j. c. (1998). a tutorial on support vector machines for pattern recognition. data mining and knowledge discovery, 2 :121–167. chang, c.-c. et lin, c.-j. (2008). libsvm : a library for support vector machines. http://www.csie.ntu. edu.tw/~cjlin/libsvm/. cherkassky, v. et mulier, f. (1998). learning from data : concepts, theory, and methods. wiley, new york. cortes, c. et vapnik, v. (1995). support vector networks. machine learning, 20 :273–297. gaüzère, b. (2013). application des méthodes à noyaux sur graphes pour la prédiction des propriétés des molécules. thèse de doctorat, université de caen. gower, j. c. (1971). a general coeﬃcient of similarity and some of its properties. biometrics, pages 857–871. platt, j. c. (1998). sequential minimal optimization : a fast algorithm for training support vector machines. technical report msr-tr-98-14, microsoft research. platt, j. c. (1999). probabilities for support vector machines. in advances in large margin classiﬁers, pages 61–74. mit press, cambridge, ma. saunders, c. et demco, a. (2007). kernels for strings and graphs. in perspectives of neural-symbolic integration, studies in computational intelligence, pages 7–22. springer, berlin, heidelberg. \f131 schölkopf, b., williamson, r. c., smola, a. j., shawe-taylor, j., et platt, j. c. (2000). support vector method for novelty detection. in advances in neural information processing systems, pages 582–588. schölkopf, b. et smola, a. j. (2002). learning with kernels : support vector machines, regularization, optimization, and beyond. mit press, cambridge, ma. smola, a. j. et schölkopf, b. (1998). a tutorial on support vector regression. neurocolt, tr-1998-030. tax, d. m. et duin, r. p. (2004). support vector data description. machine learning, 54(1) :45–66. vapnik, v. et lerner, a. (1963). pattern recognition using generalized portrait method. automation and remote control, 24. vert, j.-p., tsuda, k., et schölkopf, b. (2004). a primer on kernel methods. in kernel methods in computational biology, pages 35–70. mit press, cambridge, ma. \fchapitre 11 réduction de dimension dans certaines applications, le nombre de variables p utilisé pour représenter les données est très élevé. c’est le cas par exemple du traitement d’images haute-résolution, dans lequel chaque pixel peut être re- présenté par plusieurs variables, ou de l’analyse de données génomiques, où des centaines de milliers de positions du génome peuvent être caractérisées. bien qu’une représentation des données contenant plus de variables soit intuitivement plus riches, il est plus diﬃcile d’apprendre un modèle performant dans ces circonstances. dans ce chapitre, nous en détaillerons les raisons avant d’explorer une série de méthodes, supervisées ou non, pour réduire ce nombre de variables aﬁn d’obtenir des représentations plus compactes et des modèles plus robustes. objectifs — expliquer quel peut être l’intérêt de réduire la dimension d’un jeu de données ; — faire la diﬀérence entre la sélection de variables et l’extraction de variables ; — connaître les principales techniques pour ces deux approches ; — comprendre l’analyse en composantes principales à partir d’une déﬁnition de maximisation de la variance et comme une technique de factorisation de matrice. 11.1 motivation le but de la réduction de dimension est de transformer une représentation x ∈ rn×p des données en une représentation x ∗ ∈ rn×m où m (cid:28) p. les raisons de cette démarche sont multiples, et nous les détaillons dans cette section. 11.1.1 visualiser les données bien que le but de l’apprentissage automatique soit justement d’automatiser le traitement des données, il est essentiel pour appliquer les méthodes présentées dans cet ouvrage à bon escient de bien comprendre le problème particulier que l’on cherche à résoudre et les données dont on dispose. cela permet de mieux déﬁnir les variables à utiliser, d’éliminer éventuellement les données aberrantes, et de guider le choix des 132 \f11.1. motivation 133 algorithmes. dans ce but, il est très utile de pouvoir visualiser les données ; mais ce n’est pas tâche aisée avec p variables. ainsi, limiter les variables à un faible nombre de dimensions permet de visualiser les données plus facilement, quitte à perdre un peu d’information lors de la transformation. 11.1.2 réduire les coûts algorithmiques cet aspect-là est purement computationnel : réduire la dimension des données permet de réduire d’une part l’espace qu’elles prennent en mémoire et d’autre part les temps de calcul. de plus, si certaines variables sont inutiles, ou redondantes, il n’est pas nécessaire de les obtenir pour de nouvelles observations : cela permet de réduire le coût d’acquisition des données. 11.1.3 améliorer la qualité des modèles notre motivation principale pour réduire la dimension des données est de pouvoir construire de meilleurs modèles d’apprentissage supervisé. par exemple, un modèle paramétrique construit sur un plus faible nombre de variables est moins susceptible de sur-apprendre, comme nous l’avons vu avec le lasso au cha- pitre 6. de plus, si certaines des variables mesurées ne sont pas pertinentes pour le problème d’apprentissage qui nous intéresse, elles peuvent induire l’algorithme d’apprentissage en erreur. prenons comme exemple l’algorithme des plus proches voisins (chapitre 8), qui base ses prédictions sur les étiquettes des exemples d’entraînement les plus proches de l’observation à étiqueter. supposons que l’on utilise une distance de minkowski ; toutes les variables ont alors la même importance dans le calcul des plus proches voisins. ainsi les variables qui ne sont pas pertinentes peuvent biaiser la déﬁnition du plus proche voisin, et introduire du bruit dans le modèle, comme illustré sur la ﬁgure 11.1. figure 11.1 – en utilisant les deux dimensions, les trois plus proches voisins de l’étoile sont majoritairement des (x). en utilisant seulement la variable en abscisse, ses trois plus proches voisins sont majoritairement des (+). si la variable en ordonnée n’est pas pertinente, elle fausse le résultat de l’algorithme des trois plus proches voisins. enﬁn, nous faisons face en haute dimension à un phénomène connu sous le nom de ﬂéau de la dimen- sion, ou curse of dimensionality en anglais. ce terme qualiﬁe le fait que les intuitions développées en faible dimension ne s’appliquent pas nécessairement en haute dimension. \f134 chapitre 11. réduction de dimension en eﬀet, en haute dimension, les exemples d’apprentissage ont tendance à tous être éloignés les uns des autres. pour comprendre cette assertion, plaçons-nous en dimension p et considérons l’hypersphère centrée sur une observation (cid:126)x, ainsi que l’hypercube c((cid:126)x, r) circonscrit à cette s((cid:126)x, r) de rayon r ∈ r∗ + hypersphère. le volume de s((cid:126)x) vaut 2rpπp/2 , tandis que celui de c((cid:126)x, r), dont le côté a pour longueur pγ(p/2) 2r, vaut 2prp. ainsi lim p→∞ vol(c((cid:126)x, r)) vol(s((cid:126)x, r)) = 0. cela signiﬁe que la probabilité qu’un exemple situé dans c((cid:126)x, r) appartienne à s((cid:126)x, r), qui vaut π lorsque p = 2 et π ont tendance à être éloignées les unes des autres. 4 ≈ 0.79 6 ≈ 0.52 lorsque p = 3 (cf. ﬁgure 11.2), devient très faible quand p est grand : les données cela implique que les algorithmes développés en utilisant une notion de similarité, comme les plus proches voisins, les arbres de décision ou les svm, ne fonctionnent pas nécessairement en grande dimen- sion. ainsi, réduire la dimension peut être nécessaire à la construction de bons modèles. figure 11.2 – ici en dimension 3, on considère la proportion du volume du cube de côté a = 2r située à l’intérieur de la sphère de rayon r inscrite dans ce cube. deux possibilités s’oﬀrent à nous pour réduire la dimension de nos données : — la sélection de variables, qui consiste à éliminer un nombre p − m de variables de nos données ; — l’extraction de variables, qui consiste à créer m nouvelles variables à partir des p variables dont nous disposons initialement. la suite de ce chapitre détaille ces deux approches. 11.2 sélection de variables les approches de sélection de variables consistent à choisir m variables à conserver parmi les p existantes, et à ignorer les p−m autres. on peut les séparer en trois catégories : les méthodes de ﬁltrage, les méthodes de conteneur, et les méthodes embarquées. les méthodes que nous allons voir ici sont des méthodes supervisées : nous supposons disposer d’un jeu de données étiqueté d = {((cid:126)x i, yi)}i=1,...,n où (cid:126)x i ∈ rp. 11.2.1 méthodes de ﬁltrage la sélection de variable par ﬁltrage consiste à appliquer un critère de sélection indépendamment à cha- cune des p variables ; il s’agit de quantiﬁer la pertinence de la p-ème variable du jeu de donnée par rapport à y. quelques exemples d’un tel critère sont la corrélation avec l’étiquette, un test statistique comme celui du χ2 dans le cas d’un problème de classiﬁcation, ou l’information mutuelle. \f11.2. sélection de variables 135 la corrélation entre la variable j et l’étiquette y se calcule comme celle entre une étiquette prédite et une étiquette réelle (cf. équation 3.2) : rj = (cid:80)n i=1 (cid:0)yi − 1 n (cid:80)n (cid:113) (cid:80)n i=1 (cid:0)yi − 1 n (cid:80)n i=1 yi(cid:1)2 i=1 yi(cid:1) (cid:16) (cid:114) j − 1 xi n (cid:16) (cid:80)n i=1 (cid:80)n (cid:17) i=1 f ((cid:126)x i) j − 1 xi n (cid:80)n l=1 xl j (cid:17)2 . (11.1) déﬁnition 11.1 (information mutuelle) l’information mutuelle entre deux variables aléatoires xj et y mesure leur dépendance au sens probabiliste ; elle est nulle si et seulement si les variables sont indépen- dantes, et croît avec leur degré de dépendance. elle est déﬁnie, dans le cas discret, par i(xj, y ) = (cid:88) xj ,y p(xj = xj, y = y) log p(xj = xj, y = y) p(xj = xj)p(y = y) , et dans le cas continu par i(xj, y ) = (cid:90) (cid:90) xj y p(xj, y) log p(xj, y) p(xj)p(y) dxj dy. (cid:4) l’estimateur de kozachenko-leonenko est l’un des plus fréquemment utilisés pour estimer l’informa- tion mutuelle (kozachenko et leonenko, 1987). les méthodes de ﬁltrage souﬀrent de traiter les variables individuellement : elles ne peuvent pas prendre en compte leurs eﬀets combinés. un exemple classique pour illustrer ce problème est celui du « ou exclu- sif » (xor) : prise individuellement, x1 (resp. x2) est décorrélée de y = x1 xor x2, alors qu’ensemble, ces deux variables expliqueraient parfaitement l’étiquette y. 11.2.2 méthodes de conteneur les méthodes de conteneur, ou wrapper methods en anglais, consistent à essayer de déterminer le meilleur sous-ensemble de variables pour un algorithme d’apprentissage donné. on parle alors souvent non pas de sé- lection de variables mais de sélection de sous-ensemble, ou subset selection en anglais. ces méthodes sont le plus souvent des méthodes heuristiques. en eﬀet, il est généralement impossible de déterminer la performance d’un algorithme sur un jeu de données sans l’avoir entraîné. il n’est pas raisonnable d’adopter une stratégie exhaustive, sauf pour un très faible nombre de variables, car le nombre de sous-ensembles de p variables est de 2p − 1 (en excluant ∅). on adoptera donc une approche gloutonne. quelques exemples de ces approches sont la recherche ascendante, la recherche descendante, et la recherche ﬂottante que nous détaillons ci-dessous. étant donnés un jeu de données d = (x, (cid:126)y) où x ∈ rn×p, un sous-ensemble de variables e ⊂ {1, 2, . . . , p}, et un algorithme d’apprentissage, on notera xe ∈ rn×|e| la matrice x restreinte aux va- riables apparaissant dans e, et ed(e) l’estimation de l’erreur de généralisation de cet algorithme d’ap- prentissage, entraîné sur (xe , (cid:126)y). cette estimation est généralement obtenue sur un jeu de test ou par validation croisée. la recherche ascendante consiste à partir d’un ensemble de variables vide, et à lui ajouter variable par variable celle qui améliore au mieux sa performance, jusqu’à ne plus pouvoir l’améliorer. déﬁnition 11.2 (recherche ascendante) on appelle recherche ascendante, ou forward search en anglais, la procédure gloutonne de sélection de variables suivante : \f136 chapitre 11. réduction de dimension 1. initialiser f = ∅ 2. trouver la meilleure variable à ajouter à f : j∗ = arg min j∈{1,...,p}\\f ed(f ∪ {j}) 3. si ed(f ∪ {j}) > ed(f) : s’arrêter sinon : f ← f ∪ {j} ; recommencer 2-3. dans le pire des cas (celui où on devra itérer jusqu’à ce que f = {1, 2, . . . , p}), cet algorithme requiert de l’ordre de o(p2) évaluations de l’algorithme d’apprentissage sur un jeu de données, ce qui peut être intensif, mais est bien plus eﬃcace que o(2p) comme requis par l’approche exhaustive. à l’inverse, la recherche descendante consiste à partir de l’ensemble de toutes les variables, et à lui retirer variable par variable celle qui améliore au mieux sa performance, jusqu’à ne plus pouvoir l’améliorer. déﬁnition 11.3 (recherche descendante) on appelle recherche descendante, ou backward search en an- glais, la procédure gloutonne de sélection de variables suivante : (cid:4) 1. initialiser f = {1, . . . , p} 2. trouver la meilleure variable à retirer de f : j∗ = arg min j∈f ed(f \\ {j}) 3. si ed(f \\ {j}) > ed(f) : s’arrêter sinon : f ← f \\ {j} ; recommencer 2-3. (cid:4) l’avantage de l’approche descendante sur l’approche ascendante est qu’elle fournit nécessairement un sous-ensemble de variables meilleur que l’intégralité des variables. en eﬀet, ce n’est pas parce qu’on ne peut pas, à une étape de la méthode ascendante, trouver de variable à ajouter à f, que la performance de l’algorithme est meilleure sur (xf , (cid:126)y) que sur (x, (cid:126)y). la recherche ﬂottante permet d’explorer un peu diﬀéremment l’espace des possibles en combinant les deux approches. déﬁnition 11.4 (recherche ﬂottante) étant donné deux paramètres entiers strictement positifs q et r, on appelle recherche ﬂottante, ou ﬂoating search en anglais, la procédure gloutonne de sélection de variables suivante : 1. initialiser f = ∅ 2. trouver les q meilleures variables à ajouter à f : s ∗ = arg min s⊆{1,...,p}\\f , |s|=q ed(f ∪ s) 3. si ed(f ∪ s) < ed(f) : f ← f ∪ s 4. trouver les r meilleures variables à retirer de f : s ∗ = arg min s⊆{1,...,p}\\f , |s|=r ed(f \\ s) 5. si ed(f \\ s) > ed(f) : s’arrêter sinon : f ← f \\ s ; recommencer 2-5. (cid:4) \f11.3. extraction de variables 11.2.3 méthodes embarquées 137 enﬁn, les méthodes embarquées, ou embedded approaches en anglais, apprennent en même temps que le modèle les variables à y inclure. il s’agit généralement de modèles paramétriques parcimonieux, c’est-à- dire tels que les coeﬃcients aﬀectés à certaines variables soient nuls. ces variables sont alors éliminées du modèle. l’exemple le plus prégnant de ces techniques est le lasso, que nous avons vu dans la section 6.3. dans le même ordre d’idée, la mesure d’importance des variables dans les forêts aléatoires (voir cha- pitre 9) peut être utilisée pour décider quelles variables éliminer. 11.3 extraction de variables 11.3.1 analyse en composantes principales la méthode la plus classique pour réduire la dimension d’un jeu de données par extraction de variables est l’analyse en composantes principales, ou acp. on parle aussi souvent de pca, de son nom anglais principal component analysis. maximisation de la variance l’idée centrale d’une acp est de représenter les données de sorte à maximiser leur variance selon les nouvelles dimensions, aﬁn de pouvoir continuer à distinguer les exemples les uns des autres dans leur nouvelle représentation (cf. ﬁgure 11.3). figure 11.3 – la variance des données est maximale selon l’axe indiqué par une ﬂèche. formellement, une nouvelle représentation de x est déﬁnie par une base orthonormée sur laquelle projeter la matrice de données x. déﬁnition 11.5 (analyse en composantes principales) une analyse en composantes principales, ou acp, de la matrice x ∈ rn×p est une transformation linéaire orthogonale qui permet d’exprimer x dans une nouvelle base orthonormée, de sorte que la plus grande variance de x par projection s’aligne sur le premier axe de cette nouvelle base, la seconde plus grande variance sur le deuxième axe, et ainsi de suite. les axes de cette nouvelle base sont appelés les composantes principales, abrégées en pc pour principal (cid:4) components. \f138 chapitre 11. réduction de dimension attention dans la suite de cette section, nous supposons que les variables ont été standardisées de sorte à toutes avoir une moyenne de 0 et une variance de 1, pour éviter que les variables qui prennent de grandes valeurs aient plus d’importance que celles qui prennent de faibles valeurs. c’est un pré-requis de l’application de l’acp. cette standardisation s’eﬀectue en centrant la moyenne et en réduisant la variance de chaque va- riable : xi j ← (cid:113) 1 n xi j − ¯xj (cid:80)n l=1(xl j − ¯xj)2 , (11.2) où ¯xj = 1 n (cid:80)n l=1 xl j. on dira alors que x est centrée : chacune de ses colonnes a pour moyenne 0. théorème 11.1 soit x ∈ rn×p une matrice centrée de covariance σ = 1 de x sont les vecteurs propres de σ, ordonnés par valeur propre décroissante. n x (cid:62)x. les composantes principales (cid:4) démonstration. commençons par démontrer que, pour tout vecteur (cid:126)w ∈ rp, la variance de la pro- jection de x sur (cid:126)w vaut w(cid:62)σw. la projection de x ∈ rn×p sur (cid:126)w ∈ rp est le vecteur (cid:126)z = xw. comme x est centrée, la moyenne de (cid:126)z vaut sa variance vaut 1 n n (cid:88) i=1 zi = 1 n n (cid:88) p (cid:88) i=1 j=1 xi jwj = 1 n p (cid:88) wj n (cid:88) j=1 i=1 xi j = 0. var[(cid:126)z] = 1 n n (cid:88) i=1 zz2 i = 1 n (cid:126)w (cid:62)x (cid:62)x (cid:126)w = (cid:126)w (cid:62)σ (cid:126)w. appelons maintenant (cid:126)w1 ∈ rp la première composante principale. (cid:126)w1 est orthonormé et tel que la variance de x (cid:126)w1 soit maximale : (cid:126)w1 = arg max (cid:126)w∈rp (cid:126)w (cid:62)σ (cid:126)w avec || (cid:126)w1||2 = 1. (11.3) il s’agit d’un problème d’optimisation quadratique sous contrainte d’égalité (voir section a.4), que l’on peut résoudre en introduisant le multiplicateur de lagrange α1 > 0 et en écrivant le lagrangien l(α1, (cid:126)w) = (cid:126)w (cid:62)σ (cid:126)w − α1 (|| (cid:126)w||2 − 1) . par dualité forte, le maximum de (cid:126)w (cid:62)σ (cid:126)w sous la contrainte || (cid:126)w1||2 = 1 est égal à minα1 sup (cid:126)w∈rp l(α1, (cid:126)w). le supremum du lagrangien est atteint en un point où son gradient s’annule, c’est-à-dire qui vériﬁe 2σ (cid:126)w − 2α1 (cid:126)w = 0. ainsi, σ (cid:126)w1 = α1 (cid:126)w1 et (α1, (cid:126)w1) forment un couple (valeur propre, vecteur propre) de σ. parmi tous les vecteurs propres de σ, (cid:126)w1 est celui qui maximise la variance (cid:126)w(cid:62) 1 σ (cid:126)w1 = α1 || (cid:126)w1||2 = α1. ainsi, α1 est la plus grande valeur propre de σ (rappelons que σ étant déﬁnie par x (cid:62)x est semi-déﬁnie positive et que toutes ses valeurs propres sont positives.) la deuxième composante principale de x vériﬁe (cid:126)w2 = arg max (cid:126)w∈rp (cid:126)w (cid:62)σ (cid:126)w avec || (cid:126)w2||2 = 1 et (cid:126)w (cid:62) (cid:126)w1 = 0. (11.4) \f11.3. extraction de variables 139 cette dernière contrainte nous permet de garantir que la base des composantes principales est orthonor- mée. nous introduisons donc maintenant deux multiplicateurs de lagrange α2 > 0 et β2 > 0 et obtenons le lagrangien l(α2, β2, (cid:126)w) = (cid:126)w (cid:62)σ (cid:126)w − α2 (cid:16) || (cid:126)w||2 2 − 1 (cid:17) − β2 (cid:126)w (cid:62) (cid:126)w1. comme précédemment, son supremum en (cid:126)w est atteint en un point où son gradient s’annule : en multipliant à gauche par (cid:126)w(cid:62) 1 , on obtient 2σ (cid:126)w2 − 2α2 (cid:126)w2 − β2 (cid:126)w1 = 0. 2 (cid:126)w(cid:62) 1 σ (cid:126)w2 − 2α2 (cid:126)w(cid:62) 1 (cid:126)w2 − β2 (cid:126)w(cid:62) 1 (cid:126)w1 = 0 d’où l’on conclut que β2 = 0 et, en remplaçant dans l’équation précédente, que, comme pour (cid:126)w1, 2σ (cid:126)w2 − 2α1 (cid:126)w2 = 0. ainsi (α2, (cid:126)w2) forment un couple (valeur propre, vecteur propre) de σ et α2 est maximale : il s’agit donc nécessairement de la deuxième valeur propre de σ. le raisonnement se poursuit de la même manière pour les composantes principales suivantes. (cid:3) alternativement, on peut prouver le théorème 11.1 en observant que σ, qui est par construction déﬁnie positive, est diagonalisable par un changement de base orthonormée : σ = q(cid:62)λq, où λ ∈ rp×p est une matrice diagonale dont les valeurs diagonales sont les valeurs propres de σ. ainsi, remarque (cid:126)w(cid:62) 1 σ (cid:126)w1 = (cid:126)w(cid:62) 1 q(cid:62)λq (cid:126)w1 = (q (cid:126)w1)(cid:62) λ (q (cid:126)w1) . j=1 v2 si l’on pose (cid:126)v = q (cid:126)w1, il s’agit donc de trouver (cid:126)v de norme 1 (q étant orthonormée et (cid:126)w1 de norme 1) qui maximise (cid:80)p j λj. comme σ est déﬁnie positive, λj ≥ 0 ∀j = 1, . . . , p. de plus, ||(cid:126)v||2 = 1 implique que j ≤ maxj=1,...,p λj et ce maximum 0 ≤ v1 j λj ≤ maxj=1,...,p λj est atteint quand vj = 1 et vk = 0 ∀k (cid:54)= j. on retrouve ainsi que (cid:126)w1 est le vecteur propre correspondant à la plus grande valeur propre de σ, et ainsi de suite. j ≤ 1 ∀j = 1, . . . , p. ainsi, (cid:80)p j=1 v2 j=1 v2 (cid:80)p décomposition en valeurs singulières théorème 11.2 soit x ∈ rn×p une matrice centrée. les composantes principales de x sont ses vecteurs (cid:4) singuliers à droite ordonnés par valeur singulière décroissante. démonstration. si l’on écrit x sous la forme u dv (cid:62) où u ∈ rn×n et v ∈ rp×p sont orthogonales, et d ∈ rn×p est diagonale, alors σ = x (cid:62)x = v du (cid:62)u dv (cid:62) = v d2v (cid:62) et les valeurs singulières de x (les entrées de d) sont les racines carrées des valeurs propres de σ, tandis (cid:3) que les vecteurs singuliers à droite de x (les colonnes de v ) sont les vecteurs propres de σ. en pratique, les implémentations de la décomposition en valeurs singulières (ou svd) sont numéri- quement plus stables que celles de décomposition spectrale. on préfèrera donc calculer les composantes principales de x en calculant la svd de x plutôt que la décomposition spectrale de x (cid:62)x. \f140 chapitre 11. réduction de dimension choix du nombre de composantes principales réduire la dimension des données par une acp implique de choisir un nombre de composantes princi- pales à conserver. pour ce faire, on utilise la proportion de variance expliquée par ces composantes : la variance de x s’exprime comme la trace de σ, qui est elle-même la somme de ses valeurs propres. ainsi, si l’on décide de conserver les m premières composantes principales de x, la proportion de va- riance qu’elles expliquent est α1 + α2 + · · · + αm tr(σ) où α1 ≥ α2 ≥ · · · ≥ αp sont les valeurs propres de σ par ordre décroissant. il est classique de s’intéresser à l’évolution, avec le nombre de composantes, soit de la proportion de variance expliquée par chacune d’entre elles, soit à cette proportion cumulée, que l’on peut représenter visuellement sur un scree plot (ﬁgure 11.4a). ce graphe peut nous servir à déterminer : — soit le nombre de composantes principales qui explique un pourcentage de la variance que l’on s’est initialement ﬁxé (par exemple, sur la ﬁgure 11.4b, 95%) ; — soit le nombre de composantes principales correspondant au « coude » du graphe, à partir duquel ajouter une nouvelle composante principale ne semble plus informatif. (a) pourcentage de variance expliqué par chacune des composantes principales. à partir de 6 composantes principales, ajouter de nouvelles composantes n’est plus vraiment informatif. (b) pourcentage cumulé de variance expliquée par cha- cune des composantes principales. si on se ﬁxe une proportion de variance expliquée de 95%, on peut se contenter de 10 composantes principales. figure 11.4 – pour choisir le nombre de composantes principales, on utilise le pourcentage de variance expliquée. 11.3.2 factorisation de la matrice des données soit un nombre m de composantes principales, calculées par une acp, représentées par une matrice w ∈ rp×m. la représentation réduite des n observations dans le nouvel espace de dimension m s’obtient en projetant x sur les colonnes de w , autrement dit en calculant la matrice h ∈ rm×n peut être interprétée comme une représentation latente (ou cachée, hidden en h = w (cid:62)x. (11.5) \f11.3. extraction de variables 141 anglais d’où la notation h) des données. c’est cette représentation que l’on a cherché à découvrir grâce à l’acp. les colonnes de w étant des vecteurs orthonormés (il s’agit de vecteurs propres de xx (cid:62)), on peut multiplier l’équation 11.5 à gauche par w pour obtenir une factorisation de x : x = w h. (11.6) on appelle ainsi les lignes de h les facteurs de x. analyse factorielle cette factorisation s’inscrit dans le cadre plus général de l’analyse factorielle. supposons que les observations {(cid:126)x 1, (cid:126)x 2, . . . , (cid:126)x n} soient les réalisations d’une variable aléatoire p- dimensionnelle (cid:126)x, générée par le modèle (cid:126)x = w(cid:126)h + (cid:15), (11.7) où (cid:126)h est une variable aléatoire m-dimensionnelle qui est la représentation latente de (cid:126)x, et (cid:15) un bruit gaus- sien : (cid:15) ∼ n (0, ψ). supposons les données centrées en 0, et les variables latentes (cid:126)h 1, (cid:126)h 2, . . . ,(cid:126)h n (qui sont des réalisa- tions de (cid:126)h) indépendantes et gaussiennes de variance unitaire, c’est-à-dire (cid:126)h ∼ n (0, im) où im est la matrice identité de dimensions m × m. alors w(cid:126)h est centrée en 0, et sa covariance est w w (cid:62). alors (cid:126)x ∼ n (0, w w (cid:62) + ψ). si on considère de plus que (cid:15) est isotropique, autrement dit que ψ = σ2ip, alors (cid:126)x ∼ n (0, w w (cid:62) + σ2ip), on obtient ce que l’on appelle acp probabiliste (tipping et bishop, 1999). les paramètres w et σ de ce modèle peuvent être obtenus par maximum de vraisemblance. l’acp classique est un cas limite de l’acp probabiliste, obtenu quand la covariance du bruit devient inﬁniment petite (σ2 → 0). plus généralement, on peut faire l’hypothèse que les variables observées x1, x2, . . . , xp sont condition- nellement indépendantes étant données les variables latentes h1, h2, . . . , hm. dans ce cas, ψ est une ma- trice diagonale, ψ = diag(ψ1, ψ2, . . . , ψp), où ψj décrit la variance spéciﬁque à la variable xj. les valeurs de w , σ et ψ1, ψ2, . . . , ψp peuvent encore une fois être obtenues par maximum de vraisemblance. c’est ce que l’on appelle l’analyse factorielle. dans l’analyse factorielle, nous ne faisons plus l’hypothèse que les nouvelles dimensions sont orthogo- nales. en particulier, il est donc possible d’obtenir des dimensions dégénérées, autrement dit des colonnes de w dont toutes les coordonnées sont 0. factorisation positive de matrices dans le cas où les valeurs de x sont toutes positives, il peut être plus interprétable de chercher une fac- torisation similaire à l’équation 11.6, mais telle que w et h soient toutes les deux aussi à valeurs positives. dans ce cas, on parle de factorisation positive de matrice (ou nmf de l’anglais non-negative matrix factori- zation). le problème est alors formalisé comme arg min + ,h∈rm×n + w ∈rp×m ||x − w h||2 f , (11.8) où ||.||f désigne la norme de frobenius d’une matrice, autrement dit la racine carrée de la somme des carrés de ses entrées. ainsi, ||x − w h||2 f compare les matrices x et de w h entrée par entrée. le problème 11.8 peut être résolu par un algorithme à directions de descente (voir section a.3.3). \f142 chapitre 11. réduction de dimension exemple cette technique peut être appliquée, par exemple, dans l’analyse des notes données par des spectateurs à diﬀérents ﬁlms. supposons avoir n spectateurs qui ont donné des notes entre 0 et 5 à p ﬁlms ; une facto- ) comme la combinaison de l’importance risation non négative permet d’interpréter une de ces notes (xi j pour le spectateur i des m aspects d’un ﬁlm (donnée par (cid:126)h i ∈ rm ), et de la présence de chacun de ces + aspects dans le ﬁlm (donnée par la j-ème ligne de w ). remarque cette technique peut aussi être utilisée pour faire de la complétion de matrice, c’est-à-dire dans le cas où x a des valeurs manquantes (tous les spectateurs n’ont pas noté tous les ﬁlms). plutôt que de chercher à minimiser ||x − w h||2 , on restreint cette somme aux entrées connues de x : le problème 11.8 devient f alors arg min + ,h∈rm×n + w ∈rp×m i,jnon manquants (cid:88) (xij − (w h)ij)2 . (11.9) ce problème se résout aussi par un algorithme à directions de descente (voir section a.3.3), et on peut alors approcher x par le produit w h. cela permet de prédire les entrées manquantes de x par les entrées correspondantes de w h. cette technique est utilisée dans le cas du ﬁltrage collaboratif (voir section 8.4), où les colonnes de x correspondent à des utilisateurs et ses lignes à des produits. 11.3.3 auto-encodeurs nous avons vu à la section 7.2.5 qu’il est possible d’interpréter les réseaux de neurones artiﬁciels comme un outil pour apprendre une représentation des données (celle issue de la dernière couche intermédiaire) qui se prête bien à un apprentissage supervisé linéaire. cette idée a conduit à la notion d’auto-encodeur, qui permet d’utiliser les réseaux de neurones pour faire de la réduction de dimension non supervisée. déﬁnition 11.6 (autoencodeur) on appelle auto-encodeur un réseau de neurones dont la couche de sortie est identique à la couche d’entrée, en passant par une couche intermédiaire de taille inférieure à celles-ci. la sortie de cette couche intermédiaire constitue alors une nouvelle représentation plus compacte (cid:4) des données. bien qu’il existe des auto-encodeurs linéaires, on leur préfère généralement les machines de boltzmann restreintes, ou rbm pour restricted boltzmann machines. elles ont été proposées par paul smolensky (1986). une machine de boltzmann restreinte contient deux couches de neurones, comme illustré sur la ﬁ- gure 11.5. la première couche contient p neurones, une par variable xj représentant les données ; la deuxième en contient m, où m < p. le vecteur (cid:126)z = (z1, z2, . . . , zm) en sortie de cette couche constitue une re- présentation réduite de (cid:126)x. nous supposerons ces variables binaires, à valeur dans {0, 1} : elles peuvent correspondre à la couleur d’un pixel sur une image en noir et blanc. cette architecture est dite restreinte car il n’y a pas de connexions entre neurones d’une même couche, contrairement aux machines de boltzmann proposées précédemment et utilisées dans le cadre de la phy- sique statistique. par contre, les connexions entre les deux couches vont dans les deux sens : de la couche d’entrée vers la couche intermédiaire, et aussi de la couche intermédiaire vers la couche d’entrée. les poids de connexion seront choisis de sorte à minimiser la diﬀérence entre l’entrée et la sortie, obtenue après un passage vers l’avant puis un retour vers l’arrière dans le réseau. \f11.3. extraction de variables 143 figure 11.5 – architecture d’une machine de boltzmann restreinte qui apprend une représentation en m dimensions de données en dimension p. la particularité des machines de boltzmann restreintes est d’utiliser des fonctions d’activation stochas- tique :  p(zq = 1|(cid:126)x) = σ bq +  wjqxj  , p (cid:88) j=1 où bq est le biais du q-ème neurone intermédiaire, et  p(xj = 1|(cid:126)z) = σ aj +  wjqzq  , m (cid:88) q=1 où aj est le biais du j-ème neurone visible. en s’inspirant de la physique statistique, on peut alors déﬁnir l’énergie de ce modèle comme e((cid:126)x, (cid:126)z) = − p (cid:88) j=1 ajxj − m (cid:88) q=1 bhzh − p (cid:88) m (cid:88) j=1 q=1 xjwjqzq. (11.10) (11.11) (11.12) la loi de probabilité jointe de (cid:126)x et (cid:126)z selon ce modèle peut s’écrire comme celle d’une distribution de boltzmann en fonction de cette énergie : 1 z où la constante de normalisation z peut être vue comme une fonction de partition et s’écrit en sommant p((cid:126)x, (cid:126)z) sur tous les états possibles : p((cid:126)x, (cid:126)z) = e−e((cid:126)x,(cid:126)z) (11.13) z = (cid:88) e−e((cid:126)u,(cid:126)v). (cid:126)u∈{0,1}p,(cid:126)v∈{0,1}m (11.14) ainsi, nous pouvons apprendre les coeﬃcients {aj}j=1,...,p, {bq}q=1,...,m, et {wjq}j=1,...,p,q=1,...,m par maximum de vraisemblance. avantarrière\f144 chapitre 11. réduction de dimension pour une observation (cid:126)x i donnée, l’opposé du log de la vraisemblance vaut : − log p ((cid:126)x i) = − log p ((cid:126)x i, (cid:126)v) = log (cid:88) (cid:126)v (cid:88) (cid:126)u,(cid:126)v e−e((cid:126)u,(cid:126)v) − log (cid:88) e−e((cid:126)x i,(cid:126)v). (11.15) (cid:126)v il nous faut maintenant dériver cette expression par rapport à chacun des paramètres que l’on cherche à déterminer. en notant θ un de ces paramètres (ainsi θ peut indiﬀéremment représenter aj, bq, ou wjq), la dérivée partielle de − log p ((cid:126)x i) par rapport à θ est : ∂ − log p ((cid:126)x i) ∂θ = − 1 (cid:126)u,(cid:126)v e−e((cid:126)u,(cid:126)v) (cid:80) (cid:88) (cid:126)u,(cid:126)v ∂e((cid:126)u, (cid:126)v) ∂θ e−e((cid:126)u,(cid:126)v) + 1 (cid:126)v e−e((cid:126)x i,(cid:126)v) (cid:80) (cid:88) (cid:126)v ∂e((cid:126)x i, (cid:126)v) ∂θ e−e((cid:126)x i,(cid:126)v) = − (cid:88) (cid:126)u,(cid:126)v p((cid:126)u, (cid:126)v) ∂e((cid:126)u, (cid:126)v) ∂θ (cid:88) + p((cid:126)v|(cid:126)x i) (cid:126)v ∂e((cid:126)x i, (cid:126)v) ∂θ ce gradient se décompose donc en un gradient négatif, et un gradient positif, (cid:88) − p((cid:126)u, (cid:126)v) (cid:126)u,(cid:126)v ∂e((cid:126)u, (cid:126)v) ∂θ = −e (cid:21) (cid:20) ∂e((cid:126)u, (cid:126)v) ∂θ (cid:88) p((cid:126)v|(cid:126)x i) (cid:126)v ∂e((cid:126)x i, (cid:126)v) ∂θ = e (cid:20) ∂e((cid:126)x i, (cid:126)v) ∂θ (cid:21) . (11.16) (11.17) pour approcher ces espérances, on utilise une procédure d’échantillonnage de gibbs, qui consiste à itéra- tivement tirer une observation (cid:126)x i, faire une passe en avant pour obtenir (cid:126)z i, faire une passe en arrière pour obtenir la sortie (cid:126)x (cid:48)i, puis une dernière passe en avant pour obtenir (cid:126)z (cid:48)i, et approcher notre gradient par ∂e((cid:126)x i, (cid:126)z i) ∂θ les dérivées partielles de l’énergie e déﬁnie par l’équation 11.12 sont simples à calculer. ∂e((cid:126)x (cid:48)i, (cid:126)z (cid:48)i) ∂θ − . (11.18) on obtient ainsi l’algorithme dit de divergence contrastive pour entraîner une machine de boltzmann restreinte, et proposé initialement par geoﬀ hinton (2002). déﬁnition 11.7 (divergence contrastive) étant donnée une machine de boltzmann restreinte dont l’énergie est donnée par l’équation 11.12 et une vitesse d’apprentissage η, la procédure de divergence contras- tive, ou contrastive divergence en anglais, consiste à apprendre les poids aj, bq et wjq en itérant sur les ob- servations après avoir initialisé ces poids à une valeur aléatoire. pour une observation (cid:126)x i, on eﬀectue les étapes suivantes : 1. tirer un vecteur intermédiaire (cid:126)z i selon p((cid:126)z|(cid:126)x i) ; 2. calculer le gradient positif, qui vaut xi j 3. tirer une reconstruction (cid:126)x (cid:48)i de (cid:126)x i selon p((cid:126)x|(cid:126)z i) ; 4. tirer un vecteur intermédiaire (cid:126)z (cid:48)i selon p((cid:126)z|(cid:126)x (cid:48)i) ; 5. calculer le gradient négatif, qui vaut −x(cid:48)i j 6. mettre les poids à jour selon pour θ = aj, zi q pour θ = bq, et xi jzi q pour θ = wjq ; pour θ = aj, −z(cid:48)i q pour θ = bq, et −x(cid:48)i j z(cid:48)i q pour θ = wjq ; wjq ← wjq + η (cid:16) jzi xi q − x (cid:48)i j z (cid:48)i q (cid:17) ; aj ← aj + η (cid:16) xi j − x (cid:48)i j (cid:17) ; bq ← bq + η (cid:16) zi q − z (cid:48)i q (cid:17) . (cid:4) \f11.3. extraction de variables 145 remarque il est possible d’empiler les rbms pour obtenir une architecture profonde, appelée deep belief network (ou dbn). cette architecture permet d’eﬀectuer une réduction de dimension non supervisée, mais peut aussi être utilisée dans un cadre supervisé. il s’agit alors de connecter la dernière couche du dbn à une couche de sortie ; on obtiendra alors un réseau de neurones supervisé. les dbn, proposés par geoﬀ hinton et ruslan salakhutdinov, ﬁgurent parmi les premiers réseaux de neurones profonds réalisés (hinton et salakhutdinov, 2006). 11.3.4 autres approches non linéaires de nombreuses autres approches ont été proposées pour réduire la dimension des données de manière non linéaire. parmi elles, nous en abordons ici quelques-unes parmi les plus populaires ; les expliquer de manière détaillée dépasse le propos de cet ouvrage introductif. analyse en composantes principales à noyau nous commencerons par noter que l’analyse en composantes principales se prête à l’utilisation de l’as- tuce du noyau (cf. section 10.3.3). la méthode qui en résulte est appelée kernel pca, ou kpca. positionnement multidimensionnel le positionnement multidimensionnel, ou multidimensional scaling (mds) (cox et cox, 1994), se base sur une matrice de dissimilarité d ∈ rn×n entre les observations : il peut s’agir d’une distance métrique, mais ce n’est pas nécessaire. le but de l’algorithme est alors de trouver une représentation des données qui préserve cette dissimilarité : x ∗ = arg min z∈rn×m n (cid:88) n (cid:88) i=1 l=i+1 (cid:16)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)(cid:126)z i − (cid:126)z l(cid:12) (cid:12) (cid:12) (cid:12)2 (cid:12) (cid:17)2 . − dil (11.19) si l’on utilise la distance euclidienne comme dissimilarité, alors le mds est équivalent à une acp. le positionnement multidimensionnel peut aussi s’utiliser pour positionner dans un espace de dimen- sion m des points dont on ne connaît pas les coordonnées. il s’applique par exemple très bien à reposition- ner des villes sur une carte à partir uniquement des distances entre ces villes. une des limitations de mds est de ne chercher à conserver la distance entre les observations que globa- lement. une façon eﬃcace de construire la matrice de dissimilarité de mds de sorte à conserver la structure locale des données est l’algorithme isomap (tenenbaum et al., 2000). il s’agit de construire un graphe de voi- sinage entre les observations en reliant chacune d’entre elles à ses k plus proches observations voisines. ces arêtes peuvent être pondérée par la distance entre les observations qu’elles relient. une dissimilarité entre observations peut ensuite être calculée sur ce graphe de voisinage, par exemple via la longueur du plus court chemin entre deux points. t-sne enﬁn, l’algorithme t-sne, pour t-student neighborhood embedding, proposé en 2008 par laurens van der maaten and geoﬀ hinton, propose d’approcher la distribution des distances entre observations par une loi de student (van der maaten et hinton, 2008). pour chaque observation (cid:126)x i, on déﬁnit pi comme la loi de probabilité déﬁnie par pi((cid:126)x) = √ 1 2πσ2 (cid:32) exp − (cid:12)(cid:126)x − (cid:126)x i(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2σ2 2 2 (cid:33) . (11.20) \f146 t-sne consiste alors à résoudre chapitre 11. réduction de dimension arg min q n (cid:88) i=1 kl(pi||qi) (11.21) où kl dénote la divergence de kullback-leibler (voir section 2.4.1) et q est choisie parmi les distributions de student de dimension inférieure à p. attention, cet algorithme trouve un minimum local et non global, et on pourra donc obtenir des résultats diﬀérents en fonction de son initialisation. de plus, sa complexité est quadratique en le nombre d’observations. points clefs — réduire la dimension des données avant d’utiliser un algorithme d’apprentissage supervisé permet d’améliorer ses besoins en temps et en espace, mais aussi ses performances. — on distingue la sélection de variables, qui consiste à éliminer des variables redondantes ou peu infor- matives, de l’extraction de variable, qui consiste à générer une nouvelle représentation des données. — projeter les données sur un espace de dimension 2 grâce à, par exemple, une acp ou t-sne, permet de les visualiser. — de nombreuses méthodes permettent de réduire la dimension des variables. pour aller plus loin • le tutoriel de shlens (2014) est une introduction détaillée à l’analyse en composantes principales. • pour une revue des méthodes de sélection de variables, on pourra se réferer à guyon et elisseeﬀ (2003). • pour plus de détails sur la nmf, on pourra par exemple se tourner vers lee et seung (1999) • pour plus de détails sur les méthodes des sélection de sous-ensemble de variables (wrapper me- thods), on pourra se référer à l’ouvrage de miller (1990) • une page web est dédiée à isomap : http://web.mit.edu/cocosci/isomap/isomap.html. • pour plus de détails sur l’utilisation de t-sne, on pourra se référer à la page https://lvdmaaten. github.io/tsne/ ou à la publication interactive de wattenberg et al. (2016). bibliographie cox, t. f. et cox, m. a. a. (1994). multidimensional scaling. chapman and hall., london. guyon, i. et elisseeﬀ, a. (2003). an introduction to variable and feature selection. journal of machine learning research, 3 :1157–1182. hinton, g. e. (2002). training product of experts by minimizing contrastive divergence. neural computation, 14 :1771–1800. hinton, g. e. et salakhutdinov, r. r. (2006). reducing the dimensionality of data with neural networks. science, 313 :504–507. \f147 kozachenko, l. f. et leonenko, n. n. (1987). a statistical estimate for the entropy of a random vector. problemy peredachi informatsii, 23 :9–16. lee, d. d. et seung, h. s. (1999). learning the parts of objects by non-negative matrix factorization. nature, 401(6755) :788–791. miller, a. j. (1990). subset selection in regression. chapman and hall., london. shlens, j. (2014). a tutorial on principal component analysis. arxiv [cs, stat]. arxiv : 1404.1100. smolensky, p. (1986). information processing in dynamical systems : foundations of harmony theory. in pa- rallel distributed processing : explorations in the microstructure of cognition, volume 1 : foundations, chapter 6, pages 194–281. mit press, cambridge, ma. tenenbaum, j. b., de silva, v., et langford, j. c. (2000). a global geometric framework for nonlinear dimen- sionality reduction. science, 290(5500) :2319–2323. tipping, m. e. et bishop, c. m. (1999). probabilistic principal components analysis. statistical society series b, 61 :611–622. journal of the royal van der maaten, l. et hinton, g. (2008). visualizing data using t-sne. journal of machine learning research, 9 :2579–2605. wattenberg, m., viégas, f., et johnson, i. (2016). how to use t-sne eﬀectively. distill. http://distill. pub/2016/misread-tsne. \fchapitre 12 clustering comment étudier des données non étiquetées ? la dimension de réduction nous permet de les visuali- ser ; mais les méthodes dites de partitionnement de données, ou clustering, nous permettent d’aller beaucoup plus loin. il s’agit de séparer les données en sous-groupes homogènes, appelés clusters, qui partagent des caractéristiques communes. dans ce chapitre, nous détaillerons l’intérêt de ces techniques et verrons trois types d’approches de clustering : le clustering hiérarchique, le clustering par centroïdes, et le clustering par densité. objectifs — expliquer l’intérêt d’un algorithme de clustering — évaluer le résultat d’un algorithme de clustering — implémenter un clustering hiérarchique, un clustering par la méthode des k moyennes, et un clus- tering par densité. 12.1 pourquoi partitionner ses données les algorithmes de partitionnement de données permettent d’eﬀectuer une analyse exploratoire sur des données non étiquetées. ils permettent par exemple d’identiﬁer des utilisateurs qui ont des comporte- ments similaires (ce que l’on appelle la segmentation de marché), des communautés sur un réseau social, des motifs récurrents dans des transactions ﬁnancières, des pixels d’un même objet dans une image (segmen- tation d’image) ou des patients dont la maladie s’explique par un même proﬁl génétique. ils permettent aussi de visualiser les données, en se contentant de regarder un exemple représentatif par cluster. enﬁn, les algorithmes de clustering permettent de transférer à toutes les observations du même cluster les propriétés que l’on sait vraies de l’un des éléments de ce cluster. cela est particulièrement utile dans le cas où l’étiquetage des données est diﬃcile ou coûteux. 148 \f12.2. évaluer la qualité d’un algorithme de clustering 149 exemple prenons l’exemple de l’annotation d’un corpus de documents texte. annoter manuellement chacun de ces documents par le ou les sujets qu’il couvre est un travail fastidieux. les personnes qui l’eﬀectuent sont d’ailleurs susceptibles de commettre involontairement des erreurs d’inattention. il est ainsi moins coûteux, voire plus eﬃcace, d’utiliser un algorithme de clustering pour regrouper automatiquement ces documents par sujet. il suﬃra alors d’avoir recours à un intervenant humain pour assigner un sujet à chaque cluster en lisant uniquement un ou deux des documents qu’il contient. 12.2 évaluer la qualité d’un algorithme de clustering le clustering n’étant pas supervisé, il nous faut mettre au point des critères d’évaluation qui ne dé- pendent pas d’une vérité terrain (c’est-à-dire d’étiquettes connues). la tâche est plus délicate que dans le cadre de l’apprentissage supervisé, dans lequel le but à atteindre est beaucoup plus clair. cependant, elle n’est pas impossible, et il existe un large éventail de mesures de la performance d’un algorithme de partitionnement des données. par la suite, nous supposons avoir partitionné un jeu de données non étiqueté d = {(cid:126)x 1, (cid:126)x 2, . . . , (cid:126)x n} de n points d’un espace x en k clusters c1, c2, . . . , ck. d est une distance sur x et k((cid:126)x) est l’indice du cluster auquel (cid:126)x a été assigné. 12.2.1 la forme des clusters pour évaluer la qualité des clusters obtenus par un algorithme de partitionnement, il est possible de s’appuyer sur le souhait formulé de regrouper entre elles les observations similaires. ainsi, les observations appartenant à un même cluster doivent être proches, tandis que les observations dissimilaires doivent ap- partenir à des clusters diﬀérents. pour quantiﬁer ces notions, nous allons avoir besoin de celle de centroïde, qui est le barycentre d’un cluster. déﬁnition 12.1 (centroïde et médoïde) on appelle centroïde du cluster c le point déﬁni par (cid:126)µc = 1 |c| (cid:88) (cid:126)x. (cid:126)x∈c le médoïde est le point du cluster le plus proche du centroïde (il peut ne pas être unique, auquel cas il sera choisi arbitrairement). il sert de représentant du cluster : (cid:126)mc = arg min (cid:126)x∈c d((cid:126)x, (cid:126)µc). (cid:4) que des observations proches appartiennent au même cluster peut se traduire par la notion d’homo- généité, illustrée sur la ﬁgure 12.1 : déﬁnition 12.2 (homogénéité) on appelle homogénéité du cluster ck, ou tightness en anglais, la moyenne des distances des observations de ce cluster à son centroïde : tk = 1 |ck| (cid:88) (cid:126)x∈ck d((cid:126)x, (cid:126)µk). \f150 chapitre 12. clustering ici (cid:126)µk est le centroïde de ck. l’homogénéité globale d’un clustering de d se calcule comme la moyenne des homogénéités des clusters : t = 1 k k (cid:88) k=1 tk. (cid:4) figure 12.1 – les deux clusters représentés sur le panneau de gauche sont homogènes, resserrés sur eux- mêmes : ils sont composés de points proches les uns des autres. à l’inverse, les deux clusters représentés sur le panneau de droite sont moins homogènes. pour quantiﬁer à quel point les clusters sont distants les uns des autres, nous pouvons utiliser le critère de séparabilité, illustré sur la ﬁgure 12.2. déﬁnition 12.3 (séparabilité) on appelle séparabilité des clusters ck et cl la distance entre leurs cen- troïdes : skl = d((cid:126)µk, (cid:126)µl). la séparabilité globale d’un clustering de d se calcule comme la moyenne des séparabilités des clusters deux à deux : s = 2 k(k − 1) k (cid:88) k (cid:88) k=1 k=k+1 skl. (cid:4) plutôt que de considérer les deux critères de séparabilité (que l’on souhaite élevée) et d’homogénéité (que l’on souhaite faible) séparément, il est possible de les comparer l’un à l’autre grâce à l’indice de davies- bouldin. déﬁnition 12.4 (indice de davies-bouldin) on appelle indice de davies-bouldin du cluster ck la valeur dk = max l(cid:54)=k tk + tl skl . \f12.2. évaluer la qualité d’un algorithme de clustering 151 figure 12.2 – les trois clusters représentés sur le panneau de gauche sont bien séparés, contrairement à ceux représentés sur le panneau de droite qui sont proches les uns des autres. l’indice de davies-bouldin global d’un clustering de d se calcule comme la moyenne des indices de davies- bouldin des clusters : d = 1 c k (cid:88) k=1 dk. (cid:4) une autre façon de prendre en compte séparabilité et homogénéité est de calculer le coeﬃcient de sil- houette, qui permet de quantiﬁer pour chacune des observations si elle appartient ou non au bon cluster. déﬁnition 12.5 (coefﬁcient de silhouette) on appelle coeﬃcient de silhouette de l’observation (cid:126)x ∈ d la valeur s((cid:126)x) = b((cid:126)x) − a((cid:126)x) max(a((cid:126)x), b((cid:126)x)) où a((cid:126)x) est la distance moyenne de (cid:126)x à tous les autres éléments du cluster auquel il appartient et b((cid:126)x) est la plus petite valeur que pourrait prendre a((cid:126)x), si a appartenait à un autre cluster : a((cid:126)x) = 1 |ck((cid:126)x)| − 1 (cid:88) d((cid:126)u, (cid:126)x); (cid:126)u∈ck((cid:126)x),(cid:126)u(cid:54)=(cid:126)x b((cid:126)x) = min l(cid:54)=k((cid:126)x) 1 |cl| (cid:88) (cid:126)u∈cl d((cid:126)u, (cid:126)x). le coeﬃcient de silhouette global du clustering est son coeﬃcient de silhouette moyen : s = 1 n n (cid:88) i=1 s((cid:126)x i). (cid:4) le coeﬃcient de silhouette de (cid:126)x est d’autant plus proche de 1 que son assignation au cluster ck((cid:126)x) est satisfaisante. \f152 12.2.2 la stabilité des clusters chapitre 12. clustering un autre critère important est celui de la stabilité des clusters. on s’attend en eﬀet à obtenir les mêmes clusters si on supprime ou perturbe quelques observations, ou en initialisant diﬀéremment l’algorithme de partitionnement. ce critère peut être utilisé pour choisir les hyperparamètres de l’algorithme : si on obtient des clusters très diﬀérents pour diﬀérentes initialisations de l’algorithme de partitionnement, cela peut indiquer que les hyperparamètres sont mal choisis. 12.2.3 les connaissances expert enﬁn, il arrive parfois que l’on dispose d’un jeu de données partiellement étiqueté par des classes que nous aimerions retrouver par clustering. cela peut être le cas d’un corpus de documents dont un sous- ensemble est étiqueté par sujet, ou d’une base de données d’images dont un sous-ensemble est étiqueté en fonction de ce qu’elles représentent. on peut alors évaluer le résultat d’un algorithme de partitionnement comme on évaluerait celui d’un algorithme de classiﬁcation multi-classe. attention, il y a cependant une diﬀérence : dans le cas du clus- tering, peu importe que les objets de la classe k se retrouvent dans le premier, le deuxième ou le k-ème cluster. il faut donc évaluer la concordance de la partition des données par l’algorithme de clustering avec celle induite par les étiquettes. c’est ce que permet de faire l’indice de rand. nous supposons maintenant les observations (cid:126)x i étiquetées par yi ∈ {1, 2, . . . , c}. déﬁnition 12.6 (indice de rand) on appelle indice de rand la proportion de paires d’observations qui sont soit de même classe et dans le même cluster, soit de classe diﬀérente et dans deux clusters diﬀérents : ri = 2 n(n − 1) n (cid:88) n (cid:88) i=1 l=i+1 (cid:16) (cid:17) k((cid:126)x i) = k((cid:126)x l) δ δ(yi = yl) + δ (cid:16) k((cid:126)x i) (cid:54)= k((cid:126)x l) (cid:17) δ(yi (cid:54)= yl). (cid:4) en bioinformatique, où il est souvent délicat d’étiqueter les objets d’étude par manque de connais- sances, il est fréquent d’évaluer un clustering en le comparant à une ontologie, c’est-à-dire une classiﬁca- tion d’objets (par exemple, des gènes) en catégories décrites par un vocabulaire commun et organisées de manière hiérarchique. on peut évaluer la cohérence d’un clustering avec une ontologie par une analyse d’enrichissement, qui consiste à évaluer s’il y a plus d’objets d’une catégorie de l’ontologie au sein d’un clus- ter que ce à quoi on pourrait s’attendre par hasard. si l’on suppose les données issues d’une distribution hypergéométrique, il s’agit alors de calculer, pour un cluster ck, une catégorie g, et un seuil t ∈ n, p[|g ∩ ck| ≥ t] = 1 − t (cid:88) s=1 (cid:0)|g| s (cid:1)(cid:0) n−|g| |ck|−s (cid:1) (cid:0) n |ck| (cid:1) . (12.1) en eﬀet, le terme sous la somme est la probabilité que, lorsqu’on tire |ck| éléments parmi n, s d’entre eux appartiennent à g. muni de ces diﬀérentes façons d’évaluer un algorithme de partitionnement de données, nous pouvons maintenant découvrir ces algorithmes eux-mêmes. ces algorithmes cherchent à optimiser les critères d’ho- mogénéité et de séparabilité que nous venons de déﬁnir. comme il n’est pas possible de le faire de manière exacte, il s’agit de le faire de manière approchée. nous nous concentrerons dans ce chapitre sur les trois principales familles d’algorithmes de clustering : clustering hiérarchique, clustering par centroïdes, et clus- tering par densité. \f12.3. clustering hiérarchique 12.3 clustering hiérarchique 153 le clustering hiérarchique forme des clusters séparés par récurrence : il s’agit de partitionner les données pour toutes les échelles possibles de taille de partition, dans une hiérarchie à plusieurs niveaux. 12.3.1 dendrogramme le résultat d’un clustering hiérarchique peut se visualiser sous la forme d’un dendrogramme. il s’agit d’un arbre dont les n feuilles correspondent chacune à une observation. chaque nœud de l’arbre correspond à un cluster : — la racine est un cluster contenant toutes les observations — chaque feuille est un cluster contenant une observation — les clusters ayant le même parent sont agglomérés en un seul cluster au niveau au-dessus — un cluster est subdivisé en ses enfants au niveau au-dessous. ce sont donc les nœuds intermédiaires qui nous intéresseront le plus. enﬁn, la longueur d’une branche de l’arbre est proportionnelle à la distance entre les deux clusters qu’elle connecte. la ﬁgure 12.3 représente un exemple de dendrogramme. dans le cas où n est trop grand pour repré- senter l’intégralité de l’arbre, il est classique de le couper et de n’en représenter que la partie de la racine à un niveau que l’on choisit. figure 12.3 – un exemple de dendrogramme. en coupant au niveau de la ligne en pointillés, on obtient 3 clusters. chaque feuille de l’arbre (sur l’axe des abscisses) correspond à une observation. cette facilité à représenter visuellement le résultat d’un algorithme de clustering hiérarchique fait qu’il est très utilisé dans des domaines comme la bioinformatique. \f154 chapitre 12. clustering 12.3.2 construction agglomérative ou divisive un clustering hiérarchique peut se construire de manière agglomérative ou divisive. le clustering agglomératif, ou bottom-up clustering, commence par considérer les feuilles du dendrogramme : initialement, chaque observation forme un cluster de taille 1. à chaque itération de l’algorithme, on trouve les deux clusters les plus proches, et on les agglomère en un seul cluster, et ce jusqu’à ne plus avoir qu’un unique cluster contenant les n observations. le clustering divisif, ou top-down clustering, est l’approche inverse. on l’initialise en considérant un seul cluster, la racine du dendrogramme, contenant toutes les observations. à chaque itération, on sépare un cluster en deux, jusqu’à ce que chaque cluster ne contienne plus qu’une seule observation. par la suite, nous nous concentrerons sur l’approche agglomérative. 12.3.3 fonctions de lien déterminer les deux clusters les plus proches aﬁn de les agglomérer requiert de déﬁnir une distance entre clusters ; c’est ce qu’on appelle dans le cadre du clustering une fonction de lien, ou linkage en anglais. plusieurs approches sont possibles, qui reposent toutes sur une distance d sur x . tout d’abord, on peut choisir d’agglomérer deux clusters si deux de leurs éléments sont proches. on utilise alors le lien simple. déﬁnition 12.7 (lien simple) on appelle lien simple, ou single linkage, la distance entre deux clusters déﬁnie par dsimple(ck, cl) = min ((cid:126)u,(cid:126)v)∈ck×cl d((cid:126)u, (cid:126)v). (cid:4) on peut aussi choisir d’agglomérer deux clusters si tous leurs éléments sont proches. on utilise alors le lien complet, qui est la distance maximale entre un élément du premier cluster et un élément du deuxième. déﬁnition 12.8 (lien complet) on appelle lien complet, ou complete linkage, la distance entre deux clusters déﬁnie par dcomplet(ck, cl) = max ((cid:126)u,(cid:126)v)∈ck×cl d((cid:126)u, (cid:126)v). (cid:4) une approche intermédiaire consiste à considérer la distance moyenne entre un élément du premier cluster et un élément du deuxième. c’est le lien moyen. déﬁnition 12.9 (lien moyen) on appelle lien moyen, ou average linkage, la distance entre deux clusters déﬁnie par dmoyen(ck, cl) = 1 |ck| 1 |cl| (cid:88) (cid:88) (cid:126)u∈ck (cid:126)v∈cl d((cid:126)u, (cid:126)v). cette distance est aussi parfois appelée upgma pour unweighted paired group method with arithmetic mean. (cid:4) une alternative au lien moyen est le lien centroïdal, qui considère la distance entre les centroïdes des clusters. \f12.3. clustering hiérarchique 155 déﬁnition 12.10 (lien centroïdal) on appelle lien centroïdal, ou centroid linkage, la distance entre deux clusters déﬁnie par dcentroidal(ck, cl) = d   1 |ck| (cid:88) (cid:126)u, (cid:126)u∈ck 1 |cl| (cid:88) (cid:126)v∈cl  (cid:126)v  = d((cid:126)µk, (cid:126)µl). cette distance est aussi parfois appelée upgmc pour unweighted paired group method with centroid. (cid:4) les fonctions de lien ci-dessus s’attachent à garantir la séparabilité des clusters. à l’inverse, il est possible d((cid:126)x, (cid:126)µk). dans le cas où d (cid:126)x∈ck ||(cid:126)x − (cid:126)µk||2 . le clustering de ward utilise une formulation de chercher à maximiser leur homogénéité : il s’agit de minimiser tk = 1 |ck| est la distance euclidienne, alors tk = 1 |ck| similaire : il s’agit d’agglomérer deux clusters de sorte à minimiser la variance intra-cluster du résultat. (cid:126)x∈ck (cid:80) (cid:80) déﬁnition 12.11 (inertie) on appelle variance intra-cluster, ou inertie du cluster c la valeur varin(c) = 1 |c| (cid:88) (cid:126)x∈c ||(cid:126)x − (cid:126)µ||2 2 . l’inertie globale d’un clustering de d est alors donnée par la somme des inerties des clusters : v = k (cid:88) k=1 1 |ck| (cid:88) (cid:126)x∈ck ||(cid:126)x − (cid:126)µk||2 2 . (cid:4) 12.3.4 choix du nombre de clusters le clustering hiérarchique a l’avantage de ne pas requérir de déﬁnir à l’avance le nombre de clusters, ce qui permet d’explorer toutes les possibilités le long du dendrogramme. cependant, il faut généralement prendre cette décision à un moment. il est possible pour cela d’utiliser un dendrogramme, pour y déceler un « niveau » auquel les clusters sont clairement distants les uns des autres – on se rappellera que la longueur d’une branche est propor- tionnelle à la distance entre les deux clusters qu’elle sépare. une solution alternative est d’évaluer les diﬀérentes partitions trouvées, c’est-à-dire les diﬀérents nœuds du dendrogramme, à l’aide d’une mesure de performance telle que le coeﬃcient de silhouette. 12.3.5 complexité algorithmique la complexité algorithmique du clustering hiérarchique est élevée : à chaque itération, pour décider quels clusters regrouper, il nous faudra calculer les distances deux à deux entre toutes les paires d’obser- vations du jeu de données, pour une complexité en o(pn2). une alternative est de stocker ces distances en mémoire pour pouvoir les réutiliser, ce qui a une complexité quadratique en le nombre d’observations. le clustering hiérarchique est donc plus adapté aux jeux de données contenant peu d’échantillons. \f156 chapitre 12. clustering 12.4 méthode des k-moyennes plutôt que d’explorer toutes les partitions possibles à diﬀérentes échelles, il peut être préférable de se ﬁxer un nombre k de clusters, ce qui permet de réduire les temps de calculs. nous avons vu ci-dessus que le clustering de ward cherche à minimiser la variance intra-cluster aﬁn de créer des clusters homogènes. la méthode dite des k-moyennes, ou du k-means, proposée par hugo stein- haus (1957), cherche à résoudre ce problème pour un nombre de clusters k ﬁxé. il s’agit alors de trouver l’aﬀectation des observations à k clusters qui minimise la variance intra-cluster globale : arg min c1,c2,...,ck k (cid:88) (cid:88) k=1 (cid:126)x∈ck ||(cid:126)x − (cid:126)µk||2 2 . (12.2) cependant, résoudre ce problème de manière exacte n’est pas possible. on utilise donc une heuristique, l’algorithme de lloyd, proposé par stuart lloyd (1982). 12.4.1 algorithme de lloyd déﬁnition 12.12 (algorithme de lloyd) étant données n observations dans rp et un nombre k de clusters, l’algorithme de lloyd procède de la manière suivante : 1. choisir k observations (cid:126)µ1, (cid:126)µ2, . . . , (cid:126)µk parmi les n observations, pour servir de centroïdes initiaux ; 2. aﬀecter chaque observation (cid:126)x i ∈ d au centroïde dont elle est le plus proche : k((cid:126)x i) = arg min k=1,...,k (cid:12) (cid:12) (cid:12) (cid:12)(cid:126)x i − (cid:126)µk (cid:12) (cid:12) (cid:12)2 ; (cid:12) 3. recalculer les centroïdes de chaque cluster : (cid:126)µk = 1 |ck| (cid:88) (cid:126)x i; (cid:126)x i∈ck 4. répéter les opérations 2–3 jusqu’à convergence, c’est-à-dire jusqu’à ce que les aﬀectations ne changent plus. (cid:4) l’algorithme de lloyd implémente une stratégie gloutonne, et s’il converge en général très rapidement, il peut tomber dans un minimum local. il peut donc être pertinent de le faire tourner plusieurs fois, et de garder la solution qui a la plus faible variance intra-cluster. si l’on doit itérer t fois l’algorithme de lloyd, sachant que le coût de calculer kn distances en p dimen- sions est de l’ordre de o(npk), la complexité algorithmique de l’algorithme de lloyd est en o(npkt). k et t sont typiquement négligeables devant n, et cet algorithme est donc linéaire en le nombre d’observa- tions, par opposition au clustering hiérarchique dont le coût est quadratique en n : nous avons remplacé le calcul des distances d’une observation (cid:126)x i aux n − 1 autres points du jeu de données par un calcul de sa distance à k centroïdes. 12.4.2 forme des clusters d’après la formulation de l’algorithme des k-moyennes (équation 12.2), chaque cluster ck est constitué des observations de d qui sont les plus proches du centroïde (cid:126)µk. ils forment donc un diagramme de voronoï (cf. section 8.1.2). en particulier, cela signiﬁe que les clusters trouvés par l’algorithme des k-moyennes sont nécessairement convexes. \f12.4. méthode des k-moyennes données aberrantes 157 l’algorithme du k-means est sensible aux données aberrantes : elles vont en eﬀet tirer un cluster à elle. si une observation (cid:126)x i est très éloignée des autres observations, alors elle se retrouvera dans son propre cluster, tandis que le reste des données sera partitionné en k − 1 clusters. cette propriété est néanmoins intéressante, car elle permet d’utiliser l’algorithme des k-moyennes jus- tement pour détecter les observations aberrantes : ce sont celles qui sont seules dans leur cluster. 12.4.3 variantes k-means++ l’algorithme du k-means est stochastique : on peut obtenir des résultats diﬀérents selon l’initialisation, et certains de ces résultats peuvent avoir une inertie bien plus grande que la solution optimale. pour éviter ce problème, l’algorithme k-means++ commence par initialiser les centroïdes de manière à les disperser au maximum parmi les données. plus précisément, la procédure consiste à 1. choisir un premier centroïde (cid:126)u 1 aléatoirement parmi les observations d 2. pour k = 2, . . . , k : choisir le k-ème centroïde (cid:126)u k parmi d \\ (cid:126)u k−1, en suivant une loi proportionnelle au carré de la distance à (cid:126)u k−1, c’est-à-dire que (cid:126)u k aura de fortes chances d’être éloigné de (cid:126)u k−1. cette approche ne rend pas le k-means déterministe, mais permet d’éviter les « pires » solutions. méthode des k-moyennes avec noyau la méthode des k-moyennes requiert de décrire les données dans un espace euclidien, et ne peut for- mer que des clusters convexes, ce qui peut être limitant. cependant, l’astuce du noyau (cf. section 10.3.3) s’applique à l’algorithme de lloyd. démonstration. soit κ : x × x (cid:55)→ r un noyau. il existe un espace de hilbert h et une application φ : x → h telle que, pour tout (cid:126)x, (cid:126)x(cid:48) ∈ x × x , κ((cid:126)x, (cid:126)x(cid:48)) = (cid:104)φ((cid:126)x), φ((cid:126)x(cid:48))(cid:105)h. pour appliquer l’algorithme de lloyd aux images {φ((cid:126)x 1), φ((cid:126)x 2), . . . , φ((cid:126)x n)} des éléments de d dans h, nous avons besoin de calculer, à chaque itération, la distance de φ((cid:126)x i) à chacun des k centroïdes (cid:126)h1, (cid:126)h2, . . . ,(cid:126)hk. la position d’un centroïde (cid:126)hk se calcule comme la moyenne des images des observations appartenant à ck : (cid:126)hk = 1 |ck| (cid:88) φ((cid:126)x i). (cid:126)x i∈ck la distance de l’image d’une observation (cid:126)x i à un centroïde se calcule alors comme (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)φ((cid:126)x i) − (cid:126)hk (cid:12) (cid:12) (cid:12) 2 (cid:12) (cid:12) (cid:12) (cid:12) 2 = φ((cid:126)x i) − (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:42) = φ((cid:126)x i) − φ((cid:126)u) (cid:12) (cid:12) 2 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2 φ((cid:126)u), φ((cid:126)x i) − 1 |ck| 1 |ck| (cid:88) (cid:126)u∈ck (cid:88) (cid:126)u∈ck = κ((cid:126)x i, (cid:126)x i) − (cid:88) κ((cid:126)u, (cid:126)x i) + 2 |ck| (cid:88) (cid:43) φ((cid:126)u) (cid:126)u∈ck (cid:88) (cid:88) h κ((cid:126)u, (cid:126)v). 1 |ck| 1 |ck|2 (cid:126)u∈ck on peut ainsi réécrire l’étape (2) de l’algorithme de lloyd sans faire appel à l’application φ, et sans avoir (cid:3) besoin de recalculer explicitement les centroïdes à l’étape (3). (cid:126)u∈ck (cid:126)v∈ck \f158 chapitre 12. clustering la version à noyau de la méthode des k-moyennes ne permet généralement pas de connaître les cen- troïdes des clusters, puisqu’ils vivent dans l’espace de redescription h qui n’est pas accessible sans connaître φ. remarque 12.5 clustering par densité une autre façon d’obtenir des clusters non convexes est le clustering par densité. prenons l’exemple présenté sur la ﬁgure 12.4 : il semblerait naturel de partitionner les données en 3 cercles concentriques, ce que ni le clustering agglomératif ni la méthode des k-moyennes n’arrivent à faire, car ces clusters ne sont pas convexes. (a) il nous semble naturel de par- titionner ces données en 3 cercles concentriques. (b) partitionnement en 3 clusters par clustering agglomératif (lien moyen). (c) partitionnement en 3 clusters par k-moyennes. figure 12.4 – motivation du clustering par densité. c’est le problème que le clustering par densité cherche à résoudre, en observant que les points les plus proches d’un point donné sont sur le même cercle et non sur un autre cercle. il s’agit de former des clusters d’observations proches les unes des autres, au sens où si deux éléments (cid:126)x i et (cid:126)x l d’un même cluster ck peuvent être éloignés l’un de l’autre, il existe une séquence d’éléments (cid:126)u 1, (cid:126)u 2, . . . , (cid:126)u m ∈ ck tels que (cid:126)u 1 est proche de (cid:126)x i, (cid:126)u m est proche de (cid:126)x l, et pour tout t ∈ {1, . . . , m}, (cid:126)u t est proche de (cid:126)u t−1. cette idée est illustrée sur la ﬁgure 12.5 pour formaliser cette idée, nous allons avoir besoin de quelques déﬁnitions. déﬁnition 12.13 ((cid:15)-voisinage) soient d = {(cid:126)x 1, (cid:126)x 2, . . . , (cid:126)x n} le jeu d’éléments de x à partitionner, d une distance sur x , et (cid:15) > 0. on appelle (cid:15)-voisinage d’un élément (cid:126)x ∈ x l’ensemble des observations de d dont la distance à (cid:126)x est inférieure à (cid:15) : n(cid:15)((cid:126)x) = {(cid:126)u ∈ d : d((cid:126)x, (cid:126)u) < (cid:15)}. (cid:4) déﬁnition 12.14 (point intérieur) étant donné nmin ∈ n, on dit de (cid:126)x ∈ x qu’il est un point intérieur, (cid:4) ou core point en anglais, si son (cid:15)-voisinage contient au moins nmin éléments : |n(cid:15)((cid:126)x)| ≥ nmin. \f12.5. clustering par densité 159 figure 12.5 – il existe un chemin entre voisins proches permettant de passer d’un point à un autre du même cluster. déﬁnition 12.15 (connexion par densité) on dit que (cid:126)x et (cid:126)v ∈ x sont connectés par densité si l’on peut les « relier » par une suite d’(cid:15)-voisinages contenant chacun au moins nmin éléments. formellement, cela signiﬁe qu’il existe m ∈ n, et une séquence (cid:126)u 1, (cid:126)u 2, . . . , (cid:126)u m d’éléments de d tels que (cid:126)u 1 est un point intérieur de n(cid:15)((cid:126)x), (cid:126)u 2 est un point intérieur de n(cid:15)((cid:126)u 1), . . . , (cid:126)u m est un point intérieur de n(cid:15)((cid:126)u m−1), (cid:126)v (cid:4) est un point intérieur de n(cid:15)((cid:126)u m). l’algorithme dbscan, proposé en 1996 par martin ester, hans-peter kriegel, jörg sander et xiaowei xu (ester et al., 1996), partitionne les données en créant des clusters de points atteignables par densité les uns depuis les autres. c’est un algorithme de partitionnement populaire, qui a obtenu en 2014 une distinc- tion de contribution scientiﬁque ayant résisté à l’épreuve du temps, le test of time award de la prestigieuse conférence kdd. déﬁnition 12.16 (dbscan) on appelle dbscan, pour density-based spatial clustering of applications with noise, ou partitionnement dans l’espace par densité pour des applications bruitées, la procédure de parti- tionnement suivante : — initialisation : un ensemble d’éléments visités v = ∅, une liste de clusters k = ∅, une liste d’obser- vations aberrantes a = ∅. — pour chaque élément (cid:126)x ∈ d \\ v : 1. construire n(cid:15)((cid:126)x) 2. si |n(cid:15)((cid:126)x)| < nmin : considérer (temporairement) (cid:126)x comme une observation aberrante : a ← a ∪ {(cid:126)x} sinon : — créer un cluster c ← {(cid:126)x} — augmenter ce cluster par la procédure grow_cluster(c, n(cid:15)((cid:126)x), (cid:15), nmin) 3. ajouter c à la liste des clusters : k ← k ∪ c \f160 chapitre 12. clustering 4. marquer tous les éléments de c comme visités : v ← v ∪ c. la procédure grow_cluster(c, n , (cid:15), nmin) est déﬁnie comme suit : pour tout (cid:126)u ∈ n \\ v : — créer n (cid:48) ← n(cid:15)((cid:126)u) — si |n (cid:48)| ≥ nmin : actualiser n de sorte à prendre les éléments de n (cid:48) en considération : n ← n ∪ n (cid:48) — si (cid:126)u n’appartient à aucun autre cluster, l’ajouter à c : c ← c ∪ {(cid:126)u} — si (cid:126)u était précédemment cataloguée comme aberrante, l’enlever de la liste des observations aber- rantes : a = a \\ {(cid:126)u}. (cid:4) un des avantages de dbscan est sa robustesse aux données aberrantes, qui sont identiﬁées lors de la formation des clusters. le ﬂéau de la dimension (voir section 11.1.3) rend dbscan diﬃcile à appliquer en très grande dimen- sion : les (cid:15)-voisinages auront tendance à ne contenir que leur centre. de plus, la densité étant déﬁnie par les paramètres (cid:15) et nmin, dbscan ne pourra pas trouver de clusters de densité diﬀérente. cependant, dbscan ne requiert pas de prédéﬁnir le nombre de clusters, et est eﬃcace en temps de calcul. son application aux données de la ﬁgure 12.4a est illustrée sur la ﬁgure 12.6. figure 12.6 – partitionnement des données de la ﬁgure 12.4a par dbscan. points clefs — le clustering, ou partitionnement de données, cherche à identiﬁer des classes sans utiliser d’éti- quettes. — en l’absence d’étiquette, la qualité d’une partition peut s’évaluer sur des critères de séparabilité et d’homogénéité \f161 — le clustering hiérarchique partitionne les données de manière itérative. son résultat peut être vi- sualisé sur un dendrogramme. — le clustering par la méthode des k-moyennes s’eﬀectue grâce à l’algorithme de lloyd ou une de ses variantes. il permet de trouver eﬃcacement k clusters convexes. — la version à noyau de la méthode des k-moyennes permet de l’appliquer pour découvrir des clusters non convexes. — le clustering par densité permet d’identiﬁer des régions denses du jeu de données, c’est-à-dire des observations qui peuvent former un ensemble non convexe mais qui sont proches les unes des autres. pour aller plus loin • les algorithmes de partitionnement que nous avons vus associent un seul et unique cluster à chaque observation. il existe des variantes, appelées partitionnement ﬂou, ou fuzzy clustering en anglais, qui associent à chaque observation et chaque cluster la probabilité que cette observation appartienne à ce cluster. • l’ouvrage de jain et dubes (1988) constitue une bonne introduction au clustering. on pourra aussi se reporter à l’article de xu et wunsch ii (2005) bibliographie ester, m., kriegel, h.-p., sander, j., and xu, x. (1996). a density-based algorithm for discovering clusters in proceedings of the second international conference on knowledge in large spatial databases with noise. discovery and data mining, pages 226–231, portland (or). aaai press. jain, a. k. and dubes, r. c. (1988). algorithms for clustering data. prentice hall, new york. lloyd, s. (1982). least squares quantization in pcm. ieee transactions on information theory, 28(2) :129–137. steinhaus, h. (1957). sur la division des corps matériels en parties. bulletin de l’académie polonaise des sciences, 4(12) :801–804. xu, r. and wunsch ii, d. (2005). survey of clustering algorithms. ieee transactions on neural networks, 16 :645– 678. \fappendice a notions d’optimisation convexe dans cet ouvrage, nous formulons de nombreux modèles de machine learning comme la solution d’un problème d’optimisation. il s’agit généralement de minimiser un risque empirique, ou de maximiser une variance, souvent sous certaines contraintes. ces problèmes sont diﬃciles à résoudre eﬃcacement dans le cas général. cependant, dans le contexte de l’apprentissage automatique, les fonctions considérées sont souvent convexes, ce qui facilite grandement la tâche. le but de cet appendice est de présenter les pro- blèmes d’optimisation dits convexes et quelques techniques permettant de les résoudre. objectifs — reconnaître un problème d’optimisation convexe ; — résoudre un problème d’optimisation convexe exactement, ou par un algorithme à directions de descente ; — formuler le problème dual d’un problème d’optimisation quadratique ; — écrire les conditions de karush-kuhn-tucker pour un problème d’optimisation quadratique. a.1 convexité commençons par déﬁnir la notion de convexité, pour un ensemble puis pour une fonction. a.1.1 ensemble convexe déﬁnition 1.1 (ensemble convexe) on dit d’un ensemble s ⊆ rn qu’il est convexe si et seulement si, quels que soient (cid:126)u, (cid:126)v ∈ s et t ∈ [0, 1], t(cid:126)u + (1 − t)(cid:126)v ∈ s. en d’autres termes, le segment [(cid:126)u, (cid:126)v] est entièrement contenu dans s. (cid:4) la ﬁgure a.1 présente quelques exemples d’ensembles convexes et non convexes dans r2. 162 \fa.1. convexité 163 figure a.1 – les trois ensembles de r2 présentés sur la rangée du haut sont convexes. les trois ensembles sur la rangée du bas ne sont pas convexes, et un exemple de segment reliant deux points de l’ensemble mais n’étant pas entièrement inclus dans cet ensemble est présenté pour chacun d’entre eux. a.1.2 fonction convexe déﬁnition 1.2 (fonction convexe) soit u ⊆ rn. une fonction f : u → r est dite convexe lorsque — le domaine de déﬁnition u de f est un ensemble convexe ; — quels que soient (cid:126)u, (cid:126)v ∈ u et t ∈ [0, 1], f (t (cid:126)u + (1 − t) (cid:126)v) ≤ tf ((cid:126)u) + (1 − t)f ((cid:126)v), c’est-à-dire que, sur [(cid:126)u, (cid:126)v], f se situe au-dessous du segment [((cid:126)u, f ((cid:126)u)), ((cid:126)v, f ((cid:126)v))]. si l’inégalité est stricte pour tout (cid:126)u (cid:54)= (cid:126)vinu et t ∈]0, 1[, on parle alors de fonction strictement convexe. une fonction strictement convexe a une courbure supérieure à celle d’une fonction aﬃne. enﬁn, dans le cas où il existe k > 0 tel que f − k 2 ||(cid:126)u||2 2 est strictement convexe, f est dite fortement (cid:4) convexe. exemple a ∈ n a /∈]0, 1[ a ∈ r les fonctions ci-dessous sont convexes — f : r → r, u (cid:55)→ u2a + → r, u (cid:55)→ ua — f : r∗ — f : r → r, u (cid:55)→ eau — f : r∗ + → r, u (cid:55)→ − log(au) — f : rn → r, (cid:126)u (cid:55)→ (cid:126)a (cid:62)(cid:126)u + b — f : rn → r, (cid:126)u (cid:55)→ 1 — f : rn → r, (cid:126)u (cid:55)→ ||(cid:126)u||p = ((cid:80)n 2(cid:126)u (cid:62)q(cid:126)u + (cid:126)a (cid:62)(cid:126)u + b i=1 |ui|p) a ∈ r (cid:126)a ∈ rn, b ∈ r (cid:126)a ∈ rn, b ∈ r, q (cid:23) 0 1 p p ≥ 1. une fonction dont l’opposée est convexe est dite concave. déﬁnition 1.3 (fonction concave) soit u ⊆ rn. une fonction f : u → r est dite concave si et seulement −f est convexe : — u est un ensemble convexe ; \f164 appendice a. notions d’optimisation convexe — quels que soient (cid:126)u, (cid:126)v ∈ u et t ∈ [0, 1], f (t(cid:126)u + (1 − t)(cid:126)v) ≥ tf ((cid:126)u) + (1 − t)f ((cid:126)v). si l’inégalité est stricte pour tout (cid:126)u (cid:54)= (cid:126)vinu et t ∈]0, 1[, on parle alors de fonction strictement concave. (cid:4) les opérations suivantes préservent la convexité : — la combinaison linéaire positive : si f1, f2, . . . , fk : rn → r sont convexes, et a1, a2, . . . , ak > 0, alors est convexe. — la maximisation : si f1, f2, . . . , fk : rn → r sont convexes, alors a1f1 + a2f2 + · · · + akfk (cid:126)u (cid:55)→ max 1,...,k fk((cid:126)u) est convexe. — la minimisation partielle : si f : rn → r est convexe, et c ⊆ rn est un ensemble convexe, alors est convexe. u1, u2, . . . , uk−1 (cid:55)→ min v∈c f (u1, u2, . . . , uk−1, v) a.2 problèmes d’optimisation convexe a.2.1 formulation et vocabulaire déﬁnition 1.4 (optimisation convexe) étant donnés u ⊆ rn et une fonction f : u → r convexe, on appelle problème d’optimisation convexe le problème suivant f ((cid:126)u). min (cid:126)u∈u on appelle alors f la fonction objectif, ou encore la fonction coût ou la fonction critère. un point (cid:126)u ∗ ∈ u vériﬁant f ((cid:126)u ∗) ≤ f ((cid:126)u) ∀(cid:126)u ∈ u est appelé point minimum de f sur u, tandis que la (cid:4) valeur f ((cid:126)u ∗) est appelée minimum de f sur u. déﬁnition 1.5 (optimisation convexe sous contraintes) étant donnés m, r deux entiers positifs, des ensembles u, u1, . . . , um, v1, . . . , vr ⊆ rn, une fonction convexe f : u → r, m fonctions gi : ui → r, convexes aussi, et r fonctions hj : vj → r aﬃnes, on appelle problème d’optimisation convexe sous contraintes le problème suivant f ((cid:126)u) sous les contraintes min (cid:126)u∈d gi((cid:126)u) ≤ 0 ∀i = 1, . . . , m hj((cid:126)u) = 0 ∀j = 1, . . . , r, j=1 vj est le domaine commun à toutes les fonctions considérées. f est la fonction objectif. les m contraintes gi ≤ 0 sont les contraintes d’inégalité. les r contraintes hj = 0 i=1 ui (cid:84)r où d = u ∩ (cid:84)m sont les contraintes d’égalité. (cid:126)v ∈ d qui vériﬁe toutes les contraintes est appelé un point admissible ou point réalisable (feasible point en anglais), et l’ensemble des points admissibles est appelé la région admissible ou le domaine des contraintes (feasible domain en anglais). cet ensemble est convexe. (cid:4) \fa.2. problèmes d’optimisation convexe a.2.2 extrema locaux et globaux 165 le terme d’extremum désigne indiﬀéremment un maximum ou un minimum. ceux-ci peuvent être globaux, c’est-à-dire maximiser (respectivement minimiser) la fonction sur tout son domaine, ou locaux, c’est à dire optimaux dans leur voisinage. formellement : déﬁnition 1.6 (extremum global) soit u ⊆ rn, f : u → r, et (cid:126)u ∗ ∈ u. on dit que (cid:126)u ∗ est un point de minimum global, ou point de minimum absolu de f sur u si f ((cid:126)u ∗) ≤ f ((cid:126)u) ∀(cid:126)u ∈ u. (cid:126)u ∗ est un point de maximum global de f sur u s’il est un point de minimum global de −f sur u. (cid:4) déﬁnition 1.7 (extremum local) soient u ⊆ rn, f : u → r, et (cid:126)u ∗ ∈ u. on dit que (cid:126)u ∗ est un point de minimum local, ou point de minimum relatif de f sur u s’il existe un voisinage v de (cid:126)u ∗ dans rn tel que f ((cid:126)u ∗) ≤ f ((cid:126)u) ∀(cid:126)u ∈ u ∩ v. (cid:126)u ∗ est un point de maximum local de f sur u s’il est un point de minimum local de −f sur u. (cid:4) la notion de convexité est très importante en optimisation, car elle permet de garantir que les minima locaux sont des minima globaux. théorème 1.1 soit u ⊆ rn, f : u → r, et (cid:126)u ∗ ∈ u un point de minimum local de f sur u. alors — si f est convexe, alors (cid:126)u ∗ est un point de minimum global ; — si f est strictement convexe, alors (cid:126)u ∗ est l’unique point de minimum global. démonstration. (cid:126)u ∗ étant un point de minimum local, il existe (cid:15) > 0 tel que pour tout (cid:126)u vériﬁant ||(cid:126)u − (cid:126)u ∗||2 ≤ (cid:15), f ((cid:126)u ∗) ≤ f ((cid:126)u). (cid:4) (a.1) soit (cid:126)v ∈ u diﬀérent de (cid:126)u ∗. supposons f convexe. nous allons montrer que f ((cid:126)v) ≥ f ((cid:126)u ∗). deux cas sont possibles : soit (cid:126)v appartient au voisinage de (cid:126)u ∗, autrement dit ||(cid:126)v − (cid:126)u ∗||2 ≤ (cid:15), auquel cas f ((cid:126)v) ≥ f ((cid:126)u ∗) d’après l’équation a.1, soit (cid:126)v est à l’extérieur de ce voisinage, auquel cas déﬁnissons maintenant (cid:126)u = (1 − λ)(cid:126)u ∗ + λ(cid:126)v, avec λ = 1 2 par convexité de u, (cid:126)u ∈ u. (cid:15) ||(cid:126)v−(cid:126)u ∗||2 . d’après l’équation a.2, 0 < λ < 1. ||(cid:126)v − (cid:126)u ∗||2 > (cid:15). (a.2) ||(cid:126)u − (cid:126)u ∗||2 = ||(1 − λ)(cid:126)u ∗ + λ(cid:126)v − (cid:126)u ∗||2 = λ ||(cid:126)v − (cid:126)u ∗||2 = (cid:15) 2 < (cid:15), donc (cid:126)u appartient au voisinage de (cid:126)u ∗. d’après l’équation a.1, f ((cid:126)u) ≥ f ((cid:126)u ∗). par convexité de f , f ((cid:126)u) ≤ (1 − λ)f ((cid:126)u ∗) + λf ((cid:126)v) = f ((cid:126)u ∗) + λ(f ((cid:126)v) − f ((cid:126)u ∗)). comme f ((cid:126)u ∗) ≤ f ((cid:126)u), on obtient f ((cid:126)u ∗) ≤ f ((cid:126)u ∗) + λ(f ((cid:126)v) − f ((cid:126)u ∗)), ce dont on déduit (comme λ > 0) que f ((cid:126)v) ≥ f ((cid:126)u ∗). ainsi, pour tout (cid:126)v ∈ u diﬀérent de (cid:126)u ∗, f ((cid:126)v) ≥ f ((cid:126)u ∗) et (cid:126)u ∗ est donc bien un minimum global de f . dans le cas où f est strictement convexe, le même raisonnement tient avec des inégalités strictes et on (cid:3) obtient alors que (cid:126)u ∗ est un unique minimum global de f . nous allons maintenant voir comment résoudre un problème d’optimisation convexe. \f166 appendice a. notions d’optimisation convexe a.3 optimisation convexe sans contrainte commençons par les problèmes d’optimisation convexe sans contrainte. bien que l’optimisation convexe soit diﬃcile dans le cas général, il existe de nombreuses techniques permettant d’obtenir eﬃcacement une solution numérique approchée de très bonne qualité, notamment sous certaines hypothèses de continuité et de diﬀérentiabilité souvent vériﬁées pour les problèmes posés dans le cadre de l’apprentissage statis- tique. a.3.1 caractérisation différentielle de la convexité une fonction convexe et diﬀérentiable se situe au-dessus de n’importe laquelle de ses tangentes, comme illustré sur la ﬁgure a.2. formellement : théorème 1.2 caractérisation du premier ordre soit u ⊆ rn et une fonction f : u → r de classe c1 (autrement dit continue, et dont toutes les dérivées partielles existent et sont continues). f est convexe si et seulement si — u est convexe ; — quels que soient (cid:126)u, (cid:126)v ∈ u, f ((cid:126)v) ≥ f ((cid:126)u) + ∇f ((cid:126)u)(cid:62)((cid:126)v − (cid:126)u). démonstration. soient t ∈ [0, 1] et (cid:126)u, (cid:126)v ∈ u. supposons f convexe. alors 1 t par convexité de f , on peut écrire lim t→0+ (f ((cid:126)u + t((cid:126)v − (cid:126)u)) − f ((cid:126)u)) = ∇f ((cid:126)u)(cid:62)((cid:126)v − (cid:126)u). f ((cid:126)u + t((cid:126)v − (cid:126)u)) = f ((1 − t)(cid:126)u + t(cid:126)v) ≤ (1 − t)f ((cid:126)u) + tf ((cid:126)v), et donc 1 t (f ((cid:126)u + t((cid:126)v − (cid:126)u)) − f ((cid:126)u)) ≤ f ((cid:126)v) − f ((cid:126)u). (a.3) (cid:4) (a.4) (a.5) (a.6) en passant à la limite dans l’équation a.4 on obtient l’inégalité a.3. réciproquement, étant donnés (cid:126)u, (cid:126)v ∈ u et t ∈]0, 1[, posons (cid:126)w = t(cid:126)u + (1 − t)(cid:126)v. par convexité de u, (cid:126)w ∈ u. en supposant l’inégalité a.3, f ((cid:126)u) ≥ f ( (cid:126)w) + ∇f ( (cid:126)w)(cid:62)((cid:126)u − (cid:126)w) f ((cid:126)v) ≥ f ( (cid:126)w) + ∇f ( (cid:126)w)(cid:62)((cid:126)v − (cid:126)w). en additionnant la première inégalité multipliée par t à la deuxième multipliée par (1 − t), on obtient et donc la convexité de f . tf ((cid:126)u) + (1 − t)f ((cid:126)v) ≤ f ( (cid:126)w) + ∇f ( (cid:126)w)(cid:62)( (cid:126)w − (cid:126)w) (a.7) (cid:3) il découle du théorème 1.2 qu’une fonction f convexe et diﬀérentiable est minimale là où son gradient s’annule. formellement : théorème 1.3 soit u ⊆ rn et une fonction f : u → r convexe, de classe c1. soit (cid:126)u ∗ ∈ u. les propositions suivantes sont équivalentes : 1. (cid:126)u ∗ est un point de minimum de f sur u ; 2. ∇f ((cid:126)u ∗) = 0. (cid:4) \fa.3. optimisation convexe sans contrainte 167 figure a.2 – la fonction convexe f est située au-dessus de sa tangente en u. a.3.2 caractérisation du deuxième ordre de la convexité les fonctions convexes de classe c2 sont caractérisées par le fait que leur hessienne est semi-déﬁnie po- sitive. en d’autres termes, elles sont de courbure positive partout. cette propriété est exploitée par certain des algorithmes présentés plus bas. théorème 1.4 caractérisation du deuxième ordre soit u ⊆ rn et f : u → r une fonction de classe c2 (autrement dit continue, deux fois dérivable et de diﬀérentielles d’ordre 1 et 2 continues). f est convexe si et seulement si — u est convexe ; — quel que soit (cid:126)u ∈ u, ∇2f ((cid:126)u) (cid:23) 0. (cid:4) démonstration. soient (cid:126)u ∈ u et (cid:126)v ∈ rn. posons i = {t ∈ r|(cid:126)u + t(cid:126)v ∈ u} et déﬁnissons φ : i → r, t (cid:55)→ f ((cid:126)u + t(cid:126)v). alors i est un intervalle et la fonction φ est convexe. en eﬀet, supposons que i contient au moins deux points distincts t1 et t2. (dans le cas contraire, i est soit un singleton, soit l’intervalle vide. c’est donc un intervalle, et de plus, la convexité de φ est triviale.) soit s ∈]0, 1[ et (cid:126)w = (cid:126)u + (st1 + (1 − s)t2) (cid:126)v. alors (cid:126)w = s(cid:126)u1 + (1 − s)(cid:126)u2 avec (cid:126)u1 = (cid:126)u + t1(cid:126)v et (cid:126)u2 = (cid:126)u + t2(cid:126)v. comme t1 (cid:54)= t2, (cid:126)u1 (cid:54)= (cid:126)u2 et par convexité de u, (cid:126)w ∈ u donc (st1 + (1 − s)t2) ∈ i. ce raisonnement étant vrai pour tout s ∈]0, 1[, i est donc un intervalle. de plus, par convexité de f , φ (st1 + (1 − s)t2) = f (s(cid:126)u1 + (1 − s)(cid:126)u2) ≤ sf ((cid:126)u1) + (1 − s)f ((cid:126)u2) = sφ(t1) + (1 − s)φ(t2) et donc φ est convexe. \f168 appendice a. notions d’optimisation convexe comme f est c2 sur u, φ est c2 sur i. on a φ(cid:48)(cid:48)(t) = (cid:126)v (cid:62)∇2f ((cid:126)u + t(cid:126)v) (cid:126)v ∀t ∈ i. (a.8) par convexité de φ et le théorème 1.2, φ(cid:48) est croissante et donc φ(cid:48)(cid:48) ≥ 0. en particulier, φ(cid:48)(cid:48)(0) ≥ 0 et donc (cid:126)v (cid:62)∇2f ((cid:126)u) (cid:126)v ≥ 0. comme le raisonnement tient pour tout (cid:126)u ∈ u et tout (cid:126)v ∈ rn, ∇2f ((cid:126)u) (cid:23) 0. réciproquement, supposons ∇2f ((cid:126)u) semi-déﬁnie positive en tout point (cid:126)u ∈ u. prenons (cid:126)u1, (cid:126)u2 ∈ u, posons (cid:126)u = (cid:126)u2 et (cid:126)v = (cid:126)u1 − (cid:126)u2 et déﬁnissons φ : t (cid:55)→ f ((cid:126)u + t(cid:126)v) sur i = {t ∈ r|(cid:126)u + t(cid:126)v ∈ u}. alors, par l’équation a.8, φ(cid:48)(cid:48) est positive sur i et donc φ est convexe sur i. enﬁn, i contient les points t1 = 1 et t2 = 0. alors, pour tout s ∈]0, 1[, s = s t1 + (1 − s)t2 et f (s(cid:126)u1 + (1 − s)(cid:126)u2) = φ(s) ≤ sφ(t1) + (1 − s)φ(t2) = sf ((cid:126)u1) + (1 − s)f ((cid:126)u2) et donc f est convexe. a.3.3 algorithme du gradient (a.9) (cid:3) dans le cas où la fonction convexe que l’on cherche à minimiser est diﬀérentiable, il n’est pas toujours évident de trouver un point où son gradient s’annule. il est alors possible de trouver une solution approchée en remarquant que le gradient ∇f indique la direction de plus grande augmentation de la fonction f . ainsi, si l’on choisit un point (cid:126)u au hasard dans le domaine de f , et que ∇f ((cid:126)u) (cid:54)= 0, un réel positif t suﬃsamment petit, ((cid:126)u − t∇f ((cid:126)u)) est plus proche du point de minimum u∗ que u. ce concept est illustré sur la ﬁgure a.3. on dérive de cette observation toute une famille d’algorithmes, appelés algorithmes à directions de des- cente, dont la suite de cette section présente quelques exemples parmi les plus fréquemment utilisés. le plus simple de ces algorithmes est l’algorithme du gradient, parfois appelé descente de gradient par analogie avec son nom anglais gradient descent. déﬁnition 1.8 (algorithme du gradient) soient u ⊆ rn et f : u → r une fonction de classe c1. étant donnés un pas α > 0 et une tolérance (cid:15) > 0, on appelle algorithme du gradient l’algorithme suivant : 1. choisir (cid:126)u aléatoirement dans u. 2. tant que ||∇f ((cid:126)u)||2 2 > (cid:15), actualiser (cid:126)u : (cid:126)u est alors une approximation du point de minimum global de f sur u. (cid:126)u ← (cid:126)u − α∇f ((cid:126)u). (a.10) (cid:4) plus la tolérance est faible, plus le point de minimum sera proche numériquement du point de minimum global. attention le pas α est un paramètre très important de l’algorithme du gradient. si α est très faible, l’algorithme mettra très longtemps à converger. à l’inverse, si α est très élevé, (cid:126)u oscillera autour du minimum global et l’algorithme peut même diverger. on utilisera donc souvent un pas adaptatif qui varie à chaque itération et commencera par prendre des valeurs relativement élevées avant de diminuer progressivement lorsqu’on se rapproche de la solution. \fa.3. optimisation convexe sans contrainte 169 figure a.3 – une itération de l’algorithme du gradient déplace u de −αf (cid:48)(u). a.3.4 recherche linéaire par rebroussement une méthode couramment utilisée pour adapter la taille du pas de l’algorithme du gradient est la re- cherche linéaire par rebroussement. cette méthode repose sur l’observation, illustrée sur la ﬁgure a.4, que lorsque f ((cid:126)u − α∇f ((cid:126)u)) ≤ f ((cid:126)u) − ∇f ((cid:126)u)(cid:62)∇f ((cid:126)u), α 2 il est vraisemblable que α soit suﬃsamment faible pour que (cid:126)u reste du même côté du minimum global après une itération. déﬁnition 1.9 (recherche linéaire par rebroussement) soit u ⊆ rn et une fonction f : u → r de classe c1. étant donnés un pas initial α > 0, un coeﬃcient de réduction β ∈]0, 1[ et une tolérance (cid:15) > 0, on appelle recherche linéaire par rebroussement, ou bls pour backtracking line search en anglais, l’algorithme suivant : 1. choisir (cid:126)u aléatoirement dans u. 2. tant que ||∇f ((cid:126)u)||2 2 > (cid:15) : — si f ((cid:126)u − α∇f ((cid:126)u)) > f ((cid:126)u) − α 2 ∇f ((cid:126)u)(cid:62)∇f ((cid:126)u), réduire le pas : α ← βα — actualiser (cid:126)u : (cid:126)u ← (cid:126)u − α∇f ((cid:126)u). (cid:126)u est alors une approximation du point de minimum global de f sur u. (cid:4) \f170 appendice a. notions d’optimisation convexe (a) quand f (u − αf (cid:48)(u)) > f (u) − f (cid:48)(u) α 2 f (cid:48)(u), le pas α est trop élevé et u − αf (cid:48)(u) va se retrouver de l’autre côté du point de minimum. il faut donc le réduire. (b) quand f (u − αf (cid:48)(u)) ≤ f (u) − f (cid:48)(u) α 2 f (cid:48)(u), le pas α est suﬃsamment petit pour que u − αf (cid:48)(u) soit entre le point de minimum et u. figure a.4 – comparer f (u − αf (cid:48)(u)) à f (u) − f (cid:48)(u) α trop élevée. 2 f (cid:48)(u) permet de déterminer si la valeur de α est a.3.5 méthode de newton quand la fonction convexe à minimiser, f , est de classe c2, il existe des façons plus eﬃcaces d’adapter le pas. la méthode de newton est l’une d’entre elles et repose sur le développement de taylor au deuxième ordre de f en (cid:126)u. étant donné (cid:126)u ∈ u, nous déﬁnissons la fonction g : u → r donnée par g((cid:126)v) = f ((cid:126)u) + ∇f ((cid:126)u)(cid:62)((cid:126)v − (cid:126)u) + 1 2 ((cid:126)v − (cid:126)u)(cid:62)∇2f ((cid:126)u)((cid:126)v − (cid:126)u) (a.11) pour minimiser g, il suﬃt d’annuler son gradient et ainsi g est minimale au point ∇g((cid:126)v) = ∇f ((cid:126)u) + ∇2f ((cid:126)u)((cid:126)v − (cid:126)u) (cid:126)v ∗ = (cid:126)u − (cid:0)∇2f ((cid:126)u)(cid:1)−1 ∇f ((cid:126)u). (a.12) (a.13) ainsi, la méthode de newton consiste à utiliser comme pas α = (cid:0)∇2f ((cid:126)u)(cid:1)−1 . cette méthode suppose que la hessienne est inversible, ce qui est, entre autres, le cas pour les fonctions fortement convexes. dans le cas contraire, on pourra ajouter un peu de bruit à la hessienne (en lui ajoutant (cid:15)in, avec (cid:15) > 0 petit) aﬁn de la rendre inversible. déﬁnition 1.10 (méthode de newton) soient u ⊆ rn et f : u → r une fonction de classe c1. étant donnée une tolérance (cid:15) > 0, on appelle méthode de newton l’algorithme suivant : 1. choisir (cid:126)u aléatoirement dans u. 2. tant que ||∇f ((cid:126)u)||2 2 > (cid:15) : — calculer le pas : α = (cid:0)∇2f ((cid:126)u)(cid:1)−1 . — actualiser (cid:126)u : (cid:126)u ← (cid:126)u − α∇f ((cid:126)u). (cid:126)u est alors une approximation du point de minimum global de f sur u. (cid:4) \fa.3. optimisation convexe sans contrainte 171 a.3.6 méthode de newton par gradient conjugué la méthode de newton peut être lourde à mettre en œuvre en raison du calcul de l’inverse de la hes- sienne. pour y remédier, la méthode de newton par gradient conjugué consiste à calculer à chaque étape la quantité δ solution de ∇2f ((cid:126)u)δ = ∇f ((cid:126)u). (a.14) la règle d’actualisation de (cid:126)u dans l’algorithme du gradient devient alors (cid:126)u ← (cid:126)u − δ. l’équation a.14 est un problème de la forme a(cid:126)x −(cid:126)b = 0, où a (cid:23) 0 (d’après le théorème 1.4). c’est cette équation que l’on résout en utilisant la méthode dite du gradient conjugué. cette méthode a été proposée dans les années 1950 par cornelius lanczos, eduard stiefel et magnus hestenes (hestenes et stiefel, 1952). l’idée centrale de cette méthode est de construire une base de rn constituée de vecteurs conjugués par rapport à a : {(cid:126)v1, (cid:126)v2, . . . , (cid:126)vn} tels que (cid:126)v(cid:62) i a(cid:126)vj = 0 ∀i (cid:54)= j. (a.15) déﬁnition 1.11 (méthode du gradient conjugué) étant donnés une matrice semi-déﬁnie positive a ∈ rn×n et un vecteur b ∈ rn, la méthode du gradient conjugué, ou conjugate gradient en anglais, est l’algorithme suivant : 1. initialisation : — choisir aléatoirement (cid:126)x (0) ∈ rn. — initialiser (cid:126)r0 = (cid:126)v0 = (cid:126)b − a(cid:126)x (0). 2. pour t = 1, . . . , n : (a) actualiser (cid:126)x : (b) actualiser le résiduel : (c) actualiser (cid:126)v : (cid:126)x (n) est la solution recherchée. (cid:126)x (t) = (cid:126)x (t−1) + (cid:126)r(cid:62) t−1(cid:126)rt−1 (cid:126)v(cid:62) t−1a(cid:126)vt−1 (cid:126)vt−1. (cid:126)rt = (cid:126)b − a(cid:126)x (t). (cid:126)vt = (cid:126)rt + (cid:126)r(cid:62) t (cid:126)rt (cid:126)r(cid:62) t−1(cid:126)rt−1 (cid:126)vt−1. théorème 1.5 la méthode du gradient conjugué assure que (cid:126)v(cid:62) i a(cid:126)vj = 0 ∀i (cid:54)= j. (a.16) (a.17) (a.18) (cid:4) (cid:4) et βt = (cid:126)r(cid:62) t (cid:126)rt démonstration. posons αt = (cid:126)r(cid:62) t−1(cid:126)rt−1 nous allons commencer par poser quelques réécritures des déﬁnitions ci-dessus qui nous serviront par la suite. en remplaçant (cid:126)x (t) par sa déﬁnition (équation a.16) dans l’équation a.17, on obtient (cid:126)rt = (cid:126)b − a(cid:126)x (t−1) − αta(cid:126)vt−1 et donc : (cid:126)r(cid:62) t−1(cid:126)rt−1 (cid:126)v(cid:62) t−1a(cid:126)vt−1 . de manière équivalente, de plus, d’après l’équation a.18, (cid:126)rt = (cid:126)rt−1 − αta(cid:126)vt−1. a(cid:126)vt−1 = 1 αt ((cid:126)rt−1 − (cid:126)rt). (cid:126)rt = (cid:126)vt − βt(cid:126)vt−1. (a.19) (a.20) (a.21) \f172 appendice a. notions d’optimisation convexe enﬁn, d’après les déﬁnitions de αt et βt, βt(cid:126)v(cid:62) t−1a(cid:126)vt−1 = (cid:126)r(cid:62) t (cid:126)rt αt . (a.22) nous allons maintenant montrer par récurrence que pour tout t = 1, . . . , n, pour tout i = 1, . . . , t, t (cid:126)rt−i = 0 et (cid:126)v(cid:62) (cid:126)r(cid:62) t a(cid:126)vt−i = 0. commençons par le cas t = 1 (et donc i = 1.) il s’agit de montrer que (cid:126)r(cid:62) 1 (cid:126)r0 = 0 et (cid:126)v(cid:62) 1 a(cid:126)v0 = 0. d’après l’équation a.19, 1 (cid:126)r0 = (cid:126)r(cid:62) (cid:126)r(cid:62) 0 (cid:126)r0 − α1a(cid:126)v(cid:62) 0 (cid:126)r0 (cid:126)v(cid:62) 0 (cid:126)r0 (cid:126)v(cid:62) 0 (cid:126)v0 0 (cid:126)r0 1 − (cid:18) = (cid:126)r(cid:62) (cid:19) . a comme (cid:126)r0 = (cid:126)v0, la fraction ci-dessus vaut 1, et on obtient bien (cid:126)r(cid:62) 1 (cid:126)r0 = 0. par la déﬁnition de (cid:126)v1 (équation a.18), (cid:126)v(cid:62) 1 ((cid:126)r0 − (cid:126)r1). d’après l’équation a.22, β1(cid:126)v(cid:62) (cid:126)r(cid:62) 1 a(cid:126)v0 + β1(cid:126)v(cid:62) 1 a(cid:126)v0 = (cid:126)r(cid:62) 0 a(cid:126)v0 = (cid:126)r(cid:62) 1 (cid:126)r1 . ainsi, (cid:126)v(cid:62) α1 0 a(cid:126)v0. d’après l’équation a.20, (cid:126)r(cid:62) 1 a(cid:126)v0 = (cid:126)r(cid:62) 1 (cid:126)r0 α1 1 a(cid:126)v0 = = 0 car nous venons de 1 α1 montrer que (cid:126)r(cid:62) 1 (cid:126)r0 = 0. ceci conclut le cas t = 1. supposons maintenant t > 1, et que pour tout u < t, pour tout i = 1, . . . , u, (cid:126)r(cid:62) u (cid:126)ru−i = 0 et (cid:126)v(cid:62) u a(cid:126)vu−i = 0. nous allons tout d’abord montrer que (cid:126)r(cid:62) et le fait que a = a(cid:62) (car a (cid:23) 0,) on obtient t (cid:126)rt−i = 0 pour tout i = 1, . . . , t. en utilisant l’équation a.19, t (cid:126)rt−i = (cid:126)r(cid:62) (cid:126)r(cid:62) t−1(cid:126)rt−i − αt(cid:126)v(cid:62) t−1a(cid:126)rt−i. (a.23) trois cas sont possibles : — si i = 1, nous avons alors, en remplaçant i par 1 et αt par sa déﬁnition dans l’équation a.23 t (cid:126)rt−1 = (cid:126)r(cid:62) (cid:126)r(cid:62) t−1(cid:126)rt−1 1 − (cid:32) (cid:126)v(cid:62) t−1a(cid:126)rt−1 (cid:126)v(cid:62) t−1a(cid:126)vt−1 (cid:33) . d’après l’équation a.21, t−1a(cid:126)rt−1 = (cid:126)v(cid:62) (cid:126)v(cid:62) t−1a(cid:126)vt−1 − βt−1(cid:126)v(cid:62) t−1a(cid:126)vt−2. d’après les hypothèses de la récurrence, (cid:126)v(cid:62) la fraction ci-dessus est égale à 1, ce qui nous permet de conclure que (cid:126)r(cid:62) t−1a(cid:126)vt−2 = 0 et donc (cid:126)v(cid:62) t−1a(cid:126)rt−1 = (cid:126)v(cid:62) t (cid:126)rt−1 = 0. t−1a(cid:126)vt−1, et ainsi — si 1 < i < t, nous pouvons remplacer la deuxième occurrence de (cid:126)rt−i dans l’équation a.23 par sa valeur donnée par l’équation a.21, et obtenir ainsi t (cid:126)rt−i = (cid:126)r(cid:62) (cid:126)r(cid:62) t−1(cid:126)rt−i − αt(cid:126)v(cid:62) t−1a(cid:126)vt−i − αtβt−i(cid:126)v(cid:62) t−1a(cid:126)vt−i−1. chacun des termes de cette somme est nul d’après nos hypothèses de récurrence, ce qui nous permet de conclure que (cid:126)r(cid:62) t−1(cid:126)rt−i = 0. — enﬁn, si i = t, il s’agit d’étudier (cid:126)r(cid:62) t−1(cid:126)r0 − αt(cid:126)v(cid:62) t−1a(cid:126)r0. comme (cid:126)r0 = (cid:126)v0, les deux termes de cette somme sont nuls d’après nos hypothèses de récurrence, et (cid:126)r(cid:62) t (cid:126)r0 = 0. \fa.3. optimisation convexe sans contrainte nous pouvons maintenant nous intéresser à (cid:126)v(cid:62) t a(cid:126)vt−i. d’après l’équation a.18, t a(cid:126)vt−i = (cid:126)r(cid:62) (cid:126)v(cid:62) t a(cid:126)vt−i + βt(cid:126)v(cid:62) t−1a(cid:126)vt−i. d’après l’équation a.20, le premier terme de cette somme peut s’écrire (cid:126)r(cid:62) t a(cid:126)vt−i = 1 αt−i+1 (cid:126)r(cid:62) t ((cid:126)rt−i − (cid:126)rt−i+1). 173 (a.24) deux cas sont possibles : — si i > 1, (cid:126)r(cid:62) t (cid:126)rt−i et (cid:126)r(cid:62) première partie de cette preuve. le terme (cid:126)v(cid:62) récurrence. t (cid:126)rt−i+1 sont tous les deux nuls comme nous venons de le montrer dans la t−1a(cid:126)vt−i est lui aussi nul d’après nos hypothèses de — si i = 1, le premier terme de la somme dans l’équation a.24 vaut (cid:126)r(cid:62) est donné directement par l’équation a.22, et vaut l’opposé du premier. t a(cid:126)vt−i = 0. ainsi, quelle que soit la valeur de i, (cid:126)v(cid:62) t a(cid:126)vt−1 = − (cid:126)r(cid:62) t (cid:126)rt αt . le deuxième (cid:3) les vecteurs (cid:126)vt formant une base de rn, à partir de la n-ème itération de la méthode du gradient conju- gué, (cid:126)vt = 0. remarque a.3.7 méthodes de quasi-newton il est possible que le calcul de la hessienne nécessite beaucoup de ressources. dans ce cas, on utilise des méthodes, dites de quasi-newton, qui permettent de remplacer l’inverse de la hessienne par une approxi- mation dans la méthode de newton. cette approximation est calculée de manière itérative. déﬁnition 1.12 (méthode de quasi-newton) soient u ⊆ rn et f : u → r une fonction de classe c1. étant donnée une tolérance (cid:15) > 0, on appelle méthode de quasi-newton un algorithme prenant la forme suivante : 1. choisir (cid:126)u aléatoirement dans u. 2. initialiser w : w (0) = i, la matrice identité de même dimension que la hessienne. 3. tant que ||∇f ((cid:126)u)||2 2 > (cid:15) : — incrémenter t : t ← (t + 1) — actualiser (cid:126)u : (cid:126)u (t) = (cid:126)u (t) − w (t−1) — actualiser l’approximation w de (cid:0)∇2f ((cid:126)u)(cid:1)−1 (cid:126)u (t) est alors une approximation du point de minimum global de f sur u. (cid:4) pour approcher l’inverse de la hessienne ∇2f ((cid:126)u)−1, on cherche une matrice w (t) qui soit — symétrique et semi-déﬁnie positive ; — proche de l’approximation précédente, autrement dit telle que ||w (t) − w (t−1)||f soit minimale (ici ||.||f désigne la norme de frobenius d’une matrice, autrement dit la racine carrée de la somme des carrés de ses entrées) ; — telle que w (t) (cid:0)∇f ((cid:126)u (t)) − ∇f ((cid:126)u (t−1))(cid:1) = (cid:126)u (t)−(cid:126)u (t−1), cette dernière condition, appelée équation de la sécante, étant directement dérivée d’un développement de taylor à l’ordre 1 de ∇f en (cid:126)u (t−1). \f174 appendice a. notions d’optimisation convexe une des méthodes de quasi-newton les plus utilisées de nos jours, et les plus performantes, est la méthode bfgs, ainsi nommée d’après charles george broyden, roger fletcher, donald goldfarb et david shanno qui l’ont proposée tous les quatre indépendamment en 1970. dans cette méthode, l’approximation itérative de l’inverse de la hessienne est donnée par w (t) = w (t−1) − dtδ(cid:62) t w (t−1) + w (t−1)δtd(cid:62) t (cid:104)δt, dt(cid:105) (cid:32) + 1 + (cid:104)δt, w (t−1)δt(cid:105) (cid:104)δt, dt(cid:105) (cid:33) dtd(cid:62) t (cid:104)δt, dt(cid:105) (a.25) où dt = (cid:126)u (t) − (cid:126)u (t−1) et δt = ∇f ((cid:126)u (t)) − ∇f ((cid:126)u (t−1)). il en existe une version moins gourmande en mémoire, appelée l-bfgs pour limited-memory bfgs, qui évite d’avoir à stocker en mémoire l’intégralité de la matrice w (t). a.3.8 algorithme du gradient stochastique dans le cas où n est très grand, le gradient lui-même peut devenir coûteux à calculer. cependant, on rencontre souvent en apprentissage statistique des fonctions à minimiser qui peuvent se décomposer en une somme de n fonctions plus simples : son gradient se décompose alors aussi en f ((cid:126)u) = ∇f ((cid:126)u) = n (cid:88) i=1 n (cid:88) fi((cid:126)u). ∇fi((cid:126)u). i=1 exemple (a.26) (a.27) c’est le cas par exemple lorsque l’on cherche à minimiser une somme de moindres carrés (voir sec- tion 5.1.2) : cette somme s’écrit où φ est la fonction de prédiction. f ((cid:126)β) = n (cid:88) (cid:16) i=1 yi − φ((cid:126)x i|(cid:126)β) (cid:17)2 l’algorithme du gradient stochastique permet alors d’accélérer les calculs en n’utilisant à chaque ité- ration qu’une seule des fonctions fi : on remplace (cid:80)n i=1 ∇fi((cid:126)u) par ∇fk((cid:126)u). déﬁnition 1.13 (algorithme du gradient stochastique) soient u ⊆ rn et f : u → r une fonction de classe c1, décomposable sous la forme f ((cid:126)u) = n (cid:88) fi((cid:126)u). (a.28) i=1 étant donnés un pas α > 0 et une tolérance (cid:15) > 0, on appelle algorithme du gradient stochastique l’algorithme suivant : 1. choisir (cid:126)u aléatoirement dans u. 2. tant que ||∇f ((cid:126)u)||2 2 > (cid:15) : — choisir k aléatoirement parmi {1, 2, . . . , n}. — actualiser (cid:126)u : (cid:126)u ← (cid:126)u − α∇fk((cid:126)u). (cid:126)u est alors une approximation du point de minimum global de f sur u. (cid:4) on peut bien sûr utiliser aussi un pas adaptatif dans ce contexte. \fa.4. optimisation convexe sous contraintes a.3.9 descente de coordonnées 175 les techniques que nous avons vues ci-dessus supposent que la fonction à minimiser est diﬀérentiable. une technique similaire peut être utilisée pour une fonction non diﬀérentiable, mais qui peut se décom- poser comme une somme d’une fonction convexe de classe c1 et de n fonctions convexes chacune d’une seule des coordonnées. on minimise alors la fonction coordonnée par coordonnée. déﬁnition 1.14 (descente de coordonnées) soit u ⊆ rn et f : u → r une fonction de la forme f : (cid:126)u (cid:55)→ g((cid:126)u) + n (cid:88) i=1 hi(ui), où g est une fonction convexe de classe c1 et les n fonctions hi sont convexes. on appelle descente de coordonnées, ou coordinate descent en anglais, l’algorithme suivant : 1. choisir (cid:126)u aléatoirement dans u. 2. tant que ||∇f ((cid:126)u)||2 2 > (cid:15), actualiser (cid:126)u : — u(t) 1 — u(t) 2 — . . . — u(t) point minimal de u (cid:55)→ f (u, u(t−1) 2 point minimal de u (cid:55)→ f (u(t−1) , . . . , u(t−1) , u, . . . , u(t−1) n ) ) n n point minimal de u (cid:55)→ f (u(t−1) , u(t−1) 2 , . . . , u). 1 1 (cid:126)u est alors une approximation du point de minimum global de f sur u. (cid:4) a.4 optimisation convexe sous contraintes dans cette partie, nous nous intéressons aux problèmes d’optimisation de la forme min(cid:126)u∈u (p) : f ((cid:126)u) sous les contraintes gi((cid:126)u) ≤ 0 ∀i = 1, . . . , m hj((cid:126)u) = 0 ∀j = 1, . . . , r, (a.29) où les fonctions f , gi, hj sont supposées à valeurs réelles et de classe c1. de nombreuses méthodes permettent de résoudre ce type de problèmes. nous détaillerons dans ce qui suit la méthode des multiplicateurs de lagrange. a.4.1 lagrangien pour résoudre (p), nous allons introduire son lagrangien. déﬁnition 1.15 (lagrangien) soit (p) un problème de minimisation sous contraintes de la forme a.29. on appelle lagrangien de (p) la fonction l : rn × rm × rr → r (cid:126)u, (cid:126)α, (cid:126)β (cid:55)→ f ((cid:126)u) + n (cid:88) i=1 αi gi((cid:126)u) + r (cid:88) j=1 βj hj((cid:126)u). les variables α1, α2, . . . , αm, β1, β2, . . . , βr sont appelées multiplicateurs de lagrange ou variables duales. (cid:4) \f176 appendice a. notions d’optimisation convexe le lagrangien nous permet de déﬁnir une fonction, appelée fonction duale de lagrange, dont le maximum donne une borne inférieure à la solution de (p). déﬁnition 1.16 (fonction duale de lagrange) soient (p) un problème de minimisation sous contraintes de la forme a.29, et l son lagrangien. on appelle fonction duale de lagrange la fonction q : rm × rr → r (cid:126)α, (cid:126)β (cid:55)→ inf (cid:126)u∈u l((cid:126)u, (cid:126)α, (cid:126)β), l’inﬁmum inf (cid:126)u∈u l((cid:126)u, (cid:126)α, (cid:126)β) de l en (cid:126)u étant la plus grande valeur q∗ ∈ r telle que q∗ ≤ l((cid:126)u, (cid:126)α, (cid:126)β) pour (cid:4) tout u ∈ u, et pouvant potentiellement valoir −∞. la fonction duale de lagrange a l’intérêt d’être une fonction concave, indépendamment de la convexité de (p). théorème 1.6 la fonction duale de lagrange d’un problème de minimisation sous contraintes de la forme a.29 (cid:4) est concave. démonstration. soient ((cid:126)α1, (cid:126)β1), ((cid:126)α2, (cid:126)β2) et 0 ≤ λ ≤ 1. prenons (cid:126)α = λ(cid:126)α1 + (1 − λ)(cid:126)α2 et (cid:126)β = λ(cid:126)β1 + (1 − λ)(cid:126)β2. alors q((cid:126)α, (cid:126)β) = inf (cid:126)u∈u f ((cid:126)u) + (cid:126)α (cid:62)(cid:126)g + (cid:126)β (cid:62)(cid:126)h, où (cid:126)g = (g1((cid:126)u), g2((cid:126)u), . . . gm((cid:126)u)) et (cid:126)h = (h1((cid:126)u), h2((cid:126)u), . . . hr((cid:126)u)). ainsi, q((cid:126)α, (cid:126)β) = inf (cid:126)u∈u f ((cid:126)u) + λ(cid:126)α(cid:62) 1 (cid:126)g + (1 − λ)(cid:126)α(cid:62) (cid:126)h (cid:126)h + (1 − λ)(cid:126)β(cid:62) 2 = inf (cid:126)u∈u λ (cid:16) f ((cid:126)u) + (cid:126)α(cid:62) (cid:17) (cid:126)h 1 (cid:126)g + (cid:126)β(cid:62) 1 f ((cid:126)u) + (cid:126)α(cid:62) (cid:17) (cid:126)h 2 (cid:126)g + (cid:126)β(cid:62) 2 , 2 (cid:126)g + λ(cid:126)β(cid:62) 1 (cid:16) + (1 − λ) cette dernière égalité venant de ce que f ((cid:126)u) = λf ((cid:126)u) + (1 − λ)f ((cid:126)u). enﬁn, q((cid:126)α, (cid:126)β) ≥ λ inf (cid:126)u∈u (cid:16) f ((cid:126)u) + (cid:126)α(cid:62) (cid:17) (cid:126)h 1 (cid:126)g + (cid:126)β(cid:62) 1 + (1 − λ) inf (cid:126)u∈u (cid:16) f ((cid:126)u) + (cid:126)α(cid:62) (cid:17) (cid:126)h 2 (cid:126)g + (cid:126)β(cid:62) 2 ≥ λq((cid:126)α1, (cid:126)β1) + (1 − λ)q((cid:126)α2, (cid:126)β2), et q est donc concave. a.4.2 dualité faible (cid:3) la fonction duale de lagrange d’un problème de minimisation sous contraintes (p) donne une borne inférieure à la solution de (p). théorème 1.7 soit p∗ une solution de (p). alors quels que soient α1, α2, . . . , αm ≥ 0 et β1, β2, . . . , βr ∈ r, q(α, β) ≤ p∗. (cid:4) \fa.4. optimisation convexe sous contraintes 177 démonstration. soit (cid:126)u un point admissible. gi((cid:126)u) ≤ 0 pour tout i = 1, . . . , m et hj((cid:126)u) = 0 pour tout j = 1, . . . , r. ainsi, l((cid:126)u, (cid:126)α, (cid:126)β) = f ((cid:126)u) + (cid:80)n l((cid:126)u, (cid:126)α, (cid:126)β) ≤ f ((cid:126)u). on en déduit que i=1 αigi((cid:126)u) + (cid:80)r j=1 βjhj((cid:126)u) ≤ f ((cid:126)u) et donc, pour tout point admissible, inf (cid:126)u∈rn l((cid:126)u, (cid:126)α, (cid:126)β) ≤ min (cid:126)u∈rn f ((cid:126)u). (cid:3) puisque q(α, β) ≤ p∗ pour tout (cid:126)α ∈ rm + , (cid:126)β ∈ rr, le maximum de q sur rm + × rr est la meilleure de ces bornes inférieures de p∗. déﬁnition 1.17 (problème dual de lagrange) soient (p) un problème de minimisation sous contrainte de la forme a.29, l son lagrangien, et q sa fonction duale de lagrange. on appelle problème dual de lagrange le problème d’optimisation suivant : (q) : max(cid:126)α∈rm,(cid:126)β∈rr sous les contraintesαi ≥ 0 ∀i = 1, . . . , m q((cid:126)α, (cid:126)β) le problème (p) est alors appelé problème primal. enﬁn, les points de maximisation (cid:126)α ∗, (cid:126)β ∗ sont appelés multiplicateurs de lagrange optimaux ou variables (cid:4) duales optimales. comme q est concave, le problème dual d’un problème d’optimisation sous contraintes quelconque est un problème d’optimisation sous contraintes convexe. théorème 1.8 (dualité faible) soient (p) un problème de minimisation sous contrainte de la forme a.29 et (q) son dual de lagrange. appelons d∗ une solution de (q), et p∗ une solution du primal (p). alors d∗ ≤ p∗. c’est ce que l’on appelle la dualité faible. (cid:4) a.4.3 dualité forte si la solution du dual minore la solution du primal, elles ne sont pas égales dans le cas général. pour certaines classes de fonctions, il est possible cependant que d∗ = p∗. on parle alors de dualité forte. déﬁnition 1.18 (dualité forte) soient (p) un problème de minimisation sous contrainte de la forme a.29 et (q) son dual de lagrange. appelons d∗ la solution de (q), et p∗ la solution du primal (p). on parle de dualité forte si d∗ = p∗. (cid:4) la condition de slater est une condition suﬃsante (mais non nécessaire) pour garantir la dualité forte, que l’on doit à morton l. slater (1950). théorème 1.9 (condition de slater) soit (p) un problème de minimisation sous contrainte de la forme a.29. si (p) est convexe, c’est-à-dire f , g1, g2, . . . , gm sont convexes et h1, h2, . . . , hr sont convexes, et qu’il existe au moins un point admissible pour lequel les contraintes d’inégalités gi non aﬃnes soient vériﬁées strictement, alors la (cid:4) dualité forte est garantie. \f178 appendice a. notions d’optimisation convexe ainsi, on peut résoudre un grand nombre de problèmes d’optimisation convexe sous contraintes en passant par le dual, dont les contraintes (αi ≥ 0) sont simples à prendre en compte. on peut en particu- lier utiliser des méthodes de gradient projeté, qui consistent à « ramener » les itérations successives d’un algorithme à directions de descente dans le domaine des contraintes en les projetant sur cet ensemble. a.4.4 conditions de karush-kuhn-tucker dans un cadre de dualité forte, si les fonctions f , gi et hj sont toutes diﬀérentiables, william karush (dans sa thèse de master de 1939, non publiée) puis harold w. kuhn et albert w. tucker (kuhn et tucker, 1951) ont développé un ensemble de conditions suﬃsantes à l’optimalité de la solution d’un problème d’optimisation sous contraintes. théorème 1.10 (conditions de karush-kuhn-tucker) soit (p) un problème de minimisation sous contrainte de la forme a.29 et (q) son problème dual. soit un triplet ((cid:126)u ∗, (cid:126)α ∗, (cid:126)β ∗) de rn × rm × rr qui vériﬁe les conditions suivantes, dites de conditions de karush- kuhn-tucker ou conditions kkt : — admissibilité primale : gi((cid:126)u ∗) ≤ 0 pour tout i = 1, . . . , m et hj((cid:126)u ∗) = 0 pour tout j = 1, . . . , r ; — admissibilité duale : α∗ — complémentarité des contraintes : α∗ — stationnarité : ∇f ((cid:126)u ∗) + (cid:80)m i=1 α∗ i gi((cid:126)u ∗) = 0 pour tout i = 1, . . . , m ; i ∇gi((cid:126)u ∗) + (cid:80)r i ∇hj((cid:126)u ∗) = 0. j=1 β∗ i ≥ 0 pour tout i = 1, . . . , m ; alors (cid:126)u ∗ est un point de minimisation du primal (p) et ((cid:126)α ∗, (cid:126)β ∗) est un point de maximisation du dual (q). (cid:4) démonstration. soit un triplet ((cid:126)u ∗, (cid:126)α ∗, (cid:126)β ∗) qui vériﬁe les conditions de kkt. la condition d’admissi- bilité primale implique que (cid:126)u ∗ est admissible. la fonction (cid:126)u (cid:55)→ l((cid:126)u, (cid:126)α ∗, (cid:126)β ∗) est convexe en (cid:126)u. en eﬀet, l((cid:126)u, (cid:126)α ∗, (cid:126)β ∗) = f ((cid:126)u) + (cid:80)n j=1 β∗ i gi((cid:126)u) + sont positifs, et les hj sont aﬃnes. la condition (cid:80)r j hj((cid:126)u), les fonctions f et gi sont convexes, les α∗ i de stationnarité implique donc que (cid:126)u ∗ minimise l((cid:126)u, (cid:126)α ∗, (cid:126)β ∗). i=1 α∗ par déﬁnition de la fonction duale de lagrange, q((cid:126)α ∗, (cid:126)β ∗) = l((cid:126)u ∗, (cid:126)α ∗, (cid:126)β ∗) = f ((cid:126)u ∗). en eﬀet, la condition de complémentarité des contraintes implique (cid:80)n i gi((cid:126)u ∗) = 0, et celle d’admissibilité primale implique (cid:80)r i=1 α∗ soient p∗ la solution de (p) et d∗ celle de (q). par déﬁnition de d∗, f ((cid:126)u ∗) = q((cid:126)α ∗, (cid:126)β ∗) ≤ d∗, et par dualité faible, d∗ ≤ p∗. ainsi, f ((cid:126)u ∗) ≤ p∗ et donc f ((cid:126)u ∗) = p∗ : (cid:126)u ∗ est un point de minimisation de (p), et toutes les inégalités précédentes sont donc des égalités et, en particulier, q((cid:126)α ∗, (cid:126)β ∗) = d∗ et ((cid:126)α ∗, (cid:126)β ∗) est (cid:3) un point de maximisation de (q). j hj((cid:126)u ∗) = 0. j=1 β∗ géométriquement, les conditions de stationnarité et de complémentarité des contraintes peuvent se comprendre comme suit. considérons un problème d’optimisation convexe sous une unique contrainte d’inégalité : min(cid:126)u∈rn f ((cid:126)u) sous la contrainte g((cid:126)u) ≤ 0. (a.30) soit (cid:126)u0 un point de minimisation sans contrainte : (cid:126)u0 = min(cid:126)u∈rn f ((cid:126)u). deux cas sont possibles : — soit (cid:126)u0 appartient au domaine des contraintes, et (cid:126)u ∗ = (cid:126)u0 est un point de minimisation de (p) (cas 1) ; — soit (cid:126)u0 n’appartient pas au domaine des contraintes (cas 2). dans le premier cas, ∇f ((cid:126)u ∗) = 0 (puisque (cid:126)u ∗ est solution du problème sans contrainte) et g((cid:126)u ∗) ≤ 0 (puisque (cid:126)u ∗ appartient au domaine des contraintes). \fa.4. optimisation convexe sous contraintes 179 figure a.5 – le point de minimisation se trouve à la frontière du domaine des contraintes, en un point tangent à une ligne de niveau de f . en ce point, les gradients ∇g et ∇f sont parallèles et de directions opposées. dans le deuxième cas, illustré sur la ﬁgure a.5, un point de minimisation (cid:126)u ∗ se trouve à la bordure du domaine des contraintes (g((cid:126)u ∗) = 0), car f étant convexe et minimisée à l’extérieur de ce domaine croît quand on s’en rapproche. plus précisément, (cid:126)u ∗ se trouve à l’intersection de la bordure du domaine des contraintes et d’une ligne de niveau de f . en ce point, le gradient de f étant tangent à la ligne de niveau et celui de g étant tangent à la bordure du domaine, ∇f et ∇g sont parallèles. de plus, f croît en s’éloignant de (cid:126)u0, et g est négative à l’intérieur du domaine des contraintes. ∇f et ∇g sont donc de directions opposées. ainsi, il existe α ≥ 0 tel que ∇f ((cid:126)u ∗) = −α∇g((cid:126)u ∗). les deux cas peuvent donc être résumés par ∇f ((cid:126)u ∗) + α∇g((cid:126)u ∗) = 0 et αg((cid:126)u ∗) = 0, où — soit α = 0 et g((cid:126)u ∗) ≤ 0 (cas 1) — soit g((cid:126)u ∗) = 0 et α ≥ 0 (cas 2). on a ainsi retrouvé la condition de stationnarité ∇f ((cid:126)u ∗) + α∇g((cid:126)u ∗) = 0 et celle de complémentarité des contraintes αg((cid:126)u ∗) = 0. a.4.5 programmes quadratiques un cas particulier de problème d’optimisation convexe sous contraintes qui revient souvent en machine learning est celui de l’optimisation quadratique convexe, ou convex quadratic programming (convex qp) en anglais. dans ce cadre, la fonction f à minimiser est quadratique et les contraintes sont aﬃnes. déﬁnition 1.19 étant donnés n, m, r trois entiers positifs, une matrice semi-déﬁnie positive q ∈ rn×n, des vecteurs (cid:126)a,(cid:126)b1, . . . ,(cid:126)bm, (cid:126)c1, . . . , (cid:126)cr de rn, et des constantes d, k1, . . . , km, l1, . . . , lr ∈ r, on ap- domaine des contrainteslignes de niveau de la fonction objectifminimumsans contrainteminimum sous contrainte\f180 appendice a. notions d’optimisation convexe pelle problème d’optimisation quadratique convexe un problème de la forme min (cid:126)u∈rn (cid:126)u (cid:62)q(cid:126)u + (cid:126)a (cid:62)(cid:126)u + d sous les contraintes 1 2 (cid:126)b(cid:62) i (cid:126)u + ki ≤ 0 ∀i = 1, . . . , m (cid:126)c(cid:62) j (cid:126)u + lj = 0 ∀j = 1, . . . , r. (cid:4) il s’agit d’un problème d’optimisation convexe, vériﬁant les conditions de slater, et dont la région ad- missible est un polyèdre. il existe de nombreuses méthodes pour résoudre ce genre de problèmes, comme les méthodes de point intérieur ou les méthodes d’activation (active set en anglais.) de nombreux logiciels et librairies implé- mentent ces approches, comme cplex, cvxopt, cgal et bien d’autres. pour aller plus loin • l’ouvrage de boyd et vandenberghe (2004) fait ﬁgure de référence sur l’optimisation convexe. pour les aspects numériques, on pourra se reporter à bonnans et al. (1997) ou à nocedal et wright (2006). • pour en savoir plus sur les algorithmes à directions de descente stochastique, on pourra se reporter à l’article de léon bottou (2012). • pour plus de détails sur la dualité de lagrange, on pourra aussi se reporter au chapitre 8 de luen- berger (1969) ou à bertsekas et al. (2003). bibliographie bertsekas, d. p., nedi´, a., et ozdaglar, a. e. (2003). convex analysis and optimization. athena scientiﬁc, bel- mont, ma. bonnans, j., gilbert, j., lemaréchal, c., et sagastizábal, c. (1997). optimisation numérique : aspects théoriques et pratiques. sprinver-verlag, berlin heidelberg. bottou, l. (2012). stochastic gradient descent tricks. http://leon.bottou.org/publications/pdf/ tricks-2012.pdf. boyd, s. et vandenberghe, l. (2004). convex optimization. cambridge university press. https://web. stanford.edu/~boyd/cvxbook/. hestenes, m. et stiefel, e. (1952). methods of conjugate gradients for solving linear systems. research of the national bureau of standards, 19(6). journal of kuhn, h. w. et tucker, a. w. (1951). nonlinear programming. in neyman, j., editor, proceedings of the second berkeley symposium on mathematical statistics and probability, pages 481–492. university of california press. luenberger, d. g. (1969). optimization by vector space methods. john wiley & sons. nocedal, j. et wright, s. j. (2006). numerical optimization. springer, berlin, new york, 2nd edition. slater, m. (1950). lagrange multipliers revisited. cowles commission discussion paper, number 403. \f\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:28:13.742680Z",
     "start_time": "2025-08-10T21:28:13.607864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "model_emb = SentenceTransformer(model_path)"
   ],
   "id": "f5f69971f451de5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:28:29.571965Z",
     "start_time": "2025-08-10T21:28:15.767241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#chunks = model.decouper_en_chunks(texte,150,50)\n",
    "chunks = extract_paragraphs(r\"C:\\Users\\tallar\\Documents\\PROJETS\\GenAI\\ChatBot\\files\\IntroML_Azencott.pdf\")\n",
    "model.set_document(chunks)"
   ],
   "id": "98d7c9b793993fbd",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:29:10.401495Z",
     "start_time": "2025-08-10T21:28:38.105114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#model.embeddings = model.embedding(chunks)\n",
    "model.embeddings=encode_chunks(chunks,model_emb )"
   ],
   "id": "8565adc3707a78a8",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:12:48.752181700Z",
     "start_time": "2025-08-10T20:51:42.622857Z"
    }
   },
   "cell_type": "code",
   "source": "#model.Faiss_index()",
   "id": "e3bb60cb46d25ef4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253 documents indexés.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:32:19.397901Z",
     "start_time": "2025-08-10T21:31:55.482031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question=(\"retropropagation?\")\n",
    "#model.rag(question)\n",
    "questions = reformulation(question, 3, model_name=\"mistral\")\n",
    "contexts=[]\n",
    "all_chunks=[]\n",
    "for question in questions:\n",
    "    top_chunks = retrieve_top_k(question, chunks, model.embeddings, model_emb, k=5)\n",
    "    for chunk in top_chunks:\n",
    "        all_chunks.append(chunk)\n",
    "\n",
    "contexts.append(top_chunks)\n",
    "model.set_context(top_chunks)\n",
    "model.set_task(f\"répond à la question suivante en utilisant uniquement le contexte: {question}\")\n",
    "print(model.ask(model.task))"
   ],
   "id": "2958dcfcf7d856f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "True\n",
      "Je suis désolé, mais le contexte fourni ne contient pas suffisamment d'informations pour répondre à votre question. Avez-vous une question spécifique que je pourrais vous aider à clarifier ou à répondre en utilisant le contexte donné ?\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:32:43.865044Z",
     "start_time": "2025-08-10T21:32:43.849395Z"
    }
   },
   "cell_type": "code",
   "source": "print(model.context)",
   "id": "28d4dcf88dc258c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Boosting the margin : a new explanation for theeﬀectiveness of voting methods', 'Notre problème admet donc toujours uneunique solution explicite', 'Il s’agit généralement de minimiser un risque empirique, ou de maximiser unevariance, souvent sous certaines contraintes', 'Ainsi, à chaque nouvelle valeur de seuil, une observation que l’on prédisait précédemment négative change d’étiquette', 'Marquer tous les éléments de C comme visités : V ← V ∪ C']\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
